\documentclass[a4paper]{article}
\usepackage{dmvn}

\newcommand{\ssc}{;\bw\dots;}
\newcommand{\dx}{\,dx}
\newcommand{\dt}{\,dt}
\newcommand{\dy}{\,dy}

\DeclareMathOperator{\argmin}{argmin}

\newenvironment{petit}{\medskip\hrule\smallskip\footnotesize}{\par\smallskip\hrule\medskip}

\tocsubsectionparam{2.8em}

\begin{document}
\dmvntitle{Курс лекций по}{численным методам}{Лектор\т Георгий Михайлович Кобельков}
{IV курс, 7--8 семестр, поток математиков}{Москва, 2006 г.}

\pagebreak
\tableofcontents

\pagebreak

\section*{Предисловие}


Этот документ представляет собой курс лекций по численным методам, читаемый в 7--8 семестре.
Порядок изложения материала наиболее соответствует курсу 2005--2006 г.

Если выяснится, что в некоторых билетах чего-то катастрофически не хватает, но это не отражено в тексте, пишите.
Кое-где явно написано, что <<в лекциях муть>>, и исправить это не представляется возможным.
Поймите правильно, уважаемые читатели, нет ничего страшнее, чем написать какой-то бред и выдавать его за правду.

\subsection*{Release notes}

\begin{items}{-2}
\item [21.05] Паша Наливайко победил тяжкий бред в очень-очень быстром преобразовании Фурье.
Несмотря на всю его быстроту, текст надо было писать не торопясь\dots
\item [21.05] А ещё добавился метод Поспелова в вольном изложении Александра Воронцова, за что ему отдельная благодарность.
В нем было исправлено немножко опечаток, и стало лучше.
\item [28.05] В данной версии написан метод конечных элементов в не менее вольном изложении автора конспекта.
\item [29.05] Гип-гип, ура! Появился метод баланса. Ещё замечен бред в одном из методов линейной алгебры
(но на него для простоты на экзамене можно забить).
\item [31.05] Обработан последний поступивший багрепорт от Паши Наливайко. Жить стало легче, жить стало веселее :)
\end{items}

\medskip

\subsection*{Благодарности}

За поиск опечаток спасибо Лёхе Басалаеву, Сергею Гладких, Паше Наливайко, Саше Воронцову, а также  и всем, кого я ещё забыл :)

\medskip
\dmvntrail

\pagebreak

\section{Представление вещественных чисел в компьютере}

\subsection{Мантисса и порядок}

Всем ясно, что хранить бесконечные десятичные дроби мы пока не умеем\т памяти не хватит. Поэтому будем хранить
только их приближения с некоторой разумной точностью. Просто хранить сколько\д то знаков после запятой глупо, ибо хочется
уметь работать с числами вида $1\cdot 10^{100}$ и $1\cdot 10^{-100}$, а отводить память под 100 знаков крайне неэкономно.
Кроме того, при работе с очень маленькими (или, наоборот, очень большими) числами  нам не так уж важны младшие разряды, а
важен \emph{порядок} числа. Вот поэтому\д то числа и хранят в виде \emph{мантиссы} и порядка. Точнее говоря, в системе
$F(\be,t,L,U)$, где $\be$\т основание системы, $t$\т количество разрядов, а $L$ и $U$\т верхний и нижний пределы изменения
порядка, число $x \in F$ записывается в виде
\eqn{x = \pm \hr{\frac{d_1}{\be}+\frac{d_2}{\be^2} \spl \frac{d_t}{\be^t}}\be^k.}
Здесь $0\le d_i <\be$ (цифры числа), причём $d_1 >0$, а $k\in[L,U]$.
Очевидно, количество чисел в системе $F$ равно $2(\be-1)\be^{t-1}(U-L+1)+1$.

Введём обозначения:

\begin{items}{-2}
\item $\de$\т  наименьшее положительное число в $F$. Очевидно, $\de = \be^{L-1}$.
\item $\la$\т  наибольшее положительное число в $F$.
\item $\ep$\т расстояние между $1$ и следующим числом в $F$ (так называемое <<машинное $\ep$>>).
Очевидно, $\ep = \be^{1-t}$.
\end{items}

\subsection{Округление и ошибки}

Разберёмся с тем, как происходит округление вещественных чисел. Пусть у нас есть число
\eqn{0.d_1d_2\dots d_td_{t+1}d_{t+2}\dots d_{t+m}.}
Прибавим к нему число $0.0\dots0\frac\be2$ (ненулевая цифра стоит на позиции $t$), получим число
\eqn{0.p_1\dots p_tp_{t+1}\dots}
Результатом округления будет число $0.p_1\dots p_t$.

Мы хотели вычислить число $x$, но немного обсчитались и получили какое\д то другое число $x'$. Как измерить нашу ошибку?

\begin{df}
\emph{Абсолютной погрешностью} вычисления называется величина $\De(x) := |x-x'|$. \emph{Относительной погрешностью} вычисления называется
число $\de(x) := \hm{\frac{x-x'}{x}}$.
\end{df}

Ясно, что абсолютная погрешность куда менее информативна, чем относительная. Если $x\sim 10^{100}$, и мы ошиблись на $\pm10^{4}$,
то это не так уж плохо, а вот если $x \sim 10^{4}$, то мы, грубо говоря, ничего не вычислили.

\begin{note}
На относительно древних процессорах можно было наблюдать следующее забавное явление, связанное с порядком вычислений:
\eqn{(A+1)-A \neq A+(1-A),}
где $A = 2^{40}$, $\be=2$, а $t=40$. При этом левая часть выражения могла принимать значения $0$ и $2$, а правая всегда равна $1$.
\end{note}

Но это еще не самое страшное, что в жизни бывает. А ещё бывает потеря значащих цифр. Пример:


\newbox\plusbox
\setbox\plusbox\hbox{\texttt{-}}

\newbox\digitbox
\setbox\digitbox\hbox{\texttt{0.12345666}}

\medskip

{\baselineskip=\ht\digitbox
\advance\baselineskip by 2pt
\centerline{\hbox to \wd\digitbox{\hss\smash{\lower.7\ht\plusbox\copy\plusbox}\texttt{0.12345666}}}%
\centerline{\hbox to \wd\digitbox{\lower1.2pt\hbox{\vrule width\wd\digitbox height.4pt}\hss\texttt{0.12345665}}}%
\centerline{\hbox to \wd\digitbox{\texttt{0.00000001}}}}

\smallskip

А что будет после нормализации? Единица переползёт на первую позицию после запятой, получится число $0.10000000\cdot 10^{-7}$.
А за ней, естественно, будут нули. Но эти нули там появились совершенно спонтанно, и формально говоря, на их месте могли
бы быть любые другие знаки (это были те самые нули, которые \emph{до} вычитания находились за пределами разрядной сетки).
В результате в данном примере точность наших вычислений падает в $10^7$ раз.


Есть и другие интересные примеры того, как ничтожная ошибка в вычислениях может колоссально повлиять на результат.
Рассмотрим многочлен $P_{20} = \prodl{n=1}{20}{(x-n)}$. Он, конечно же, имеет 20 вещественных корней. Но предположим, что
мы слегка наврали, вычисляя коэффициент при $x^{19}$, и вместо 210 получили $210+2^{-23}$. Казалось бы, ерунда. Ан нет:
$x_1 = 1.000\dots$, $x_2 = 2.000\dots$, а вот десятый и одиннадцатый корни у нового многочлена уже комплексные,
и их мнимая часть весьма далека от нуля: $x_{10,11} = 10.095\pm 0.643i$.

\section{Аппроксимация функций}

\subsection{Интерполяция многочленом Лагранжа}

\subsubsection{Постановка задачи и оценка её сложности}

Пусть $f\cln[a,b]\ra \R$\т некоторая функция, и нам известны её значения в точках $x_1<\dots< x_n\in [a,b]$. Задача интерполяции
состоит в приближении данной функции на отрезке $[a,b]$. Хорошо известно, что существует и притом единственный многочлен
(Лагранжа) степени $n-1$, проходящий через заданные $n$ точек и принимающий в них заданные значения.

Рассмотрим базисные функции
\eqn{\ph_j(x) := \prods{i\neq j}\frac{x-x_i}{x_j-x_i}.}
Тогда многочлен имеет вид
\eqn{L_n(x) := \suml{j=1}{n} f(x_j) \ph_j(x).}

Через $\om_n(x)$ будем всегда обозначать многочлен такого вида: $\om_n(x) := \prodl{j=1}{n}(x-x_j)$.

Оценим число операций, необходимых для вычисления многочлена Лагранжа в точке $x$. Если делать совсем тупо,
будет $4n(n-1)+n \sim 4 n^2$ операций. Можно схалтурить, домножив числитель и знаменатель каждой дроби $\ph_j(x)$ на
$(x-x_j)$, тогда числитель можно будет вычислить однократно, тем самым получим $2n^2$ операций. А если вспомнить про
запись многочлена в форме Ньютона, то можно ещё немного наэкономить, затратив $\frac32n^2$ операций.

\subsubsection{Оценка погрешности приближения функции многочленом Лагранжа}

Везде, где не оговорено противное, норма $\hn{\cdot}$ обозначает обычную равномерную норму в $\Cb[a,b]$.

\begin{note}
Многочлен Лагранжа можно применять для интерполяции только внутри отрезка $[a,b]$.
Пытаться экстраполировать функцию вне отрезка этим многочленом недопустимо!
\end{note}

\begin{theorem}\label{thm:LargangeErrorTheorem}
Пусть функция $f$ такова, что $f^{(n)}\in\Cb[a,b]$. Тогда имеет место формула для погрешности:
\eqn{\hn{f-L_n} \le \frac{f^{(n)}(\xi)}{n!}\hn{\om_n(x)}.}
\end{theorem}
\begin{proof}
Рассмотрим разность
\eqn{\ph(t):= f(t)- L_n(t) - k\cdot \om_n(t).}
Подберём константу $k$ так, что функция $\ph$ имела бы нуль в некоторой точке $x$. А именно, положим
\eqn{k := \frac{f(x)-L_n(x)}{\om_n(x)}.} Тогда функция $\ph(t)$ будет иметь $n+1$ нуль, \те точки $\hc{x,x_1\sco x_n}$.
Значит, по теореме Ролля $\ph'(t)$ имеет $n$ нулей, а $\ph^{(n)}(t)$ имеет хотя бы один нуль\т некоторую точку $\xi_x$.
Дифференцируя формулу для $\ph$, получаем
\eqn{\ph^{(n)}(t) = f^{(n)}(t)-k\cdot n!}
Подставляя значение $k$, получаем
\eqn{f(x)-L_n(x) = \frac{f^{(n)}(\xi_x)}{n!}\om_n(x).}
Остаётся навестить на эту формулу знак нормы, и заявить, что теорема доказана.
\end{proof}

\begin{note}
Мы видим, что качество приближения зависит от функции $\om_n$, а фактически\т от выбора узлов.
Далее мы узнаем, что если их выбирать особым образом, то можно добиться того, чтобы $\hn{f-L_n} \ra 0$
при $n\ra\bes$. Для равномерно распределённых узлов есть функция $\frac{1}{1+25x^2}$,
для которой погрешность неограниченно возрастает при $n\ra\bes$. Связано это с тем, что у неё есть полюса $\pm \frac{i}{5}$.
\end{note}

\subsubsection{Многочлены Чебышёва}

Для начала заметим, что существует многочлен со старшим коэффициентом $1$,
имеющий на заданном отрезке $n$ корней и обладающий наименьшей
равномерной нормой. Действительно, такой многочлен имеет вид
\eqn{\prodl{i=1}{n}(x-x_i), \quad x_i \in [a,b],}
то есть ограничение на нули замкнутое (и даже компактное). Далее, норма на отрезке, очевидно,
непрерывно зависит от этих нулей, а потому где\д то достигает минимума.

Поиском таких многочленов мы сейчас и займёмся. Точнее говоря, искать мы ничего не будем, а просто
<<с потолка>> предъявим ответ и докажем, что это в точности то, что нам нужно. Рассмотрим так называемые
\emph{многочлены Чебышева}, задаваемые рекуррентным соотношением
\eqn{\label{eqn:Cheb}T_0(x) := 1, \quad T_1(x) := x, \quad T_{n+1}(x) := 2xT_n(x) - T_{n-1}(x).}

Они обладают некоторыми очевидными свойствами:

\begin{nums}{-1}
\item $\deg T_n = n$;
\item $T_{2n}$\т чётная функция, $T_{2n+1}$\т нечётная функция (следует из рекуррентной формулы).
\item $T_n = 2^{n-1}x^n + \dots$
\item $T_n = \cos(n\arccos x)$. В самом деле, по известной тригонометрической формуле имеем
\eqn{\cos\br{(n+1)\arccos x} + \cos\br{(n-1)\arccos x} = 2x \cos(n \arccos x).}
\end{nums}

Рекуррентное соотношение \eqref{eqn:Cheb} позволяет найти явную формулу для $T_n$. Будем искать решение в виде
$T_n \bw= \mu^n$. Имеем
\eqn{\mu^{n+1} -2x\mu^n +\mu^{n-1}=0,}
откуда после сокращения на $\mu^{n-1}$ имеем
$\mu_{1,2} = x\pm \sqrt{x^2-1}$. Отсюда $T_n = C_1 \mu_1^n + C_2 \mu_2^n$.
Из начальных условий $T_0(x)=1$, $T_1(x)=x$ получаем систему уравнений на $C_i$:
\eqn{\case{C_1 + C_2 = 1,\\
C_1 \mu_1 + C_2 \mu_2 =x.}}
Очевидно, $C_i = \frac12$, а потому
\eqn{T_n(x) = \frac12\hs{\hr{x+\sqrt{x^2-1}}^n + \hr{x-\sqrt{x^2-1}}^n}.}

Найдем в явном виде нули многочленов $T_n$. Рассмотрим уравнение $\cos(n\arccos x)=0$. Имеем
\eqn{n\arccos x = \frac\pi2 + k\pi.}
Отсюда $\arccos x = \frac{\pi}{2n} + \frac{k\pi}{n}$. Далее берём косинус левой и правой части, получаем ответ:
\eqn{x_k = \cos\frac{(2k+1)\pi}{2n}.}
Также легко видеть, что максимумы и минимумы многочлена $T_n$ находятся в точках $\cos\frac{k\pi}{2n}$.

\begin{note}
Многочлены Чебышёва образуют ортогональную систему на отрезке $[-1,1]$ с весом $\frac{1}{\sqrt{1-x^2}}$.
\end{note}

Кроме рекуррентного  соотношения, с помощью которого мы определяли многочлены Чебышева, есть ещё одно
(читателю предлагается в него либо поверить, либо проверить самостоятельно):
\eqn{T_{3n}(x) = 4T_n(x) T_n\hr{x-\frac{\sqrt3}{2}} T_n\hr{x+\frac{\sqrt3}{2}}.}

Рассмотрим семейство многочленов со старшим коэффициентом $1$:
\eqn{K_n := \hc{P_n = x^n + \dots \in \R[x], \quad \deg P_n = n}.}

\begin{theorem}
Среди многочленов из $K_n$ многочлен $\ol T_n := \frac{1}{2^{n-1}}\cdot T_n$ обладает минимальной равномерной нормой.
\end{theorem}
\begin{proof}
От противного: пусть нашёлся многочлен $P \in K_n$ такой, что $\hn{P} < \hn{\ol T_n}$. Рассмотрим разность этих многочленов
$R := \ol T_n - P_n$. Имеем $\deg R < n$, так как $x^n$ сократится. Заметим, что в точках $x_k := \cos \frac{k\pi}{n}$
имеем $\sgn R(x_k) = \sgn T_n(x_k)$, потому что эти точки являются экстремумами многочленов Чебышёва (и потому в
них многочлен Чебышёва достигает своей нормы), а норма $P$ строго меньше, значит, при вычитании знак не поменяется.
Теперь заметим, что в этих точках знаки экстремумов чередуются. Значит, есть $n+1$ точка, где разность $R$ меняет знак,
то есть имеет не менее $n$ корней. Но $\deg R < n$, поэтому $R\equiv0$. Теорема доказана.
\end{proof}

А вот графики многочленов Чебышева $T_n$ при $n\in \hc{1\sco9}$:

\centerline{\epsfbox{pictures.1}\;\epsfbox{pictures.2}\;\epsfbox{pictures.3}\;\epsfbox{pictures.4}\;\epsfbox{pictures.5}\;
\epsfbox{pictures.6}\;\epsfbox{pictures.7}\;\epsfbox{pictures.7}\epsfbox{pictures.8}\epsfbox{pictures.9}}


Мы строили многочлены Чебышёва на отрезке $[-1,1]$. Однако нам может потребоваться
перетащить многочлен Чебышёва на любой другой отрезок. Это можно сделать с помощью линейной
замены ($x \in [-1,1]$ и $y \in [a,b]$):
\eqn{x = \frac{2y - (a+b)}{b-a}.}
Тогда корни нового многочлена получатся такие:
\eqn{y_j = \frac{a+b}{2} + \frac{b-a}{2}\cos\frac{(2j-1)\pi}{n}, \quad j = 0\sco n-1.}

Таким образом, для произвольного отрезка $[a,b]$ мы получаем формулу для оценки погрешности
при интерполяции многочленом Лагранжа:
\eqn{\label{eqn:LagrangeErrorEstimation}
\hn{f - L_n}_{\Cb[a,b]} \le \frac{\hn{f^{(n)}}}{n!}\cdot \frac1{2^{n-1}}\cdot\hr{\frac{b-a}{2}}^n.}

\subsection{Тригонометрическая интерполяция}

\subsubsection{Дискретное преобразование Фурье}

Сопоставим функции $f\cln[0,1]\ra \R$ из пространства $L_1$ её ряд Фурье:
\eqn{f(x) \sim \sums{k \in \Z} a_k e^{2\pi i k x}.}
Будем считать, что функция такова, что
\eqn{\sums{k}\hm{a_k} < \bes.}
Зафиксируем некоторое число $N \in \N$. Положим $h:=\frac1N$. Рассмотрим равномерную сетку $\Om_n$ из $N$ узлов
на отрезке $[0,1]$, \те \eqn{\Om_n  := \hc{x= n\cdot h, \quad n=0\sco N-1}.}

Пусть $ k =mN + j$, где $0\le j < N$, а $x = nh \in \Om_n$. Тогда можно переписать ряд в виде
\mln{\sums{k \in \Z} a_k e^{2\pi i k x} = \sums{k \in \Z} a_k e^{2\pi i k (nh)} =
\sums{m\in \Z} \suml{j=0}{N-1} a_{mN+j} e^{2\pi i(mN+j) nh} \stackrel!=\\
\stackrel!= \suml{j=0}{N-1} \sums{m\in \Z} a_{mN+j} e^{2\pi i(mn)} e^{2\pi i j(nh)} =
\suml{j=0}{N-1} \bbr{\sums{m\in\Z} a_{mN+j}} e^{2\pi i j x}.}
В переходе, отмеченном <<!>>, мы воспользовались абсолютной сходимостью, а потом\т тем, что
$Nh=1$ и $e^{2\pi i (mn)}=1$.

Обозначим
\eqn{f_j := \sums{m\in\Z} a_{mN+j}.}
Таким образом, если ряд для функции $f$ сходится абсолютно в точках сетки $\Om_n$, то для $x\in \Om_n$ имеет место формула
\emph{дискретного преобразования Фурье}:
\eqn{f(x) = \suml{j=0}{N-1}f_j e^{2\pi i jx}.}
Ясно, что эта формула будет работать и для функций, заданных лишь на дискретном множестве $\Om_n$:
для этого нужно взять функцию, задать её в узлах, а далее продолжить константой на каждый отрезок
и для неё уже взять разложение Фурье.

\subsubsection{Быстрое дискретное преобразование Фурье}

Найдём коэффициенты $f_k$ разложения функции $f$ по экспонентам. Рассмотрим дискретное скалярное произведение
\eqn{(f,g) := \suml{n=0}{N-1} h f(nh) \ol g(nh),}
где $h$\т расстояние между узлами.

\begin{stm}
Функции вида $e_k(x) := e^{2\pi ik x}$ ортогональны при разных $k$.
\end{stm}
\begin{proof}
Имеем
\eqn{(e_k,e_l) = \hr{e^{2\pi ik x},e^{2\pi i lx}} = \sum h e^{2\pi i (k-l)(nh)}.}
Если $k=l$, то $(e_k,e_l) = 1$, а если $k\neq l$, то получается геометрическая прогрессия, в формуле для которой в числителе
стоит разность двух экспонент с показателями, кратными $2\pi i$. Значит, при $k\neq l$ имеем $(e_k,e_l) =0$.
\end{proof}

У нас есть формулы для прямого и обратного преобразования Фурье:
\eqn{f_k = (f,e_k) = \suml{n=0}{N-1} h f(nh) e^{-2\pi ik (nh)}, \quad f(nh) = \suml{k=0}{N-1} f_k e^{2\pi ik(nh)}.}


Теперь мы будем решать такую задачу: по известным коэффициентам $f_k$ хотим определить значения функции в узлах, то есть
числа $f(nh)$.


Если тупо вычислять значения коэффициентов, то мы получим сложность $O(N^2)$. Это очень медленно и на практике
не применимо. Сейчас мы получим гораздо более быстрый алгоритм, работающий за время $O(N\log N)$.

Пусть $N=N_1\cdot N_2$. Разложим число $k$ по модулю $N_1$: пусть $k = k_1N_1 + k_2$, где $0\le k_2 < N_1$.
Разложим также число $n$ по модулю $N_2$: пусть $n = n_1 N_2 + n_2$, где $0\le n_2 < N_2$.
Тогда имеем
\eqn{\label{eqn:FFT}f(nh)=\suml{k=0}{N-1}f_k e^{2\pi ik(nh)} =
\suml{k_1 =0}{N_2-1}\suml{k_2=0}{N_1-1} f_{k_1N_1+k_2}e^{2\pi i(k_1N_1+k_2)(n_1N_2+n_2)h}=
\suml{k_1 =0}{N_2-1}\suml{k_2=0}{N_1-1} f_{k_1N_1+k_2} e^{2\pi ik_1N_1n_2h}e^{2\pi ik_2 nh},}
так как произведение скобок в показателе экспоненты равно
\eqn{k_1n_1\ub{N_1N_2h}_{1} + k_1 n_2 N_1 h + k_2(\ub{n_1N_2+n_2}_n)h.}
Отсюда получаем, что \eqref{eqn:FFT} преобразовывается к виду
\eqn{f(nh)=\suml{k_2=0}{N_1-1}\hr{\suml{k_1=0}{N_2-1}f_{k_1 N_1+k_2} e^{2\pi ik_1N_1n_2h}}e^{2\pi ik_2nh}.}

Заметим, что выражение в скобках не зависит от $n_1$, а зависит только от $n_2$. Значит, его можно вычислить один
раз и подставлять в формулу $N_1$ раз при вычислении тех значений $f(nh)$, для которых $n \pmod{N_2} =  n_2$.

Итого получаем $N \cdot N_2$ операций на вычисление коэффициентов, и $N\cdot N_1$ операций на вычисление функции.
Рассмотрим случай $N_1 = N_2 = \sqrt N$. Тогда будет $O(N\cdot \sqrt N)$ операций.
Однако $\sqrt N$\т это еще не обещанный $\log N$. Но никто не мешает применить к внутренней сумме ту же процедуру,
и так далее, пока не останется одно слагаемое.
Пусть $N = 2^l$. Тогда всего будет $O(N\cdot l) = O(N \log N)$ операций.

\subsection{Разделённые разности}

\subsubsection{Определение разделённой разности и её простейшие свойства}

\begin{df}
Разделённой разностью функции $f$ нулевого порядка назовём  саму функцию $f(x)$. Разделённой разностью порядка 1
называется функция
\eqn{f(x_1;x_2) = \frac{f(x_2)-f(x_1)}{x_2-x_1}.}
Разделённая разность порядка $n$ определяется индуктивно через разделённую разность порядка $n-1$:
\eqn{f(x_1; x_2; \dots; x_n) = \frac{f(x_1;\dots;x_{n-1})-f(x_2;\dots;x_n)}{x_1-x_n}.}
\end{df}

\begin{theorem}
Имеет место формула
\eqn{f(x_1\ssc x_n) = \suml{j=1}{n} \frac{f(x_j)}{\prods{i\neq j}(x_j-x_i)}.}
\end{theorem}
\begin{proof}
Докажем индукцией по порядку разделённой разности. Имеем
\eqn{\frac{1}{x_1-x_n}\hr{\suml{j=1}{n-1} \frac{f(x_j)}{\prodl{\substack{i=1\\i\neq j}}{n-1}(x_j-x_i)}-
\suml{j=2}{n} \frac{f(x_j)}{\prodl{\substack{i=2\\i\neq j}}{n}(x_j-x_i)}} = \frac{f(x_1)}{\prodl{i=2}{n}(x_1-x_i)}+ \frac{f(x_n)}{\prodl{i=1}{n-1}(x_n-x_i)}
+ \text{ средние члены}.}
Приводя средние члены к общему знаменателю, легко\footnote{В книжке Богачёва всё это тупо и подробно расписано на двух страницах. Если интересно\т посмотрите там.}
видеть, что получилось то, что надо.
\end{proof}
\begin{imp}Разность многочлена Лагранжа и приближаемой функции выражается через разделённые разности порядка $n$.
\end{imp}
\begin{proof}
Совсем легко видеть, что
\eqn{L_n(x) = \suml{j=1}{n}f(x_j) \frac{\om_n(x)}{(x-x_j)\prods{i\neq j}(x_i-x_j)}.}
Тогда, умножив и разделив разность $f(x)-L_n(x)$ на $\om_n(x)$, а заодно поменяв знак у скобки $(x-x_j)$ в знаменателе,
получаем
\eqn{\label{eqn:LagrApprox}f(x)-L_n(x) = \hs{\frac{f(x)}{\om_n(x)} + \suml{j=1}{n}\frac{f(x_j)}{(x_j-x)\prods{i\neq j}(x_j-x_i)}}\om_n(x) = f(x;x_1\ssc x_n)\om_n(x),}
что и требовалось. Вспоминая формулу для погрешности приближения многочленом Лагранжа, получаем, что
\eqn{f(x;x_1\ssc x_n) = \frac{f^{(n)}(\xi)}{n!}.}
\hfill\end{proof}

\subsubsection{Интерполяционная формула Ньютона}

Пусть  нам даны точки $x_1\sco x_{m+1}$ и заданы значения функции $f(x)$ в этих точках. Рассмотрим также многочлен Лагранжа по
точкам $x_1\sco x_m$ (просто выкинем последний узел). Тогда, очевидно, многочлен $D(x) := L_{m+1}(x)-L_m(x)$ имеет нули в
точках $x_1\sco x_m$, а кроме того, $\deg D = m$. Значит, он пропорционален многочлену $\om_n(x)$.
Пусть $D(x) = A_m\om_m(x)$. Теперь вычислим константу $A_m$.
Применим формулу~\eqref{eqn:LagrApprox} к многочлену $L_{m+1}$, приближая его многочленом $L_m$ и подставляя $x=x_{m+1}$:
\eqn{L_{m+1}(x_{m+1})-L_m(x_{m+1}) = A_m \om_m(x_{m+1}) = L_{m+1}(x_1\ssc x_{m+1})\om_m(x_{m+1}).}
Осталось заметить, что $L_{m+1}(x_1\ssc x_{m+1}) = f(x_1\ssc x_{m+1})$ (потому что разделённая разность
зависит только от значений в узлах), поэтому
в общем случае имеем $A_m = f(x_1\ssc x_{m+1})$.

Теперь представим многочлен $L_n(x)$ в виде
\eqn{L_n = L_1 + (L_2-L_1) + (L_3-L_2) \spl (L_n-L_{n-1}).}
Применяя предыдущие рассуждения, получаем \emph{интерполяционную формулу Ньютона}:
\eqn{L_n(x) = f(x_1) + f(x_1;x_2)\om_1(x) \spl f(x_1\ssc x_{n-1})\om_{n-1}(x).}

Из этой формулы видно, как можно ускорить вычисление многочлена Лагранжа.
Мы видим, что нам просто нужно вычислить набор разделённых разностей.
Будем вычислять их по такой схеме:
$$\epsfbox{pictures.10}$$
На это уйдёт по 2 вычитания и по одному делению на каждую разность. Итого $3$ операции.
А всего этих разностей $\frac{n^2}{2} +O(n)$.
Значит, всего будет $3\cdot\frac{n^2}{2} = \frac32 n^2$ операций. Экономия грошовая, но всё равно приятно.

\subsubsection{Интерполяция с кратными узлами}

Ранее узлы $x_i$ были однократными, то есть нам были заданы только значения функции в этих узлах. Но теперь  мы хотим
большего: пусть у нас заданы значения производных в этих узлах до порядка $m_i-1$ включительно (говорят, что узел $x_i$
имеет кратность $m_i$). Сведём задачу к предыдущей: <<расклеим>> узлы, рассмотрев точки $\wt x_k := x_i + \frac{k\ep}{m_i}$, где $i=\ol{0,\,m_i-1}$.
Для таких узлов построим обычный многочлен Лагранжа. Мы знаем, что $f(x;x_1\ssc x_n) = \frac{f^{(n)}(\xi)}{n!}$. Подставляя точки $\wt x_k$
и устремляя $\ep$ к нулю, замечаем, что $\xi \ra x_i$, \те
\eqn{f(\wt x_0\ssc \wt x_{m_i-1})\ra \frac{f^{(m_i)}}{m_i!}.}
Это даёт повод для введения следующего определения:

\begin{df}
Разделённой разностью кратности $m_1$ называется величина
\eqn{f(\ub{x_1\ssc x_1}_{m_1}) := \frac{f^{(m_1-1)}(x_1)}{m_1!}.}
Определение разделённых разностей для случая многих узлов с кратностями аналогично.
\end{df}

Формулу для многочлена Лагранжа с кратными узлами проще всего выписать, расклеив узлы, а потом склеив их заново.
При этом получится такое выражение:
\eqn{\begin{aligned}
L_{m_1\spl m_n}(x) & = f(x_1) + f(x_1;x_1)(x-x_1) \spl f(\ub{x_1\ssc x_1}_{m_1})(x-x_1)^{m_1-1} + \\
& + f(x_1\ssc x_1; x_2)(x-x_1)^{m_1} + f(x_1\ssc x_1; x_2;x_2)(x-x_1)^{m_1} (x-x_2) + \dots
\end{aligned}}
Её можно получить так: выписать обычную формулу (считая узлы однократными), а потом склеивать множители
в степени.

\subsection{Наилучшее приближение в нормированных пространствах}

\subsubsection{Общая теория}

Пусть $R$\т линейное нормированное пространство (ЛНП). Рассмотрим $f \in R$.
Пусть $g_1\sco g_n \in R$, причём $g_i$ линейно независимы.
Мы хотим приблизить вектор $f$ линейной комбинацией $\sum c_i g_i$.

\begin{df}
Положим $\De = \inf \hn{f - \sum c_jg_j}$.
Если существует вектор $c = (c_1\sco c_n)$, на котором этот $\inf$ достигается,
то элемент $\sum c_j g_j$ называется элементом наилучшего приближения.
\end{df}

Введём обозначение $F_f(c) = \hn{f - \sum c_j g_j}$.

\begin{theorem}
Элемент наилучшего приближения всегда существует.
\end{theorem}
\begin{proof}
Покажем, что $F_f$ непрерывна. В самом деле,
$$\bbm{ \hn{f - \sum c_j^1 g_j} - \hn{f - \sum c_j^2 g_j}} \le
\hn{\sum (c_j^1 - c_j^2) g_j} \le \sum \hm{c_j} \cdot \max \hn{g_j}.$$

Рассмотрим $F_0(c)$ на сфере $S^n$. Поскольку сфера\т компакт, функция $F_0$ достигает там своего
минимума. Положим $\wt F = \minl{S^n} F_0(c)$. Заметим, что $\wt F \neq 0$,
потому что иначе функции $g_i$ были бы линейно зависимыми.

Заметим, что $F_0(c) = \hm{c} \cdot F_0\hr{\frac{c_1}{\hm{c}}\sco \frac{c_n}{\hm{c}}} \ge \hm{\vec c} \cdot \wt F$.
Зафиксируем число $\ga > \frac{2 \hn{f}}{\wt F}$. Положим $B = \hc{c\cln \hm{c} \le \ga}$.
Поскольку шар\т это компакт, то на нём функция $F_f$ достигает своего минимума.
Положим $F^0 = \minl{B} F_f(c)$.
Имеем $F^0 \le F_f(0\sco 0) = \hn{f}$.
Теперь покажем, что вне этого шара значения функции $F_f$ заведомо далеки от минимума.
Имеем $$F_f(c) \ge \hn{\sum c_j g_j}- \hn{f} \ge \hm{c} \cdot \wt F - \hn{f} >
\frac{2\hn{f}\cdot \wt F}{\wt F} - \hn{f} = \hn{f} \ge F^0.$$
Это означает, что вне шара функция заведомо не достигает минимума. Значит, она достигает
его внутри шара~$B$ либо на его границе.
\end{proof}

\begin{note}
Можно доказать это немного изящнее: $\De := \infl{v\in V} \hn{f-v}$. Выберем последовательность $v_n$,
для которой $\hn{f - v_n} \searrow \De$. Можно считать, что последовательность $\hn{v_n}$ ограничена, так как
для достаточно больших $n$ будет выполнено
\eqn{\hn{v_n}\le \hn{f}+ \hn{v_n-f} \le \hn{f}+\De + 1.}
Выберем из неё сходящуюся подпоследовательность (так как ограниченное множество в $\R^n$ предкомпактно). Её предел и есть искомый элемент.
\end{note}

\begin{df}
Пространство $R$ называется \emph{строго нормированным}, если из того, что $\hn{f+g} = \hn{f} + \hn{g}$
следует, что элементы $f$ и $g$ пропорциональны.
\end{df}

\begin{theorem}
В строго нормированном пространстве элемент наилучшего приближения единствен.
\end{theorem}
\begin{proof}
Допустим, что функция $F$ достигает минимума в двух точках $c'$ и $c''$.
Рассмотрим
\eqn{\hn{f - \sum \frac{c'_j + c''_j}{2} g_j} \le \frac12\hn{f - \sum c'_j g_j} + \frac12\hn{f-\sum c''_j g_j} = \frac{\De}{2} + \frac{\De}{2}.}
Но в нашем случае имеет место равенство, потому что это элемент наилучшего приближения.
Поскольку пространство является строго нормированным, получаем $f - \sum c'_j g_j = \al\hr{f - \sum c''_j g_j}$.
Если $\al = 1$, то всё доказано. А если $\al \neq 1$, то $f = \frac{\sum \hr{c'_j - \al c''_j}g_j}{1-\al}$,
то есть $f$ раскладывается по базису $\hc{g_j}$, а разложение по базису всегда однозначно.
\end{proof}


\begin{problem}
Гильбертово пространство является строго нормированным.
\end{problem}

Получим более или менее явный вид для коэффициентов $c_i$.
Рассмотрим функцию
\eqn{\Ph(c) = \hn{f - \sum c_jg_j}^2 = \hr{f - \sum c_j g_j, f - \sum c_j g_j}.}
Можно доказать, что у функции $\Ph$ только один экстремум, являющийся минимумом.
\begin{problem}
Доказать этот факт.
\end{problem}
Найдём коэффициенты, используя необходимое условие экстремума $\pf{\Phi}{c_k} = 0$.
Дифференцируя, имеем
$$\hr{-g_k, f- \sum c_i g_i} + \hr{f - \sum c_i g_i, -g_k} = 0, \quad k = 1\sco n.$$
Положим $G = (g_{ij})$, где $g_{ij} = (g_i, g_j)$.
Получаем систему линейных уравнений с матрицей $G$ и некоторым столбцом свободных членов.

Матрица $G$ называется матрицей Грама системы функций $g_1\sco g_n$.
\begin{stm}
Матрица $G$ является положительно определённой.
\end{stm}
\begin{proof}
Имеем
$\hn{\sum c_j g_j}^2 = (c \cdot g, c \cdot g) = \sums{i,j} c_i c_j g_{ij} = (Gc, c) \ge 0$.
При этом если выражение справа равно нулю, то и норма слева равна нулю, а это означает,
что либо векторы $g_1\sco g_n$ линейно зависимы, что невозможно, либо $c = 0$. Это и означает
положительную определённость.
\end{proof}

Однако число обусловленности матрицы Грама для многочленов $g_j = x^{j-1}$ на отрезке
$[-1,1]$ растёт очень быстро с ростом $n$. Именно,
$$\frac{\la_{\max}(G)}{\la_{\min}(G)} \sim \const \cdot\frac{(\sqrt2 + 1)^{2n}}{n^b}, \quad b > 0.$$

\subsubsection{Наилучшее приближение многочленами. Чебышёвский альтернанс}

Теперь мы будем рассматривать пространство непрерывных функций $\Cb[a,b]$ с чебышёвской нормой.
В качестве базисных векторов выберем многочлены:
$Q_n(x) = \suml{k=0}{n} a_k x^k$.

Через $Q_n^0$ обозначим многочлен наилучшего равномерного приближения функции $f$.
Также будем обозначать
\eqn{\begin{aligned}
\De_n(f) &:= \hn{f - Q_n^0} \le \hn{f - Q_n}, \quad Q_n \in \R[x]_{\le n},\\
\mu & := \minl{i} \hm{f(x_i) - Q_n(x_i)}.
\end{aligned}}

\begin{theorem}[Валле\ч Пуссен]
Пусть существуют $n+2$ точки $x_0 <\ldots<x_{n+1}$, для которых
\eqn{\sgn \br{f(x_i) - Q_n(x_i)}\cdot (-1)^i = \const.}
Тогда $\De_n(f) \ge \mu$.
\end{theorem}
\begin{proof}
Если $\mu = 0$, то доказывать нечего.
Пусть теперь $\mu > 0$. Допустим, что $\mu > \De_n(f)$.
Рассмотрим
\eqn{\sgn\bs{\ub{\br{f(x)-Q_n(x)}}_{D} - \ub{\br{f(x)-Q_n^0(x)}}_{D_0}}.}
Так как $|D_0| \le \De_n(f)$, по нашему предположению при $x = x_i$ имеем $|D| > |D_0|$.
Значит, при вычитании $D_0$ из $D$ знак не поменяется, и
\eqn{\sgn\bs{\br{f(x_i)-Q_n(x_i)} - \br{f(x_i)-Q_n^0(x_i)}} = \sgn \br{f(x_i) - Q_n(x_i)}.}
Поэтому многочлен $D-D_0 = Q_n(x) - Q_n^0(x)$ имеет $n+1$ перемену знака, а с другой стороны, имеет степень не выше $n$.
Значит, $Q_n \equiv Q_n^0$, поэтому $\De_n(f) = \max |f(x)-Q_n^0(x)| \ge \mu = \min |f(x_i)-Q_n^0(x_i)|$.
\end{proof}

\begin{df}
Точки $x_0\sco x_{n+1}$ называются точками \emph{чебышёвского альтернанса}.
\end{df}

\begin{theorem}[Чебышёв]
Многочлен $Q_n$ является многочленом наилучшего равномерного приближения на отрезке $[a,b]$ тогда и только
тогда, когда на этом отрезке существует $n+2$ точки $x_k$ такие, что
\eqn{f(x_k) -Q_n(x_k) = \al \cdot (-1)^k \hn{f-Q_n}, \quad \al \in \hc{\pm 1}.}
\end{theorem}
\begin{proof}

\framebox{$\Lar$}
Пусть $\hc{x_k}$\т данные точки. Пусть $L := \hn{f-Q_n}$. Так как в точках $x_k$ достигается норма разности,
то $L = \mu$ (в самом деле, $|f(x_k) -Q_n(x_k)| = |\al| \cdot \hn{f-Q_n} = \hn{f-Q_n} = L$).
Далее, имеем
\eqn{L = \mu \le \De_n(f) \le \hn{f-Q_n} = L.}
Значит, $L = \De_n(f)$. Следовательно, $Q_n$\т многочлен наилучшего равномерного приближения.

\framebox{$\Ra$} Пусть $Q_n$\т многочлен наилучшего равномерного приближения. Рассмотрим ближайшую к $a$ точку,
в которой достигается норма разности, \те положим
\eqn{y_1 := \min\hc{x\in [a,b]\cln |f(x)-Q_n(x)| =L}.} Для определённости $f(y_1)-Q_n(y_1) = L$. Теперь положим
$y_2 := \min\hc{x\in [y_1,b]\cln f(x)-Q_n(x) =-L}$, и так далее. Если мы набрали $m := n+2$ таких точек, то мы победили.
Допустим теперь, что $m < n+2$. По непрерывности при $k=2\sco m$ найдутся точки $z_{k-1} \in [y_{k-1},y_k]$ такие, что
$|f(x)-Q_n(x)| < L$ при $x\in [z_{k-1},y_k)$. Положим $z_0 := a$, $z_m = b$.

Рассмотрим многочлен $v(x) := \prodl{j=1}{m-1}(z_j-x)$. Рассмотрим  отрезок $[z_0,z_1]$. Имеем $v(x) > 0$ при $x\in [z_0,z_1)$.
Рассмотрим многочлен $Q_n^d(x) := Q_n(x)+d v(x)$, где $d > 0$. Мы знаем, что $f(x)-Q_n(x) > -L$ на отрезке $[z_0,z_1]$.
Значит, при достаточно малом $d$ будем иметь $f(x)-Q_n^d(x)  > -L$ на $[z_0,z_1)$. Кроме того,
$|f(z_1)-Q_n^d(z_1)| \bw= |f(z_1)-Q_n(z_1)| <L$. Значит, на всём отрезке $[z_0,z_1]$ имеем $|f(x)-Q_n^d(x)| < L$.
Подбирая подходящее значение $d$ для каждого отрезка $[z_{k-1},z_k]$ и выбирая минимум по всем таким $d$, получаем, что
многочлен $Q_n^d$ доставляет лучшее приближение, чем $Q_n$. Противоречие.
\end{proof}

\begin{petit}
Кажется, следующую теорему мы уже доказывали в общем случае. Во всяком случае идея ровно та же самая\т рассмотрение полусуммы.
Кому\д то может показаться, что общее доказательство не проходит, так как пространство непрерывных функций на отрезке не гильбертово.
Но мы рассматриваем только значения в узлах, поэтому фактически имеем дело с конечномерным пространством, которое, ежу ясно, строго
нормированное.
\end{petit}

\begin{theorem}
Многочлен наилучшего равномерного приближения степени $n$ для функции $f$ единственный.
\end{theorem}
\begin{proof}
Допустим, нашлось два многочлена $P$ и $Q$. Тогда
\eqn{\hm{f - \frac{P+Q}2} = \hm{\hr{\frac f2 - \frac P2} +\hr{\frac f2 - \frac Q2}} \le \frac12 \hn{f-P} + \frac12 \hn{f-Q}= \De_n(f).}
Значит, $R := \frac{P+Q}{2}$ тоже является многочленом наилучшего равномерного приближения.
Пусть $x_0\sco x_{n+1}$\т точки альтернанса многочлена $R$. Тогда
\eqn{\hm{f(x_i) - \frac{P(x_i)+Q(x_i)}{2}} = \De_n(f),} то есть
\eqn{\label{eqn:Uniqueness}\hm{\br{f(x_i) - P(x_i)}+\br{f(x_i)-Q(x_i)}} = 2\De_n(f).} Но так как
\eqn{\hm{\br{f(x_i) - P(x_i)}+\br{f(x_i)-Q(x_i)}} \le |f(x_i)-P(x_i)|+ |f(x_i)-Q(x_i)| \le \De_n(f) + \De_n(f),}
откуда следует, что $\hm{f(x_i) - P(x_i)} = \hm{f(x_i) - Q(x_i)}$. Но модули можно убрать, ибо, если
бы эти выражения отличались знаком, то в левой части равенства~\eqref{eqn:Uniqueness} получился бы нуль.
Таким образом, получаем $P(x_i) = Q(x_i)$, то есть два многочлена степени $n$ совпадают в $n+2$ точках.
Значит, $P\equiv Q$.
\end{proof}

Пусть задана функция $f$ и пусть $Q_n^0$\т её МНРП.

\begin{stm}
$Q_n^0$ совпадает с некоторым многочленом Лагранжа по $n+1$ точке для функции $f$.
\end{stm}
\begin{proof}
В самом деле, если $Q_n^0$\т МНРП, то по теореме Чебышёва получаем, что существуют $n+2$
точки альтернанса. Значит, у разности $f(x) - Q_n^0(x)$ имеется хотя бы $n+1$ нуль.
Итак, существуют точки $\xi_1\sco\xi_{n+1}$, для которых имеем $Q_n^0(\xi_i) = f(\xi_i)$.
Это и требуется доказать.
\end{proof}

А раз $Q_n^0$\т это многочлен Лагранжа, то мы можем применить рассуждения
из доказательства теоремы~\ref{thm:LargangeErrorTheorem}, и заявить, что имеет место равенство
\eqn{f(x) - Q_n^0(x) = \frac{f^{(n+1)}(\xi_x)}{(n+1)!}\om_{n+1}(x),}
где $\om_{n+1}(x) = (x-\xi_1)\sd(x-\xi_{n+1})$.
А поскольку аналогичная формула верна, вообще говоря, для любого многочлена Лагранжа,
а $Q_n^0$\т МНРП, то имеет место оценка
\eqn{\hn{f - Q_n^0} \le \frac{\maxl{\xi}\hm{f^{(n+1)}(\xi_x)}}{(n+1)!}\hm{\om_{n+1}(x)}.}
Ну а теперь справа можно выбрать в качестве $\om_{n+1}$ нули многочлена Чебышёва, для которого известна
оценка. Тогда получим такое неравенство:
\eqn{\hn{f - Q_n^0} \le \frac{\hn{f^{(n+1)}}}{(n+1)!}\cdot \frac{1}{2^n} \cdot \hr{\frac{b-a}{2}}^{n+1}.}
Используя теперь то свойство, что многочлены Чебышёва наименее уклоняются от нуля,
можно получить и обратную оценку:
\eqn{\hn{f - Q_n^0} \ge \frac{\min\hm{f^{(n+1)}(x)}}{(n+1)!}\cdot \frac{1}{2^n} \cdot \hr{\frac{b-a}{2}}^{n+1}.}

\subsubsection{Примеры многочленов наилучшего приближения}

\begin{ex}
Пусть мы приближаем непрерывную функцию на отрезке $[a,b]$ многочленом нулевой степени. Пусть $M:= \max f$, $m := \min f$.
Тогда, очевидно, $Q(x) = \frac{M+m}{2}$, а точками альтернанса (их будет 2 штуки) будут какие-нибудь точки достижения максимума
и минимума.
\end{ex}

\begin{ex}
Пусть нам дана непрерывная функция $f$, выпуклая (вверх) на некотором отрезке $[a,b]$. Тогда её наилучшим линейным
приближением будет функция $Q(x)= kx+c$, где $k = f(a;b)$. Точки $a$ и $b$ будут точками альтернанса, так как экстремум
функции на отрезке может быть только один. Если функция $f$ дифференцируема, то найдётся такая  точка $\xi$
(третья точка альтернанса), что касательная $y_2(x)$ к графику функции $f$ параллельна прямой $y_1(x) = k(x-a)+f(a)$.
Число $c$ выбирается так, что график $Q(x)$ проходит в точности по середине между графиками $y_1(x)$ и $y_2(x)$,
то есть $Q(x) = \frac{y_1(x)+y_2(x)}{2}$.
$$\epsfbox{pictures.11}$$
\end{ex}

\begin{ex}
Рассмотрим функцию, у которой очень много нулей, например, $f(x) = \sin 10\pi x$.
Тогда $Q_3^0 \equiv 0$. Действительно, взяв нулевой многочлен, мы без труда найдём $5$ точек
альтернанса. Применяя теорему Чебышёва, получаем, что $Q_3^0$\т МНРП.
\end{ex}

\begin{ex}
Будем приближать многочлен $f(x) := x^7$ с помощью многочлена $Q_6^0$. Нам нужно минимизировать
норму многочлена (7\д й степени) $g := f- Q_6^0$. Ежу ясно, что для этой цели нужно
взять приведённый многочлен Чебышёва, потому что он наименее уклоняется от нуля.
\end{ex}

\begin{problem}
Показать, что если приближать (на симметричном отрезке) чётную функцию,
то МНРП будет чётной функцией. Аналогично, если мы приближаем нечётную функцию,
то МНРП будет нечётной функцией.
\end{problem}


\begin{ex}
Рассмотрим предыдущий пример, но пусть теперь нам нужно приблизить многочлен $x^7$
многочленом 5\д й степени на отрезке $[-1,1]$ и оценить погрешность приближения. Если всё написать в лоб
по той формуле, которую мы вывели, то получается такая оценка:
\eqn{\hn{f - Q_5^0} \le \frac{7!\cdot 2^6}{6!\cdot 2^{11}} = \frac{7}{32}.}
Но можно схитрить. За счёт того, что мы приближаем нечётную функцию,
мы можем считать, что на самом деле мы приближаем многочленом $Q_6^0$, ибо
он всё равно на самом деле окажется степени на единицу меньше в силу сформулированной задачи
(много ли мы знаем многочленов 6\д й степени, которые являются нечётными функциями?).
Значит, $Q_5^0 = Q_6^0$, а для многочленов 6\д й степени оценка точнее:
\eqn{\hn{f - Q_6^0} \le \frac{7!\cdot 2^7}{7!\cdot 2^{13}} = \frac{1}{64}.}
Видим, что оценка получается лучше в $14$ раз.
\end{ex}


\subsection{Ортогональные системы и их свойства}

\subsubsection{Гильбертовы пространства. Процесс ортогонализации}

Пусть $H$\т гильбертово пространство со скалярным произведением $(\cdot,\cdot)$ и индуцированной нормой
$$\hn{f} := \sqrt{(f,f)}.$$

Пусть $p\cln [a,b] \ra \R$, $p \ge 0$ на $[a,b]$, $p$ имеет лишь конечное количество нулей
и ограничена на $[a,b]$. Тогда
$$
\begin{aligned}
&H = \hc{f \cln [a,b] \ra \Cbb\cln \hn{f} < \infty}\\
&(f,g) := \intl ab p f \ol g\, dx
\end{aligned}
$$
будет гильбертовым пространством. Проверка аксиом предоставляется читателю.

\begin{df}
Функция $p$ называется \emph{весом}.
\end{df}

\begin{df}
Система векторов $\hc{f_i}$ называется \emph{ортогональной}, если $(f_i,f_j) \neq 0$ при $i = j$ и $(f_i,f_j) = 0$ при $i \neq j$.
\end{df}

\begin{lemma}[Процесс ортогонализации Грама\ч Шмидта]
Пусть дано гильбертово пространство $H$.
Пусть векторы $f_1\sco f_n$ ($f_i \in H$) линейно независимы.
Построим из них ортогональную систему $g_1\sco g_n$.
\end{lemma}
\begin{proof}
Поскольку все векторы $f_i$ ненулевые, возьмём $g_1 = f_1$.
Пусть на $k$\д м шаге уже построены векторы $g_1\sco g_{k-1}$. Покажем, как строить очередной вектор $g_k$.
Будем искать его так, чтобы $\ha{f_1\sco f_k} = \ha{g_1\sco g_k}$.
Имеем условия
$$(g_k, g_i) = 0, \quad i = 1\sco k-1.$$
Попробуем найти этот вектор $g_k$ в виде
$$g_k = \la_1 g_1\spl \la_{k-1} g_{k-1} + f_k.$$
Используя условия ортогональности, получаем при всех $i = 1\sco k-1$:
$$0 = (g_k, g_i) = \la_i (g_i,g_i) + (f_k,g_i),$$
откуда $\la_i$ однозначно определяется, благо $(g_i, g_i) = \hn{g_i}^2 > 0$.
Таким образом находим очередной вектор $g_k$. Отметим, что матрица преобразования векторов $\hc{f_i}$ в систему $\hc{g_i}$
является унитреугольной.
\end{proof}

\begin{note}
При необходимости полученные вектора $g_i$ можно пронормировать, но сама процедура ортогонализации того не требует.
\end{note}

\subsubsection{Ортогональные многочлены и их свойства}

В дальнейшем мы будем работать с пространством вещественнозначных функций на отрезке $[a,b]$
со скалярным произведением
$$(f,g) = \intl ab pfg\dx,$$
потому что именно оно чаще всего используется.

\begin{ex}
Можно показать, что процедура ортогонализации стандартного базиса в $\R[x]_{\le n}$, а именно системы многочленов $\hc{1,x,x^2\sco x^n}$,
с весовой функцией $p = \frac{2}{\pi\sqrt{1-x^2}}$ даёт многочлены Чебышёва, которые удовлетворяют рекуррентной формуле
$$T_{n+1} = 2x T_n - T_{n-1}.$$
Как мы потом увидим, наличие такой рекуррентной формулы для ортогональных многочленов не является чистой случайностью.
\end{ex}

\begin{ex}
Если взять $p \equiv 1$, то процедура ортогонализации даёт многочлены Лежандра.
\end{ex}

\begin{ex}
Если брать весовую функцию равной $p = (1-x)^\al(1+x)^\be$ ($\al, \be \ge -\frac12$), то при ортогонализации
получим так называемые многочлены Якоби.
\end{ex}

Тот базис, который получается ортогонализацией Грама\ч Шмидта (относительно некоторого скалярного произведения),
будем называть системой ортогональных многочленов.
Будем использовать для обозначения системы ортогональных многочленов букву $P$.
Итак, $P_0, P_1\etc$\т ортогональные многочлены.

\begin{theorem}
Всякий ортогональный многочлен $P_n$ степени $n$ имеет ровно $n$ простых корней на интервале~$(a,b)$.
\end{theorem}
\begin{proof}
Предположим, что это не так, и на интервале $(a,b)$ многочлен имеет только $m < n$ нулей нечётной кратности,
обозначим эти нули через $x_1\sco x_m$. Тогда многочлен
$$G(x) := P_n(x) Q_m(x), \text{ где } Q_m :=  \prodl{k=1}{m}(x-x_k),$$
не меняет знак на $[a,b]$, значит,
$$I := \intl ab p(x) G(x)\dx \neq 0.$$
С другой стороны, $I = (P_n, Q_m) = 0$, потому что $m < n$, а все многочлены степени меньше $n$ ортогональны $P_n$
по построению ортогональной системы. Противоречие.
\end{proof}


\begin{theorem}
Для ортогональных многочленов выполнено рекуррентное соотношение вида
\eqn{P_{n+1}(x) = (x - \al_n) P_n - \be_n P_{n-1}.}
\end{theorem}
\begin{proof}
По определению, $P_{n+1} \bot \ha{P_0\sco P_n}$. Мы докажем, что процедуру ортогонализации
можно вести с использованием этой рекуррентной формулы. Пусть ортогональные
многочлены $P_0\sco P_n$ уже вычислены. Многочлен $P_{n+1}$, полученный
по этой формуле, имеет степень $n+1$. Осталось проверить, что при подходящем
выборе коэффициентов $\al_n$ и $\be_n$ он будет ортогонален все предыдущим.

Рассмотрим три случая. Пусть сначала $k < n-1$. Умножая выражение для $P_{n+1}$  скалярно на $P_k$, получим
\eqn{0 \stackrel?= (P_{n+1}, P_k) = (x P_n, P_k) - \al_n(P_n, P_k) - \be_n(P_{n-1}, P_k).}
Поймём, почему получится нуль. Действительно, последние два слагаемых равны нулю в силу ортогональности $P_1\sco P_n$.
Далее, $(xP_n, P_k) = (P_n,xP_k) = 0$, потому что $\deg (x P_k) < n$.

Пусть теперь $k = n-1$. Добьёмся равенства нулю за счёт выбора $\be_n$:
\eqn{0 = (P_{n+1}, P_{n-1}) = (x P_n, P_{n-1}) - \al_n\ub{(P_n, P_{n-1})}_{=0} - \be_n\ub{(P_{n-1}, P_{n-1})}_{> 0}.}
Итак, отсюда можно определить коэффициент $\be_n$, а именно
\eqn{\be_n = \frac{(x P_n, P_{n-1})}{(P_{n-1}, P_{n-1})}.}

Подставим теперь $k=n$.
\eqn{0 = (P_{n+1}, P_n) = (x P_n, P_n) - \al_n\ub{(P_n, P_n)}_{> 0} - \be_n\ub{(P_{n-1}, P_n)}_{=0}.}
На этот раз можно однозначно определить $\al_n$:
\eqn{\al_n = \frac{(x P_n, P_n)}{(P_n, P_n)}.}

Теорема доказана.
\end{proof}

\begin{petit}
Следующего утверждения на лекции доказано не было. Однако, оно используется
в доказательстве того факта, что нули соседних ортогональных многочленов
перемежаются (а этот факт был по крайней мере сформулирован на лекции).
\end{petit}

\begin{stm}
Коэффициенты $\be_n$ рекуррентного соотношения положительны.
\end{stm}
\begin{proof}
Как видно из доказательства предыдущей теоремы, достаточно доказать, что
$$(xP_n,P_{n-1}) > 0.$$
Для этого докажем лемму.
\begin{lemma}
Пусть $L\cln H \ra H$\т самосопряжённый оператор, и пусть $f_n := L^{n-1} f_1$, причём $\hc{f_n}$ линейно независимы.
Пусть векторы $\hc{g_n}$ получены из системы $\hc{f_n}$ процедурой ортогонализации. Тогда
\eqn{(L g_n, g_{n-1}) > 0 \quad\text{при всех }n.}
\end{lemma}
\begin{proof}
В силу самосопряжённости оператора $L$ имеем
\eqn{
(L g_n, g_{n-1}) = (g_n, L g_{n-1}) = \Br{g_n, Lf_{n-1} + \suml{k=1}{n-2}\la_k L f_k}=
\Br{g_n, f_n + \ub{\suml{k=1}{n-2} \la_k f_{k+1}}_{\in g_n^\bot}}=(g_n, f_n).}
Далее, пользуясь ортогональностью, можно заменить вектор $f_n$ на вектор $g_n$ (они отличаются
на вектор из $g_n^\bot$, поэтому скалярное произведение не поменяется). Но $(g_n,g_n) > 0$,
то есть $(g_n,f_n) > 0$, и лемма доказана.
\end{proof}
Теперь утверждение следует из леммы, применённой к $(Lf)(x) = xf(x)$ и $f_1 \equiv 1$.
\end{proof}

\def\z#1#2{x_{{#1}}^{{(#2)}}}

\begin{theorem}
Пусть $\bc{\z in}$\т все нули ортогонального многочлена $P_n$.
Тогда нули $P_n$ и $P_{n-1}$ перемежаются:
\eqn{a < \z1n < \z1{n-1} < \z2n < \z2{n-1} < \ldots < \z{n-1}{n-1} < \z{n}{n} < b,}
то есть нули перемежаются.
\end{theorem}
\begin{proof}
Подставим в рекуррентную формулу для многочлена $P_{n-1}$ нули многочлена $P_n$.
Первое слагаемое в правой части пропадёт, останется
\eqn{P_{n+1}(\z in) = - \be_n P_{n-1}(\z in).}
Таким образом, поскольку $\be_n > 0$, по предположению индукции $P_{n-1}(\z in) \neq 0$,
а это значит, что многочлен $P_{n+1}$ в нулях $P_n$ имеет значения в точности противоположного
знака, чем многочлен $P_{n-1}$. Заметим, что многочлены, полученные процессом ортогонализации
из стандартного базиса, имеют старший коэффициент, равный~$1$. Следовательно, при $x > \z nn$ многочлен
$P_n$ положителен и меняет знак при переходе через каждый из своих нулей.
$$\epsfbox{pictures.12}$$
Осталось заметить, что знаки $P_{n+1}$ и $P_{n-1}$ в концах отрезка совпадают, откуда следует,
что многочлен $P_{n+1}$ имеет по нулю на интервалах $(a,\z 1n)$ и $(\z nn,b)$. Но это означает,
что мы уже всё знаем про нули $P_{n+1}$\т ведь их ровно $n+1$ на интервале $(a,b)$.
\end{proof}


\subsection{Наилучшее приближение в гильбертовых пространствах}

Пусть $G_\ph \subs \Cbb$\т множество точек, из которых отрезок $[-1,1]$ виден под углом $\al \ge \ph$.

\begin{theorem}[Из Бахвалова, с.~243, без доказательства]
Пусть задана некоторая функция $f$, и пусть последовательность многочленов
\eqn{
P^m(x) = \suml{j=0}{n_m} a_j^m x^j}
удовлетворяет условиям
\eqn{\hn{f - P^m} \le \frac{1}{2^m}.}
и
\eqn{l_m := \suml{j=0}{n_m} \hm{a_j^m} \le M\cdot 2^{qm}, \quad q \ge 0, \quad M \ge 1.}
Тогда функция $f$ может быть аналитически продолжена в область $G_{\ph_0}$,
где $\ph_0 := \pi\frac{2q + 1}{2q+2}$.
\end{theorem}

Рассмотрим функцию $f(x) = \frac{1}{1 + 25x^2}$. Она имеет полюса в точках $\pm\frac{i}{5}$.
Рассмотрим такие значения угла $\ph$, при которых полюса попадают в область $G_\ph$. Поскольку
функция заведомо не будет голоморфной в этой области, неравенство для коэффициентов не может выполняться,
а это значит, что они имеют экспоненциальную скорость роста (порядка не меньше чем $2^{qm}$.
Это означает, что при вычислении погрешность будет очень велика, если использовать стандартный базис
в пространстве многочленов.

Значит, нужно придумать такой способ хранения данных, чтобы числа получались не очень большие.
Если $E$\т евклидово пространство, $\hc{e_k}$\т ортонормированный базис в $E$, и $f \in E$,
то имеет место равенство Парсеваля:
\eqn{\hn{f}^2 = \sum \hm{f_k}^2.}
Имеем
\eqn{\Br{\sumkun \hm{f_k}}^2 \le (n+1) \sumkun \hm{f_k}^2 = (n+1)\cdot \hn{f},}
\те получается оценка на рост порядка $\sumkun \hm{f_k} \sim  O(\sqrt{n})$.

\begin{petit}
Далее следовал рассказ лектора про задачу интерполяции в двумерном случае,
возникшую из практики (построение профилей дна Охотского моря).

Имеется задача, в общем случае плохо решённая. Найти разумный способ приближения
функции в квадрате при условии, что сетка задана нерегулярно, то есть в квадрат
накиданы случайные точки, и в них значения функции заданы.
\end{petit}


Из формулы оценки погрешности~\eqref{eqn:LagrangeErrorEstimation} следует, что чем меньше отрезок, на котором приближается
функция, тем лучше. Поэтому лучше всего делать так: разбиваем отрезок приближения
на несколько частей, и на каждом отрезке приближаем интерполяционным многочленом Лагранжа.
Есть только одна проблема: такая процедура гарантирует только непрерывность функции,
потому что никаких условий на согласование производной в точках стыка у нас нет.

\subsection{Сплайны}

\subsubsection{Определение сплайнов}

\begin{df}
\emph{Сплайном} $m$\д го порядка для функции $f$ называется функция $S_m(x)$ такая, что
\begin{points}{-3}
\item $S_m(x) \in \Cb^{m-1}[a,b]$;
\item $S_m(x_j) = f(x_j)$;
\item На каждом отрезке $[x_{k-1},x_k]$ $S_m$ является многочленом степени $m$, причём выполнено условие гладкости
на концах отрезка до порядка $m-1$ включительно.
\end{points}
\end{df}

Посчитаем, насколько однозначно задан сплайн. Пусть у нас $N$ отрезков, тогда неизвестных коэффициентов многочленов
имеется $N(m+1)$ штук. Условия гладкости дают $(N-1)(m-1)$ уравнение (совпадение производных во всех внутренних
точках), и ещё $2N$ уравнений мы получаем из того, что знаем значения многочленов в концах всех отрезков.
Таким образом, свободных неизвестных останется $Nm + N - Nm + N + m - 1 - 2N  = m-1$ штук.

Заметим, что при $m=1$ сплайн определён однозначно и даёт просто ломаную, соединяющую точки $(x_i, f(x_i))$.

Рассмотрим функционал
\eqn{I_k(s) := \intl{a}{b} (s^{(k)})^2\,dx.}

Рассмотрим класс функций, имеющих кусочно непрерывную производную на отрезке $[a,b]$
и в узлах принимающие заданные значения. Обозначим этот класс $K_1$.
Сплайны доставляют экстремум функционалу $I_1$ на данном классе.
Имеем $I_1(s + \de) = I_1(s) + 2\intl{a}{b} s'\de' dx + \intl{a}{b} (\de')^2dx$.
Всё будет хорошо, если мы покажем, что второе слагаемое обнуляется. Интегрируя по частям, получаем,
что $\intl ab s'\de'dx = 0 $ тогда и только тогда, когда $\intl ab s'' \de\dx = 0$.
Поскольку хочется, чтобы это было верно для всех $\de$, то по лемме Дюбуа\ч Реймона получаем, что $s'' = 0$.
А это и означает, что функция является линейной.


Для более высоких степеней проще использовать соответствующие результаты
вариационного исчисления. Для экстремалей функционала
\eqn{\Phi(x) := \intl{a}{b} F(t, x, \dot x\sco x^{(k)})dt \ra \min}
имеет место уравнение Эйлера\ч Пуассона:
\eqn{F_x - \frac{d}{dt}\hr{F_{\dot x} - \frac{d}{dt}\hr{F_{\ddot x} - \dots - \frac{d}{dt}\hr{F_{x^{(k-1)}} - \frac{d}{dt}F_{x^{(k)}}}\dots}} = 0.}
В частном случае, при $k = 2$, оно превращается в уравнение
\eqn{\frac{d^2}{dt^2} F_{\ddot x} - \frac{d}{dt}F_{\dot x} + F_x = 0.}
В нашем случае $F = (\ddot x)^2$, поэтому экстремаль функционала $I_2$ удовлетворяет уравнению
\eqn{\frac{d^2}{dx^2}s''(x) = 0,}
то есть получаем условие экстремума $s^{(4)(x)} = 0$. Это значит, что экстремаль\т многочлен степени не более~3.
Введём обозначение $h_{n+1} := x_{n+1} - x_n$.
Следовательно, $s''(x)$ имеет вид
\eqn{s''(x) = M_n \frac{x_{n+1}-x}{h_{n+1}} + M_{n+1} \frac{x-x_n}{h_{n+1}}.}
У нас есть условия
$s(x_n) = f(x_n)$ и $s(x_{n+1}) = f(x_{n+1})$. Можно показать, что в этом случае $s$ имеет
вид
\eqn{s(x) = \frac{M_n}{6} \frac{(x_{n+1} - x)^3}{h_{n+1}} + \frac{M_{n+1}}{6}\cdot \frac{(x-x_n)^3}{h_{n+1}}
+ A_n \frac{x_{n+1}-x}{h_{n+1}} + B_n \frac{x-x_n}{h_{n+1}}.}
Подставляя узлы $x = x_n$ и $x = x_{n+1}$, получаем уравнения на коэффициенты:
\eqn{A_n = f(x_n) - \frac{M_n}{6}h_{n+1}^2, \quad
B_n = f(x_{n+1}) - \frac{M_{n+1}}{6} h_{n+1}^2.}
Осталось найти $M_n$, и его мы найдём из соображений непрерывности первой производной.
Напишем формулу для производной сплайна для отрезков $[x_{n-1},x_n]$ и $[x_n,x_{n+1}]$:
\eqn{
s'(x) = -\frac{M_{n-1}}{2} \frac{(x_n-x)^2}{h_n} + \frac{M_n}{2}\frac{(x-x_{n-1})^2}{h_n} - \frac{A_{n-1}}{h_n} + \frac{B_{n-1}}{h_n},
}
\eqn{
s'(x) = -\frac{M_n}{2} \frac{(x_{n+1}-x)^2}{h_{n+1}} + \frac{M_{n+1}}{2}\frac{(x-x_n)^2}{h_{n+1}} - \frac{A_n}{h_{n+1}} + \frac{B_n}{h_{n+1}},
}
Затем подставим в оба равенства узел $x_n$:
\eqn{
s'(x_n) = \frac{1}{h_n} \hr{\frac{M_n}{3} h_n^2 - f(x_{n-1}) + \frac{M_{n-1}}{6} h_n^2 + f(x_n)}.
}
\eqn{
s'(x_n) = \frac{1}{h_{n+1}} \hr{-\frac{M_n}{3} h_{n+1}^2 - f(x_n) - \frac{M_{n+1}}{6} h_{n+1}^2 + f(x_{n+1})}.
}
 Итого получим такое уравнение:
\eqn{\frac{M_{n+1}}{6}h_{n+1} + M_n \frac{h_{n+1} + h_n}{3} + \frac{M_{n-1}}{6} h_n =
\frac{f(x_{n+1}) - f(x_n)}{h_{n+1}} - \frac{f(x_n) - f(x_{n-1})}{h_n} = f(x_{n+1};x_n) - f(x_{n-1};x_n).}
Получаем систему с трёхдиагональной матрицей, у которой имеется диагональное преобладание.
В самом деле,
\eqn{h_{n+1} + h_n \ge \frac{h_{n+1}}{2} + \frac{h_n}{2}.}

Впрочем, пока здесь есть одна проблема. Это равенство справедливо только в промежуточных
узлах, и поэтому нам не удастся получить замкнутую систему линейных уравнений.
Для замыкания системы нужно ещё два уравнения, которые приходится брать <<с потолка>>.
Например, можно сделать так: провести интерполяционный многочлен Лагранжа $L$
через узлы $x_0, x_1, x_2,x_3$ и положить $M_0 := L''(x_0)$, потому что коэффициенты~$M_i$
в каком\д то смысле аппроксимируют значения производной исходной функции.
Аналогично поступаем и с точкой~$x_n$.

\begin{df}
Сплайны такого типа называются \emph{интерполяционными}.
\end{df}

Для сплайнов второго порядка имеют место оценки:
\eqn{\bn{f^{(k)} - s^{(k)}} \le C h^{4-k}, \quad k = 0\sco 3, \quad C = \bn{f^{(4)}}, h = \max h_i,}
то есть приближается не только сама функция, но и её производные.

Но сразу видно, чем эти сплайны плохи. При добавлении всего одной точки в систему узлов
придётся все коэффициенты пересчитывать заново, что очень неудобно.
Значит, нужно придумывать что\д то более локальное. О нём пойдёт речь в следующем параграфе.

\subsubsection{В-сплайн}

Этот сплайн изобретён как попытка бороться с недостатками интерполяционных сплайнов.
Правда, приходится отказаться от того, чтобы сплайн проходил через точки $(x_i, f(x_i))$.

Дальнейшая конструкция не является очень естественной, поэтому мы приведём только
готовый рецепт, не поясняя, как до этого додумались.

Рассмотрим отрезок $[-2,2]$ и разделим его на маленькие отрезки длины $1$.
Рассмотрим функцию $B(x)$, удовлетворяющую таким свойствам:
\begin{items}{-2}
\item $\supp B = [-2,2]$;
\item на каждом маленьком отрезке $B$ является кубическим многочленом;
\item $B \in \Cb^2(\R)$;
\item $B(x)$ чётна;
\item выполнено нормирующее условие $B(0) + 2B(1) = 1$.
\end{items}
Если немного подумать, можно придумать, например, такую функцию
\eqn{B(x) := \case{\frac23 - x^2 + \frac12\hm{x}^3, & \hm{x} \le 1;\\
\frac16(2-\hm{x})^3, & \hm{x} \in [1,2];\\
0, &\hm{x} > 2.}}
А теперь построим \emph{кубический сплайн} с использованием этой функции.

Рассмотрим разбиение отрезка $[a,b]$ на $N$ частей с постоянным шагом $h$.
Рассмотрим функцию
\eqn{
B_3^{(i)}(x) := \suml{n=-1}{N+1} \al_n^{(i)} B\hr{\frac{x-nh}{h}}.
}
Это и будет наш сплайн. Пока мы не определили, что такое $\al_n^{(i)}$.
Существуют два вида сплайнов.

Первая разновидность ($i = 1$). Нам заданы значения функции $f_0\sco f_N$ в узлах.
Тогда с помощью линейной интерполяции
строятся значения $f_{-1}$ и $f_{N+1}$. Затем полагаем $\al_n^{(1)} := f_n$
и рисуем наш сплайн.

Вторая разновидность ($i=2$). Для этого мы сначала с помощью кубической экстраполяции
определяем значения функции $f_{-2}, f_{-1}$ и $f_{N+1}, f_{N+2}$, проводя кубические сплайны
через наборы точек $x_0\sco x_3$ и $x_{N-3}\sco x_N$ соответственно.
Затем определяем коэффициенты $\al_n^{(2)}$ по формулам
\eqn{\al_n^{(2)} := \frac{8f_n - f_{n+1} - f_{n-1}}{6}.}
Почему нужно брать именно такие, а не другие, тайна сия велика есть.

Но как бы там ни было, для этих сплайнов имеют место оценки:
\eqn{
\bn{f^{(k)} - (B_3^{(1)})^{(k)}} \le C h^{2-k}, \quad k = 0,1;
}
\eqn{
\bn{f^{(k)} - (B_3^{(2)})^{(k)}} \le C h^{4-k}, \quad k 0\sco 3.
}
Основное преимущество этих сплайнов таково: они локальны. То есть если значение
функции меняется в одной точке, то не нужно пересчитывать все коэффициенты.

\section{Численные методы и дифференциальное исчисление}

\subsection{Численное дифференцирование}

Будем считать, что заданы узлы $x_1\sco x_n$ и значения функции $f(x_1)\sco f(x_n)$.
Самое простое\т это свести задачу к предыдущей, то есть приблизить функцию интерполяционным
многочленом Лагранжа и считать, что $L_n'(x) \approx f'(x)$. Однако пример Адамара
$f_\ep(x) = f(x) + \ep\sin\frac{x}{\ep^2}$ показывает, что при стремлении к нулю в норме $\Cb$ погрешности,
норма производной погрешности стремится к бесконечности. Отсюда следует, что оператор дифференцирования
плохой и, вообще говоря, неограниченный.

Для борьбы с плохими, сильно осциллирующими функциями, можно применять фильтрацию частот.
Именно, для фильтрации спектра, сосредоточенного на отрезке $[a,b]$, можно к функции $f$ применять преобразование
$\wh F^{-1} \chi_{[a,b]}\wh F$.

Выведем формулы для приближения производных в виде линейных комбинаций значений функции  в некоторых точках, то
есть
\eqn{f'(x) \approx \sum a_i f(x_i).}

Рассмотрим формулу $f'(x) =\frac{f(x+h)-f(x)}{h} + r(x,h)$.
Имеем $r(x,h) = \frac{f''(\xi)}{2} \cdot h$, поэтому $|r| \le \frac h2 \maxl{[x,x+h]} |f''(x)|$.
Таким образом, эта формула приближает производную с точностью до $O(h)$.

Можно также применять формулу $f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}$. Её точность\т $O(h^2)$.

Покажем, как строить формулы для аппроксимации производных по более чем двум узлам. Построим, например, формулу
для первой производной по 3 узлам:
\eqn{f'(x) \approx \frac1h\br{a_1f(x) + a_2 f(x+h)+a_3 f(x+2h)}.}
Будем подбирать  коэффициенты такими, чтобы формула давала точное значение производной для многочленов степеней не выше 2.
Получаем систему уравнений:
\eqn{\case{a_1+a_2+a_3 = 0,\\
a_2 + 2a_3 = 1,\\
a_2+4a_4=0.}}
Её решение\т $a_1 = -\frac32$, $a_2 = 2$, $a_3 = -\frac12$.
Значит, формула имеет вид
\eqn{f'(x) \approx \frac1{2h}\br{-3f(x) + 4 f(x+h)- f(x+2h)}.}

Аналогично получим формулу для второй производной:
\eqn{f''(x)\approx \frac{f(x+h)-2f(x)+f(x-h)}{h^2} - \frac{h^2}{12}f^{(4)}(x) + O(h^4).}
Оценим погрешность. Пусть $|f''(x)| \le M$, а $\ep$\т равномерная погрешность измерения функции, то есть $|f-\wt f| \le \ep$.
Тогда  $|f'-\wt f'| \le \frac{Mh}{2} + \frac{2\ep}{h}$. Чтобы погрешность измерений не сильно влияла на оценку  производной,
берём $h = 2\sqrt{\frac\ep M}$. Отсюда
\eqn{\De \le \frac{2M  \sqrt{\ep}}{2\sqrt M} + \frac{2\ep \sqrt{M}}{2\sqrt\ep} = 2\sqrt{M\ep}.}

\subsection{Сжатие информации}

Здесь была рассказана стандартная сказка про разложение по ОНБ в гильбертовом пространстве.
Основная мысль: если хранить коэффициенты Фурье, то можно сильно сэкономить. Во\д первых, часть коэффициентов (высокие частоты)
можно выкинуть без особой потери смысла, ибо обычно они всё равно стремятся к нулю. Более того, набор коэффициентов можно
более эффективно сжимать обычными методами\т для не слишком поганых функций он выглядит куда приличнее, чем сама функция.

\subsubsection{Двумерный случай}

Ещё было что\д то про сингулярное разложение матриц. Какое оно имеет отношение к реальным алгоритмам сжатия, автор конспекта
понимает с большим трудом. Точнее говоря, мы не будем сжимать никакие данные, а мы просто будем приближать функцию некоторыми
базисными функциями, а хранить лишь коэффициенты разложения.

\begin{df}
Рассмотрим самосопряжённый оператор $A$. Разложение $\Sig = U^tAV$, где матрица $\Sig$ диагональна, а $U$ и $V$ ортогональны,
называется \emph{сингулярным}.
\end{df}

Поставим такую задачу: найти наилучшее приближение для функции $f \in L_2([0,1]^2)$ с помощью функции вида $\wt u(x)\wt v(y)$.
В качестве нормы в данном случае будем использовать $L_2$\д норму.
Положим
\eqn{ u := \frac{\wt u}{\hn{\wt u}}, \quad \ol v := \frac{\wt v}{\hn{\wt v}}.}
Как уже было сказано, мы хотим решить задачу
\eqn{\hn{f - \mu\cdot u(x)v(y)}^2 \ra \min.}
Распишем эту норму:
\eqn{\hn{f - \mu u v}^2 = \hn{f}^2 - 2\mu\int u(x) \bbr{\int f(x,y) v(y)dy}dx + \mu^2 \int u^2(x)dx \cdot \int v^2(y)dy.}
Выражение в скобках под интегралом\т это интегральный оператор с ядром $f$, применённый к функции $v$.
Обозначим этот оператор через $A$. Далее, заметим, что последнее слагаемое равно $\mu^2$, потому что каждый
из интегралов равен~1 в силу выбранной нормировки функций $u$ и~$v$.
С учётом всего этого можно переписать норму в таком виде:
\eqn{\hn{f - \mu \cdot u v}^2 = \hn{f}^2 + \br{\mu - (A v,u)}^2 - (A v,u)^2.}
Дальнейшая идея такова: будем максимизировать число $(Av,u)$, чтобы вычитать побольше,
а потом выберем такое $\mu$, чтобы второе слагаемое умерло, а именно, потом положим $\mu := (Av,u)$.

Реализуем этот план. Сначала оценим скалярное произведение:
\eqn{\hm{(A v,u)} \le \hn{A v}\cdot \hn{u} = \hn{A v}.}
Нам нужна функция $v$, для которой достигается максимум $\hn{A v}$.
или, что то же самое, максимум $\hn{A v}^2$.
Имеем
\eqn{\hn{A v}^2 = (A v, A v) = (A^*A v,v).}
Стало быть, если $v$ отвечает максимальному собственному значению оператора $A^*A$,
то это число будет максимальным. Такой её и выберем.

\begin{petit}
Остаётся вот какой вопрос: как найти это максимальное собственное значение. Для этого можно сделать сингулярное
разложение матрицы, и выбрать максимальный по модулю диагональный элемент матрицы $\Sig$.

Алгоритм осуществления сингулярного разложения на лекциях рассказан не был. Его можно сдуть из книжки
Уилкинсона и др. \emph{<<Справочник алгоритмов по линейной алгебре>>}.\т М., 1976.

Фактически этот алгоритм эквивалентен полному QR-алгоритму нахождения собственных значений. Это ужасно долго.
Если немного подумать и вспомнить доказательство одной теоремы из функана, то можно приближённо найти максимальное
собственное значение иным (итерационным) путём. А именно, берём любой единичный вектор, и начинаем  к нему применять оператор $A$.
Между итерациями будем нормировать вектор, чтобы он не раздулся до бесконечности. Ежу ясно, что он будет стремиться к вектору с
максимальным собственным значением (попробуйте доказать или привести контрпример, возможно, получится и то и другое).
Во всякому случае я проверял\т у  меня просто зашибись как всё сходится. Конечно, в словах <<любой вектор>> таится угроза того,
что он случайно может оказаться собственным, и тогда он никуда с места не стронется. Поэтому лучше говорить о векторах
общего положения, которые заполняют плотное подмножество нашего пространства. А именно, это в точности те вектора, у которых все
координаты в разложении по собственному базису ненулевые.
\end{petit}

Далее, вектор $u$ уже выбрать легко. Чтобы максимизировать скалярное произведение, его нужно выбрать
пропорциональным вектору $Av$ и отнормировать. После этого вычисляем $\mu$ и получаем тройку $(\mu, u,v)$,
для которой значение $\hn{f - \mu u v}$ минимально. Это будет первым приближением.

Положим теперь $\mu_1 := \mu$, $u_1 := u$, $v_1 := v$ и рассмотрим функцию $f_1 := f - \mu_1u_1v_1$.
Несложно показать, что $f_1 \bot u_1v_1$ в $L_2$ (это общий факт, который в народе называют теоремой
Пифагора). Теперь можно применить ту же процедуру к функции $f_1$ и найти тройку $(\mu_2,u_2,v_2)$,
которая минимизирует значение нормы $\hn{f_1 - \mu_2 u_2 v_2}$, и так далее.

Можно показать, что
\eqn{\sum \mu_i u_i(x) v_i(y) \xra{L_2} f.}

У этого метода есть очевидные преимущества. Когда мы заранее фиксируем базис пространства $L_2[0,1]$
и пытаемся приближать функцию из $L_2\otimes L_2$ линейной комбинацией разложимых тензоров $\ph_i(x)\otimes \ph_j(y)$
при фиксированном базисе, то получается плохо. А в этом алгоритме мы на каждом шаге строим оптимальный разложимый тензор,
и получается, вообще говоря, лучше. Но недостаток тоже ясен. Этот метод хорош теоретически, но на практике
мы не знаем ничего про оператор $A$ и его собственные значения, поэтому непонятно, как этот алгоритм можно применять.


\subsection{Численное интегрирование}

Как и в случае дифференцирования, будем приближать интегралы линейными комбинациями значений функции в некоторых узлах.
Будем рассматривать интегралы вида
\eqn{I(f) = \intl{a}{b}p(x)f(x)\dx,}
где $p(x)$\т некоторая весовая функция.
Приближения в общем виде записываются так:
\eqn{I(f) \approx \sum c_i f(x_i).}

Один из способов численного интегрирования\т приблизить функцию многочленом Лагранжа степени $n$ и проинтегрировать его.
Естественно, значение интеграла заведомо будет точным для многочленов степени $n$.
На практике применяются так называемые \emph{квадратурные формулы} прямоугольников, трапеций и Симпсона (как частные случаи
формулы Лагранжа). Они получаются из соображений того, что приближённая формула должна быть точной для  многочленов наиболее
высокой степени.

Оценки погрешности для всех трёх формул будут доказаны единообразно чуть ниже.

\begin{df}
Квадратуры с равноотстоящими узлами называются \emph{квадратурами Ньютона\ч Котеса}.
\end{df}

\begin{petit}
На лекциях этого определения не было, и оно было нагло сдуто из лекций Арушаняна.
Также неясно, как быть с доказательствами оценок формул, которые будут ниже. На лекциях их также не было, но теоретически
могут попросить вывести.
\end{petit}


\subsubsection{Формула прямоугольников}

Найдём квадратурную формулу для одного узла. Она должна быть точна на многочленах нулевой степени, и потому
имеет вид $K_1(f) = (b-a)\cdot f\hr{\frac{a+b}{2}}$.

Погрешность вычислений данной формулы $R(f) \le \hn{f'}\cdot \frac{(b-a)}{4}$ и $R(f) \le \hn{f''}\cdot \frac{(b-a)^3}{24}$.

\subsubsection{Метод трапеций}

В этой формуле используется два узла. Формула должна быть точной для многочленов нулевой и первой степеней. Пусть
$x_1= a$, $x_2 = b$. Найдём коэффициенты: $K_2(f) = c_1f(x_1) + c_2 f(x_2)$.
Имеем
\eqn{\case{b-a = c_1 + c_2,\\\frac{b^2-a^2}{2} = c_1 a  + c_2 b.}}
Отсюда $c_1 = c_2  = \frac{b-a}{2}$.

Погрешность вычислений данной формулы $R(f) = \hn{f''}\cdot \frac{(b-a)^3}{12}$.

\subsubsection{Метод Симпсона}

Абсолютно аналогично получается формула
\eqn{K_3(f) = \frac{b-a}6\hr{f\hr{a} + 4 f\hr{\frac{a-b}{2}} + f\hr{b}}.}
Погрешность вычислений данной формулы
$R(f) \le \hn{f'''}\cdot \frac{(b-a)^4}{192}$ и
$R(f) \le \hn{f^{(4)}}\cdot \frac{(b-a)^5}{2880}$.
Она оказывается точной и для многочленов третьей степени за счёт симметрии узлов.

\subsection{Оценка погрешности квадратурных формул}

Будем считать, что  узлы заданы. Будем проверять точность формулы $I(f)$ на функциях $f_k(x) = x^k$. Пусть формула имеет вид
\eqn{I(f) =\suml{i=1}{n} c_i f(x_i).}
Получаем систему уравнений  с матрицей Вандермонда:
\eqn{
\case{c_1\spl c_n = I(1),\\
c_1x_1\spl c_nx_n = I(x),\\
c_1x_1^2\spl c_nx_n^2 = I(x^2),\\
\dots\\
c_1x_1^{n-1}\spl c_nx_n^{n-1} = I(x^{n-1}).}}
Она имеет единственное решение, поскольку все узлы различны.
Оценим погрешность:
$R(f) := I(f)-K_n(f)$. Имеем $R(P_m) = 0$ при $m=1\sco n-1$.
\eqn{|R(f)| \le \intl{a}{b} |p|\cdot |f-P_m|\dx + \suml{i=1}{n} |c_j| \cdot |f(x_i)-P_m(x_i)| \le
\intl{a}{b} |p|\dx \cdot \hn{f-P_m}_C + \suml{i=1}{n} |c_i|\cdot \hn{f-P_m}_C,}
причём можно считать, что $P_m$\т МНРП (раз уж эта оценка верна для любого многочлена).
Значит,
\eqn{|R(f)| \le \hr{\int |p|\dx + \sum |c_i|} E_m(f),}
где $E_m(f) = \inf \hn{f - P_m}$ (константа наилучшего приближения).

Допустим, что $c_j \ge 0$ и $p \ge 0$.
Покажем, что в этом случае $R(f)\ra 0$ при $m\ra \bes$.
В самом деле, поскольку квадратура точна, в частности, на многочленах нулевой степени, имеем
\eqn{M := \sum c_j = \int p\dx,}
откуда $R(f) \le M \cdot E_m(f) \ra 0$.

В частности, получаем оценки для квадратурных формул:
\eqn{\begin{aligned}
R_1(f) \le \frac{1}{24}\cdot \max |f^{(2)}|\cdot (b-a)^3,\\
R_2(f) \le \frac{1}{12}\cdot\max |f^{(2)}|\cdot (b-a)^3,\\
R_3(f) \le \frac{1}{2880}\cdot \max |f^{(4)}|\cdot (b-a)^5.
\end{aligned}}

Теперь фиксируем число узлов $n$ и попробуем построить квадратурную формулу, которая будет точна для многочленов наиболее
высокой степени. У нас имеется $2n$ неизвестных $c_j$ и $x_j$.
Хочется ожидать, что формула окажется точной для многочленов степени $2n-1$.
Ну что же, напишем систему уравнений:
\eqn{\bcase{
I(1) &= c_1\spl c_n,\\
I(x) &= c_1x_1\spl c_nx_n,\\
\dots\\
I(x^{2n-1}) &= c_1x_1^{2n-1}\spl c_nx_n^{2n-1}.
}}

\begin{stm}
Существует квадратурная формула, точная на многочленах степени не выше $2n-1$.
\end{stm}
\begin{proof}
Построим систему ортогональных многочленов на отрезке $[a,b]$ относительно веса $p$.
Мы знаем, что ортогональный многочлен степени $n$ имеет ровно $n$ простых корней.
Пусть мы уже построили квадратуру $K_n$, которая точна на многочленах степени $n-1$.
Покажем, что за счёт выбора узлов $x_1\sco x_n$ можно сделать так,
что она будет точна на многочленах степени не выше~${2n-1}$.
Пусть узлы выбраны так, что $\om_n$ совпадает с приведённым ортогональным многочленом степени $n$.

Пусть $Q_{2n-1}$\т произвольный многочлен степени не выше $2n-1$.
Разделим его с остатком на многочлен $\om_n$, получим
\eqn{Q_{2n-1}(x) = Q_{n-1}(x)\om_n(x) + r_{n-1}(x).}
Мы знаем, что $I(r_{n-1}) = K_n(r_{n-1})$.

Заметим, что
\eqn{I(Q_{n-1}\om_n) = \int p(x) Q_{n-1}(x)\om_n(x)\dx = 0,}
потому что $\om_n \bot Q_{n-1}$.
С другой стороны, имеем
\eqn{K_n(Q_{n-1}\om_n) = \sum c_j Q_{n-1}(x_j) \om(x_j) = 0,}
потому что $x_j$\т корни $\om_n$. Тогда, в силу линейности и квадратуры, и интеграла $I$, получим
\eqn{I(Q_{2n-1}) = I(Q_{n-1}\om_n) + I(r_{n-1}) = 0 + I(r_{n-1}) = K(r_{n-1}) = K(Q_{n-1}\om_n) + K(r_{n-1}) = K(Q_{2n-1}),}
и мы показали, что квадратурная формула точна на многочленах степени не выше $2n-1$.
\end{proof}


Мы доказали, что можно построить достаточно хорошую квадратурную формулу.
А теперь покажем, что ничего лучше сделать уже нельзя.

\begin{stm}
Не существует квадратурной формулы, которая была бы точна на  всех многочленах
степени $2n$.
\end{stm}
\begin{proof}
В самом деле, пусть квадратурная формула точна на многочленах степени $2n$.
Пусть она использует узлы $x_1\sco x_n$. Рассмотрим многочлен $\om_n$ по этим узлам.
Тогда, в частности, имеем
\eqn{I(\om_n^2) = \int p(x)\om^2(x)\dx > 0,}
а с другой стороны,
\eqn{K_n(\om_n^2) = \sum c_j \om_n^2(x_j) = 0,}
потому что это нули многочлена $\om_n$. Противоречие.
\end{proof}

\begin{stm}
Коэффициенты $c_j$ квадратурной формула Гаусса положительны.
\end{stm}
\begin{proof}
Зафиксируем некоторое $k$ и рассмотрим
многочлен
\eqn{Q_{2n-2}(x) := \frac{\om_n^2(x)}{(x-x_k)^2}.}
Для него формула точна, и
\eqn{I:= I(Q_{2n-2}) = \int p(x) \frac{\om^2_n(x)}{(x-x_k)^2}\dx > 0.}
Но правая часть равенства есть
\eqn{I = \suml{j=1}{n} c_j \frac{\om^2_n(x_j)}{(x_j-x_k)^2},}
а здесь все слагаемые равны нулю, кроме одного (при $j = k$).
Отсюда следует, что $c_k > 0$. Но $k$ можно было брать любым,
поэтому всё доказано.
\end{proof}

Значит, можно применить оценки скорости сходимости, полученные нами для
квадратурных формул с неотрицательными коэффициентами.
\eqn{|R_n(f)| \le 2\intl{a}{b}p(x)\dx \cdot E_{2n-1}(f),}
\eqn{E_{2n-1}(f) = \frac{\hn{f^{(2n)}}}{(2n)!} \cdot \frac{1}{2^{2n-1}}\cdot \hr{\frac{b-a}{2}}^{2n}.}

\subsection{Подсчёт интегралов по составным квадратурным формулам}

\subsubsection{Составные квадратурные формулы}

Из всего вышесказанного видно, как можно вычислять интегралы. Разобьём отрезок на большое число частей $N$
(назовём эти части $I_k$) и применим на каждом отрезке какую\д нибудь квадратурную формулу.
Потом просуммируем то, что получится.

Например, составная квадратурная формула трапеций выглядит так:
\eqn{I(f) = K_N(f) = \suml{k=0}{N-1} \frac{f(a+kh)+f\br{a+(k+1)h}}{2}h.}
А теперь посчитаем, насколько мы при этом наврём. На каждом шаге мы врём на величину
\eqn{r_k \le \frac{1}{12}\maxl{I_k} |f''|h^3,}
поэтому
\eqn{R_N = \suml{k=0}{N-1}r_k \le \frac{1}{12}\maxl{[a,b]}|f''|\cdot (b-a)h^2.}
Но это довольно грубо, мы выносим за скобку $\max |f''|$. Давайте улучшим эту формулу:
\eqn{\sums{k} r_k \le \frac{h^2}{12} \suml{k=0}{N-1}\maxl{I_k} |f''|\cdot h = Mh^2 + o(h^2),}
где $M := \frac{1}{12}\intl{a}{b}|f''|\dx$.

\subsubsection{Правило Рунге}

Хотелось бы иметь некоторый метод оценки погрешности квадратурной формулы даже в том случае, когда
мы ничего не знаем про точное значение интеграла (именно так оно и бывает на практике).
Применим квадратурную формулу для $N$ узлов и для $2N$ узлов.  Имеем
\eqn{\bcase{I(f) &= K_N(f) + Mh^2 + o(h^2),\\
I(f) &= K_{2N}(f) + M\hr{\frac{h}{2}}^2 + o\hr{\frac{h^2}{4}}.}}
Разность значений даёт оценку для главного члена погрешности:
\eqn{\frac34 Mh^2 = K_{2N}-K_N,}
откуда
\eqn{M = \frac{4}{3}\cdot \frac{K_{2N} - K_N}{h^2}.}
Это так называемый \emph{метод Рунге} оценки погрешности.


\subsubsection{Интегрирование быстро осциллирующих функций}

Пусть нужно вычислить интеграл от функции  $f = e^{i\om x }g(x)$, где $1 \ll \om$. Тогда нужно взять
в качестве веса функцию $p = e^{i\om x}$. Предполагается, что функция $g$ уже достаточно хорошая,
чтобы её можно было приближать многочленом Лагранжа.

Итак, пусть, для простоты, всё происходит на отрезке $[-1,1]$.
Заменим исходный интеграл
\eqn{\int e^{i\om x} g(x)\dx}
на интеграл
\eqn{\int e^{i\om x} L_n(x)\dx = \sums{j} f(x_j) \int e^{i\om x} \prods{k\ne j} \frac{x - x_k}{x_j-x_k}\dx =: \sums{j} f(x_j) C_j(\om).}
Здесь функции $C_j(\om)$ можно вычислить точно, поскольку интеграл от квазимногочлена явно считается.
Остаётся оценить погрешность. Имеем
\eqn{R_n(f) \le \int \hm{g(x) - L_n(x)}\dx.}
Стандартным образом оценивая подынтегральное выражение и домножая на длину отрезка,
получаем оценку для $R_n(f)$.

\begin{df}
В случае, когда многочлен Лагранжа берётся по трём узлам, эта формула называется
\emph{формулой Филона}.
\end{df}

\subsubsection{Оптимальные квадратуры}

Тут были слова про поиск оптимальной квадратуры на заданном классе.
Подробности см. БЖК., а в программе экзамена этого, судя по всему, нет.


Пусть $F$\т какой\д то класс функций, а $K$\ некоторый класс квадратур. Рассмотрим $E_k(f) := |I(f)-K(f)|$,
где $k$\т квадратура класса $K$, а $f \in F$. Наилучшей квадратурой на заданном классе называется квадратура
\eqn{k^* := \argmin \infl{k\in K} \supl{f\in F} E_k(f).}

Оценки погрешностей квадратур выражаются через производные функций данного класса.
Так, например, формула прямоугольников является оптимальной для функций с ограниченной второй производной
(теорема Никольского).



\section{Численные методы линейной алгебры}

В этой главе мы будем пытаться решать линейные системы вида $Ax= b$.


\subsection{Точные методы}

Имеет место очевидная оценка для сложности алгоритмов решения систем и нахождения собственных значений\т
$O(m^2)$, так как даже самый оптимальный алгоритм должен задействовать  все элементы матрицы.
Имеется и оценка сверху\т алгоритм Гаусса даёт оценку $\frac23 m^3$ операций для решения линейных систем.

Классический алгоритм Гаусса ломается довольно быстро, потому что набегает погрешность.
Поэтому обычно используют его модификации (метод Гаусса с выбором главного элемента \итп).

Кроме метода Гаусса, имеются более качественные методы (например, метод отражений, метод вращений, метод Холецкого).
Многие из них лучше обычного метода Гаусса тем, что используют ортогональные преобразования, а это уменьшает погрешность.
Естественно, что трудоёмкость при этом возрастает (за всё приходится платить).

Про эти методы можно почитать, например, в книжке Богачёва, но мы всё же опишем некоторые из них.


\subsubsection{Метод отражений}

Пусть $w$\т вектор единичной длины. Рассмотрим матрицу $U_w = E - 2ww^t$.
Изучим её свойства:
\begin{nums}{-2}
\item $U_w$ симметрична.
\item $U_w^2 = E$. В самом деле,
\eqn{U_w^2 = I - 4 w w^t + 4 w(w^t w) w^t = E.}
\item Из свойств 1 и 2 следует, что $U_w U_w^t = E$, то есть матрица $U_w$ ортогональна.
\item Из свойства 2 следует, что $\Spec U_w \subs \hc{1, -1}$.
\item $U_w w = w - 2 w(w^t w) = - w$.
\item Если $x \bot w$, то $U_w x = x - w w^t x = x - w (w,x) = x$, ибо $(w,x) = 0$.
\end{nums}

Таким образом, матрица $U_w$ является матрицей отражения
относительно гиперплоскости с нормалью $w$.

Пусть заданы два вектора $e$ и $y$. Найдём вектор $w$, такой что $U_w y = \al e$ ($\al \in \R$).
Запишем вектор $y$ в виде $y = \ga w + v$, где $v \bot w$. Поскольку $U_w$ не меняет длины векторов,
получаем, что $\al = \pm\frac{\hm y}{\hm e}$.
После применения матрицы отражения получаем систему
\eqn{\case{-\ga w + v = \al e,\\
\ga w + v = y,}}
значит,
\eqn{2\ga w = y - \al e.}
Поскольку $w$ должен быть единичным, получаем, что
\eqn{w = \frac{y - \al e}{\hm{y - \al e}}.}

Теперь уже можно построить алгоритм. Подбираем вектор $w_1$ так, чтобы после отражения
с помощью матрицы $U_1 := U_{w_1}$ первый столбец
матрицы $A$ перешёл бы в вектор, пропорциональный $e_1$ (здесь $\hc{e_i}$\т стандартный базис).
После этого переходим к системе $U_1 A x = U_1b$, и так далее.

Тогда в процессе работы алгоритма получаем система будет преобразовываться так:
\eqn{U_{m-1} U_{m-2}\sd U_1 A x = U_{m-1}U_{m-2}\sd U_1 b.}
Отметим, что вычисление числа $\al$ требует порядка $m$ операций, вычисление $w$\т ещё порядка $m$ операций.
Что касается умножения на матрицу $U_w$ слева, то оно требует порядка $m^2$ операций, если его производить последовательно,
то есть дважды умножая матрицу на вектор (а не $m^3$, как может показаться, если сначала явно вычислить матрицу $U_w$,
а потом умножать её на матрицу системы!)

Преимущество этого метода: его устойчивость. Мы всякий раз совершаем ортогональные преобразования,
значит, погрешность будет накапливаться не очень сильно.

\subsubsection{Метод Холецкого}

Метод Холецкого, он же метод квадратного корня, основан на построении разложения
матрицы в произведение $A = S D S^t$, где $D$\т диагональная матрица, а $S$\т верхнетреугольная.


На этом краткий обзор точных методов заканчивается, и мы переходим к итерационным методам.

\subsection{Итерационные методы}

\subsubsection{Метод простой итерации}

Одним из наиболее примитивных является метод простой итерации, который состоит в преобразовании системы $Ax=b$ к виду
$x = Bx +c$ и применении итераций $x_{n+1} = Bx_n+c$.

Если $\hn{B} <1$, то данный процесс, очевидно, сходится со скоростью геометрической прогрессии. В самом деле,
пусть $\ol x$\т точное решение то есть $\ol x = x_n -r_n$. Имеем
\eqn{\bcase{\ol x &= B \ol x +c,\\x_{n+1} &= Bx_{n}+c.}}
Вычитая уравнения друг из друга, получаем
$\hn{r_{n+1} = B r_n} \ra 0$ при $n\ra \bes$.


В дальнейшем мы будем рассматривать следующие матричные нормы:
\begin{items}{-2}
\item $\hn{A}_\bes = \maxl{j}\sums{i}|a_{ij}|$,
\item $\hn{A}_1 = \maxl{i}\sums{j}|a_{ij}|$,
\item $\hn{A}_2 = \sqrt{\max \la(A^tA)}$.
\end{items}

Чтобы доказать сходимость метода, хотя бы в одной из этих норм должно быть выполнено $\hn{B} < 1$.

\begin{theorem}[Критерий сходимости метода простой итерации]
Пусть $\la_i$\т собственные значения матрицы $B$. Метод простой итерации
сходится тогда и только тогда, когда $|\la_i|<1$ при всех $i$.
\end{theorem}
\begin{proof}
Докажем сначала обратное утверждение. Нам потребуется

\begin{lemma}
Для всякой матрицы $B$ и всякого $\eta > 0$ существует невырожденная матрица $T$, такая что
\eqn{B = T^{-1} \La T,}
где $\La$ имеет вид матрицы из жордановых клеток, у которых над главной диагональю
вместо единиц стоит число $\eta$ (а на диагонали, как обычно, стоят собственные значения матрицы $B$).
\end{lemma}
\begin{proof}
Потом напишем. Применить теорему о ЖНФ и подкрутить.
\end{proof}

Пусть $\hm{\la_i} < 1$ при всех $i$. Их конечное число, поэтому можно выбрать столь малое
$\eta > 0$, что $\hm{\la_i} + \eta < 1$ при всех $i$. Тогда имеем $\hn{\La}_1 < 1$.
Покажем, что метод сойдётся:
\eqn{r_n = B^n r_0 = (T^{-1} \La T)^n r_0 = T^{-1} \La^n T r_0,}
но $\hn{\La^n}_1 \ra 0$, поэтому и $r_n \ra 0$.

Теперь докажем прямое утверждение. От противного. Допустим, что существует собственное значение $\la$, такое что
$\hm{\la} \ge 1$, и пусть вектор $e$ отвечает собственному значению $\la$.
Выберем подходящее начальное условие, такое что $\ol x - x_0 = e$,
то есть $r_0 = e$. Тогда $r_n = B^n r_0 = \la^n e \nra 0$, что и требовалось доказать.
\end{proof}

\begin{ex}
Это будет не просто пример, а даже контрпример, который показывает, что в общем случае всё плохо.
Пусть $n < m$ ($m$ судя по всему, размерность пространства).

Пусть $B = \al E + \be J(0)$, где $J(\la)$\т жорданова клетка с собственным значением $\la$, а $\al$ и $\be$\т некоторые числа.
Тогда $\hn{B^n}_1 = (|\al|+|\be|)^n$. В самом деле, при $n < m$ матрица будет иметь вид
\eqn{B^n = \rbmat{\al^n & \Cb_n^1 \al^{n-1}\be & \dots & \be^n & 0 & \dots & 0\\
0 & \al^n & \hdotsfor{5}\\
\hdotsfor{7}\\
0 & \hdotsfor{4} & 0 & \al^n},}
то есть она будет верхнетреугольной, а по строкам будут биномиальные слагаемые.
Ясно, что максимум при подсчёте нормы будет достигаться в первой строке, а сумма модулей чисел в ней\т
это как раз $(\hm{\al} + \hm{\be})^n$.

Пусть $c := (0,0\sco0, 1)$.
Несложно видеть, что итерации отображения $B$ имеют вид
\eqn{x_n = \frac{1}{1-\al}\hr{0\sco 0, \hr{\frac{\be}{1-\al}}^{n-1},  \hr{\frac{\be}{1-\al}}^{n-2}\sco \frac{\be}{1-\al},1}}
(это проще всего проверить по индукции).

Пусть числа $\al$ и $\be$ таковы, что $|\al| <1$, $\al < 0$, $|\be|>1$, и при этом $|\al|+|\be| > 1$ и $\hm{\frac{\be}{1-\al}} <1$.
Отметим, что при таком выборе $\hn{B}_1$ растёт с экспоненциальной скоростью.
Оценим норму итерации решения:
\eqn{\hn{x_n}_1  = \frac{1- \frac{\be^n}{(1-\al)^n}}{1- \frac{\be}{1-\al}} \le \frac{1-\al}{1-(\al+\be)}.}
Казалось бы, ничего особенного. Положим $r_0 := (0,0\sco 0,1)$. А теперь начинаем считать погрешность:
\eqn{r_n  = B^n r_0 = (0\sco 0, \be^n, \dots, \Cb_n^1 \al^{n-1}\be, \al^n).}
Если размерность достаточно большая, то погрешность сначала будет экспоненциально расти, так как $\be^n\ra\bes$. Через несколько
итераций это прекратится, так как $\be^n$ <<выползет за пределы>> вектора (количество нулевых координат у вектора $x_n$ уменьшается
с ростом $n$). Но та ошибка, которая успеет набежать за это время, уже успеет всё испортить.

Чтобы поверить в то, что ни одна машина с такой системой не справится, рекомендуется прогнать
этот метод на таких параметрах: $\al = -\frac12$, $\be = \frac54$. Тогда $\hm{\al} + \hm{\be} = \frac74$,
а $\hm{\frac{\be}{1-\al}} = \frac{5}{4}\cdot\frac{2}{3} = \frac{5}{6} < 1$, то есть все условия выполнены.
Отметим, что сама норма итераций решения, как видно из оценки, не превосходит $\frac{\frac{3}{2}}{1 + \frac12 - \frac54} = 6$.
Достаточно взять $m \sim 100$, $n \sim 50$, чтобы всё сломалось.
\end{ex}


\begin{petit}
Тут был замечен такой бред: на 50 итерациях $\be^n$ вырастет сильно больше, чем оценка нормы решения...
\end{petit}

\subsubsection{Модификация метода простой итерации (метод Ричардсона)}

Из приведённого в предыдущем разделе примера видно, что просто
так применять описанный метод простой итерации нельзя. Будем пытаться его улучшить, исходя из того,
что мы обладаем некоторой информацией о спектре исходного оператора $A$.

Пусть мы знаем, что $A = A^t$ и $\Spec(A) \subs [\mu,M]$, причём $\mu > 0$.

Перепишем наше уравнение в виде
\eqn{x = x - \tau (Ax-b).}
Здесь $\tau \ne 0$\т некоторое число, которое мы сейчас будем подбирать.
Это самый обычный метод простой итерации: достаточно переписать это уравнение в виде
\eqn{x = \ub{(E - \tau A)}_B x + \ub{\tau b}_c.}
Теперь сделаем из нашего уравнения итерационный процесс:
\eqn{x_{n+1} = x_n - \tau(Ax_n -b).}
Если переписать его в виде
\eqn{\frac{x_{n+1}-x_n}{\tau} + Ax_n = b,}
получим, что это разностный аналог параболического уравнения.

Когда он сходится? Имеем $\la_B = 1 - \tau \la_A$, поэтому, применяя теорему о сходимости метода простой итерации,
получаем, что сходимость будет тогда и только тогда, когда
\eqn{\hm{1 - \tau \la_A} < 1 \quad \Lra \quad \case{\tau \la_A > 0, \\ \tau < \frac{2}{\la_A}.}
\quad \Lra \quad 0 < \tau < \frac{2}{M}.}

А теперь поставим себе целью найти оптимальное $\tau_{\text{opt}} \in \hr{0,\frac{2}{M}}$, для которого метод сходится
быстрее всего. Фактически, нас интересует $\tau = \arg \minl{\tau} \hn{B}_{\text{евкл}}$,
а в силу симметричности матрицы $B$ получаем, что
\eqn{\tau_{\text{opt}} = \arg \minl{\tau} \maxl{\la \in[\mu,M]} \hm{1-\tau\la}.}
Поскольку функции кусочно\д линейные, максимум достигается в концах отрезка, стало быть,
\eqn{\tau_{\text{opt}} = \arg \minl{\tau} \max\hc{\hm{1-\tau \mu}, \hm{1-\tau M}}.}

Для наглядности построим графики функций $f_1(\tau) := |1-\tau\mu|$ и $f_2(\tau) := |1-\tau M|$.
$$\epsfbox{pictures.13}$$
При $\tau_{\rm opt} = \frac{2}{M+\mu}$ имеем минимум нормы $\hn B = \frac{M-\mu}{M+\mu}$.
Предположим, что $t:= \frac\mu M \ll 1$. Тогда $t$ можно рассматривать как малый параметр и раскладывать по нему в ряд.
Отсюда
\eqn{\frac{M-\mu}{M+\mu} = \frac{1-t}{1+t} = (1-t)(1-t+t^2-t^3+\dots) = 1 - 2(t-t^2+\dots)}

Мы хотим, чтобы $|r_n| \le \hn{B}^n |r_0| \le \ep |r_0|$, то есть $n\ln \hn B \le \ln \ep$.
Таким образом, число итераций для достижения точности $\ep$ должно быть не меньше, чем
\eqn{n \ge  \frac{\ln \ep^{-1}}{-\ln \hn B} \approx \frac{\ln \ep^{-1}}{2t} =  \frac12 \frac M{\mu}\ln \ep^{-1}.}

\begin{petit}
На лекциях тут, судя по всему, была допущена ошибка: потерян множитель $\frac12$, который
возникает из разложения по~$t$.
\end{petit}

\subsubsection{Upgrade метода Ричардсона, или чебышевское ускорение}

\begin{petit}
Судя по всему, этот текст соответствует билету про чебышевское ускорение. На лекциях этих слов, кажется,
не произносилось, но многочлены Чебышёва присутствуют достаточно явно.
\end{petit}

Но нам этого мало. Мы хотим уменьшить зависимость параметра $n$ от отношения $\frac{\mu}{M}$.
Будем стараться подбирать $\tau$ так, чтобы норма оператора перехода от 1 шага к $n$\д му была минимальной.
На каждом шаге будем использовать поправку $\tau_k$.
Имеем
\eqn{r_n = \ub{(E - \tau_{n-1}A)\sd(E - \tau_0 A)}_{P_n(A)}r_0.}
На $r_0$ действует некоторый многочлен $P_n$ от оператора $A$. Имеем $P_n(0)=1$.
Так как матрица $A$  симметрична, то по лемме об отображении спектра имеем
\eqn{\hn{P_n(A)} = \maxl{\la\in \Spec A} |P_n(\la)|.}
Впрочем, у нас нет полной информации о спектре, а мы знаем только, на каком отрезке он лежит.
Поэтому мы сможем лишь оценить сверху норму этого оператора:
\eqn{\maxl{\la\in \Spec A} |P_n(\la)| \le \hn{P_n(\la)}_{\Cb[\mu,M]}.}

Среди всех многочленов найдём многочлен с минимальной нормой. Разумеется, это многочлен Чебышёва степени $n$
на отрезке $[\mu,M]$.
Автоматически получаем $n$ корней и $n$ значений параметров $\tau_k$. Пусть $t_k$\т нули стандартного многочлена Чебышёва.
Тогда имеем
\eqn{t_k := \frac{M+\mu -2\la_k}{M-\mu} = \cos\frac{(2k+1)\pi}{2n},}
откуда
\eqn{\la_k = \frac{M+\mu}{2} - \frac{M-\mu}{2}\cos \frac{(2k+1)\pi}{2n}, \quad \tau_k = \frac{1}{\la_k}.}
Оценим сверху нормировочный множитель $\frac{1}{t_n}$, где $t_n := T_n\hr{\frac{M + \mu}{M - \mu}}$.
А для его вычисления нам нужно будет вспомнить явную формулу для многочленов Чебышёва:
\eqn{T_n(x) = \frac12\hs{\hr{x + \sqrt{x^2+1}}^n + \hr{x - \sqrt{x^2+1}}^n}.}
Вычислим $t_n$ по этой формуле:
\eqn{t_n   =
\frac12\hs{
\hr{\frac{M + \mu + 2\sqrt{M\mu}}{M - \mu}}^n +
\hr{\frac{M + \mu - 2\sqrt{M\mu}}{M - \mu}}^n} =
\frac12\hs{
\hr{\frac{\sqrt{M} + \sqrt{\mu}}{\sqrt{M} - \sqrt{\mu}}}^n +
\hr{\frac{\sqrt{M} - \sqrt{\mu}}{\sqrt{M} + \sqrt{\mu}}}^n}.}
\begin{petit}
Далее у Кобелькова была ошибка! Он не в ту сторону оценил, поэтому получался полный бред.
А потом он ещё раз ошибся, и сработало свойство чётности ошибок.
А надо-то вот как:
\end{petit}
Второе слагаемое мы просто откинем (оно мал\'о по сравнению с первым).
Обозначив $t := \frac{\mu}{M}$, получаем
\eqn{
t_n \ge \frac12\hr{\frac{1 + \sqrt{t}}{1-\sqrt{t}}}^n.}
Отсюда
\eqn{\frac{1}{t_n} \le 2\hr{\frac{1 - \sqrt{t}}{1+\sqrt{t}}}^n \approx 2\hr{1 - 2\sqrt{t}}^n.}

Итак, мы считаем по формуле
\eqn{r_n = \frac1{t_n} \cdot T_n\hr{\frac{(M+\mu)I-2A}{M-\mu}}r_0,}
то есть в среднем на $n$ шагах знаменатель геометрической
прогрессии составляет $\sqrt[n]{2}\hr{1-2\sqrt t}$.
Отсюда несложно вывести, что для достижения точности $\ep$ нужно сделать порядка
\eqn{n \approx \sqrt{t}\ln \ep^{-1}}
итераций. При этом общее число шагов должно  быть
кратно $n$. В этом большой дефект метода. В реальности метод работает плохо, для улучшения его нужно переставлять корни.

Можно использовать  следующий алгоритм, предложенный в 1969 году Лебедевым (учеником Соболева) для перестановки корней при $n= 2^m$.
Пусть имеется некоторая нумерация корней на $(m-1)$\д м шаге (здесь $s := 2^{m-1}$)
\eqn{(b_0,b_1\sco b_s, s  + b_0 \sco s + b_s).}
Эта последовательность переходит в последовательность
\eqn{(s + 1 + b_0, b_0,\; s + 1 - b_1, b_1,\; s+1-b_2, b_2,\dots).}
Оказывается, в этом случае нормы промежуточных многочленов при вычислении ограничены константой~$1$.

\subsubsection{Линейный оптимальный процесс}

Недостаток метода Ричардсона заключался в том, что нужно было идти шагами по $n$. А мы хотим
метод, который давал бы оптимальную
аппроксимацию на каждом шаге при любом значении $n$. Оптимальным мы назовём такой
процесс, при котором на каждом шаге имеет место равенство
\eqn{r_n = \frac1{t_n}\cdot T_n\ub{\hr{\frac{(M+\mu)I-2A}{M-\mu}}}_{p}r_0, \quad t_n := T_n\hr{\frac{M+\mu}{M-\mu}}.}

Напишем три последовательных шага метода. Получим соотношения
\eqn{\case{
t_{n-1} r_{n-1} &= T_{n-1}(p) r_0,\\
t_n r_n &= T_n(p) r_0,\\
t_{n+1} r_{n+1} &= T_{n+1}(p) r_0,
}}
Вспомним, что для $T_n$ выполнены рекуррентные соотношения
\eqn{T_{n-1}(p) -2p T_n(p) + T_{n+1}(p)=0.}
Умножим второе уравнение  на $-2p$ и сложим.
Получаем соотношение
\eqn{t_{n+1} r_{n+1} -2t_n \frac{M+\mu}{M-\mu}r_n + \frac{4t_n}{M-\mu} A r_n + t_{n-1} r_{n-1} = 0.}
Так как $r_n = x_n - \ol x$, то, в силу равенства
\eqn{\label{eqn:LOPChebRecurrent}t_{n+1} -2t_n \frac{M+\mu}{M-\mu} + t_{n-1}= 0,}
получаем
\eqn{t_{n+1} x_{n+1} -2t_n \frac{M+\mu}{M-\mu}x_n + \frac{4t_n}{M-\mu} (Ax_n-b) + t_{n-1} x_{n-1} = 0.}
Отсюда получаем процедуру оптимального поиска решения,
которая использует только три <<слоя>>.

Осталось сделать данную процедуру устойчивой. В силу равенства
\eqref{eqn:LOPChebRecurrent} можно переписать соотношение так:
\eqn{t_{n+1} x_{n+1} - (t_{n+1}+ t_{n-1}) x_n + \frac{2(t_{n+1}+t_n)}{M+\mu} (Ax_n-b) + t_{n-1} x_{n-1} = 0.}
Поделим всё на $t_{n+1}$  и введём обозначение $\om_n := -\frac{t_n}{t_{n+1}}$.
Тогда
\eqn{x_{n+1} - (1+ \om_{n-1}\om_n) x_n + \frac{2}{M+\mu} (1+ \om_{n-1}\om_n)(Ax_n-b)+ \om_{n-1}\om_n x_{n-1} = 0.}
Имеем $\om_0 = -\frac{t_0}{t_1} = -\frac{M-\mu}{M+\mu}$.
Очевидно, $|\om_0| < 1$ по построению.

Разделив соотношение~\eqref{eqn:LOPChebRecurrent} на $t_{n+1}$, получаем
\eqn{1+ 2\om_n \frac{M+\mu}{M-\mu} + \om_{n-1}\om_n = 0,}
откуда
\eqn{\om_n = -\frac{1}{2\frac{M+\mu}{M-\mu}+\om_{n-1}},}
а значит, $|\om_n| < \de(M, \mu) < 1$, и потому процесс сходится.

Чтобы сделать первый шаг, нужно вычислить $x_1$ по формуле
\eqn{x_1 = x_0 - \frac{2}{M+\mu} (Ax_0-b).}

Среди недостатков этого процесса можно отметить то, что в нём используется
информация о спектре матрицы. В реальном мире спектр далеко не всегда известен даже приблизительно.

\subsection{Другие методы}

\subsubsection{Метод скорейшего спуска}

Мы будем рассматривать симметричные положительно определённые матрицы $A$.
Рассмотрим функционал
\eqn{F(x) := (Ax,x) - (2b,x).}
Пусть $\ol x$\т точное решение, то есть $A\ol x = b$. Заметим, что
\eqn{\br{A(x-\ol x), x - \ol x} = (Ax,x) - 2(b,x) + (A\ol x, \ol x) = F(x) + (A\ol x, \ol x).}
Поскольку $A$\т положительно определённая матрица, имеем $\br{A(x - \ol x), x - \ol x} \ge 0$,
причём равенство нулю достигается тогда и только тогда, когда $x = \ol x$.
Значит,
\eqn{\arg\min F(x) = \ol x.}

Введём норму, задаваемую матрицей $A$:
\eqn{\hn{x}_A^2 := (Ax,x).}

Метод скорейшего спуска устроен следующим образом:
\eqn{x_{n+1} = x_n - \al_n \grad F(x_n).}
Несложно видеть, что (здесь верхние индексы обозначают координаты векторов)
\eqn{\pf{F}{x^k} = 2(Ax-b)^k.}
Обозначим $\xi_n := Ax_n - b$, $\de_n := 2\al_n$.
С учётом этого обозначения, задача переписывается в виде
\eqn{x_{n+1} = x_n - \de_n \xi_n.}
Итак, нам нужно найти $\min$ функционала $F$ по $\de_n$:
\mln{F(x_{n+1}) =\br{A(x_n - \de_n \xi_n), x_n - \de_n\xi_n} - 2(b,x_n- \de_n\xi_n)=\\=
(Ax_n,x_n) - 2\de_n(Ax_n,\xi_n) + \de_n^2(A\xi_n,\xi_n) - 2(b,x_n) + 2\de_n(b,\xi_n).}
Дифференцируем по $\de_n$, получаем
\eqn{F'(x_{n+1}) = 2\de_n\hn{\xi_n}_A^2 - 2 (Ax_n - b,\xi_n) = 2\de_n\hn{\xi_n}_A^2 - 2\hn{\xi_n}^2.}
Приравнивая эту производную, как и положено, к нулю, получаем
\eqn{\de_n = \frac{\hn{\xi_n}^2}{\hn{\xi_n}^2_A}.}

Рассмотрим
\eqn{y := x_n - \frac{2}{M + \mu}(Ax_n - b).}
Это шаг в направлении градиента $F$. Поскольку при вычислении $x_{n+1}$
мы выбирали параметр оптимально, а при вычислении $y$\т рассмотрели некоторое фиксированное его значение,
то во всяком случае имеем
\eqn{\label{eqn:OptStepIneq}F(x_{n+1}) \le F(y).}
Теперь можно оценить погрешность, причём нам будет удобно оценивать её в норме $\hn{\cdot}_A$.
Из определения $F$ следует, что
\eqn{\label{eqn:OptFDef}F(x) = \hn{x - \ol x}_A^2 - \hn{\ol x}_A^2.}
Заметим, что имеет место равенство
\eqn{y - \ol x = x_n - \ol x - \frac{2}{M +\mu} A(x_n - \ol x).}
Отсюда
\eqn{r := y - \ol x = \hr{E - \frac{2}{M + \mu}A}r_n.}
Применим к последнему равенству матрицу $A$:
\eqn{Ar = \hr{E - \frac{2}{M + \mu}A}A r_n.}

Применим известное утверждение из линейной алгебры: для всякого самосопряжённого оператора существует
ортогональный собственный базис $\hc{e_1\sco e_m}$, и пусть $e_i \sim \la_i$.
Пусть $r_n = \sum c_i e_i$, тогда
\eqn{
r = \sum \hr{1 -\frac{2\la_i}{M + \mu}} c_i e_i,\qquad
Ar = \sum \hr{1 - \frac{2\la_i}{M + \mu}} c_i \la_i e_i,}
откуда
\eqn{\hn{r}^2_A = (Ar,r) = \sum \hr{1 - \frac{2\la_i}{M + \mu}}^2 c_i^2 \la_i.}
Заменяя $\la_i$ в скобках на самое маленькое, что там может быть, а именно $\mu$, получаем
оценку
\eqn{\hn{r}^2_A \le \hr{\frac{M - \mu}{M + \mu}}^2 \sum  c_i^2 \la_i = \hr{\frac{M - \mu}{M + \mu}}^2 \hn{r_n}^2_A,}
следовательно,
\eqn{\label{eqn:OptConvCoeff}\hn{r}_A \le \frac{M - \mu}{M + \mu} \hn{r_n}_A.}
Из формул~\eqref{eqn:OptStepIneq} и \eqref{eqn:OptFDef} следует, что
\eqn{\hn{x_{n+1} - \ol x}_A \le \hn{y - \ol x}_A,}
то есть
\eqn{\hn{r_{n+1}}_A \le \hn{r}_A.}
Комбинируя это неравенство с неравенством~\eqref{eqn:OptConvCoeff}, получаем
\eqn{\hn{r_{n+1}}_A \le \frac{M - \mu}{M + \mu} \hn{r_n}_A,}
то есть имеет место сходимость со скоростью геометрической прогрессии.
То, что мы оценили сходимость в какой\д то левой норме, нас не смущает,
потому что пространство конечномерно и все нормы эквивалентны.

У этого метода имеется один недостаток: на каждом шаге приходится вычислять $\xi_n$,
то есть умножать матрицу на вектор. Кроме того, при вычислении $\de_n$ опять придётся
умножать матрицу на вектор. Этот недостаток можно вылечить следующим образом:
вычислять $\xi_{n+1} = \xi_n - \de_n A \xi_n$, но при этом тащить за собой не только сам вектор невязки $\xi_n$,
но и его произведение на матрицу, то есть вектор $A\xi_n$. Тащить ещё один вектор\т это небольшая потеря памяти, поскольку
памяти под саму матрицу $A$ всё равно нужно на порядок больше.

\subsubsection{Метод Ричардсона для несимметричных матриц}

Здесь мы рассмотрим итерационный метод, который нам уже встречался. НО теперь мы будем решать уравнение $Ax = b$
с несимметричной матрицей.

\begin{theorem}
Пусть  $A_s := \frac12(A + A^t) > 0$. Тогда существует $\tau_0 > 0$, такое что для всех $\tau \in (0,\tau_0)$
метод
\eqn{\frac{x_{n+1} - x_n}{\tau} + A x_n = b}
сходится.
\end{theorem}
\begin{proof}
Мы уже знаем, что со всякой симметричной положительно определённой матрицей $B$
можно связать норму $\hn{x}^2_B := (Bx,x)$.

Пусть $\ol x$\т точное решение, $r_n$\т погрешность на $n$\д м шаге. Тогда наш метод
перепишется в виде
\eqn{\frac{r_{n+1} - r_n}{\tau} + Ar_n = 0.}
Введём обозначения $r := r_n$, $\wh r := r_{n+1}$, $r_t := \frac{\wh r - r}{\tau}$. Тогда имеет место тождество
$\wh r = r + \tau r_t$.

В новых обозначениях метод имеет вид $r_t + Ar = 0$. Пусть $A_a$\т кососимметрическая часть
матрицы, то есть $A_a := \frac12(A - A^t)$. Тогда $A = A_s + A_a$, следовательно,
метод можно переписать так:
\eqn{r_t + A_s r + A_a r = 0,}
или, подставляя $r$ во втором слагаемом,
\eqn{\label{eqn:NotSymIter}\ub{(E - \tau A_s)}_B r_t + A_s\wh r  + A_a r = 0.}
Очевидно, матрица $B$ симметрична. Кроме того, ясно, что при всех достаточно малых $\tau$ матрица $B$ будет положительно определённой.
Будем считать, что $\tau_0$ уже выбрано так, чтобы при $\tau \in (0,\tau_0)$ имеем $B > 0$.

Имеет место тождество $\wh r = \frac{\wh r + r}{2} + \frac{\tau}{2} r_t$,
поэтому $2\tau \wh r = \tau (\wh r + r) + \tau^2 r_t$.
Отсюда
\mln{(B r_t, 2\tau \wh r) =
\br{B r_t, \tau(\wh r + r)} + (B r_t, \tau^2 r_t)=
\br{B(\wh r - r), \wh r + r} + \tau^2(B r_t, r_t)=\\=
(B\wh r, \wh r) - (B r, \wh r) + (B \wh r, r) - (B r, r) + \tau^2(B r_t, r_t) =
\hn{\wh r}^2_B - \wh{r}^2_B + \tau^2\hn{r_t}^2_B.}
Теперь умножим скалярно равенство~\eqref{eqn:NotSymIter} на $2\tau \wh r$,
получим
\eqn{\hn{\wh r}^2_B - \hn{r}_B^2 + \tau^2\hn{r_t}_B^2 + 2\tau \hn{\wh r}^2_{A_s} + 2\tau (A_a r, \wh r) = 0.}
В силу кососимметричности матрицы $A_a$ имеем $(A_a \wh r, \wh r) = 0$, поэтому
\eqn{(A_a r, \wh r) = (A_a \wh r, \wh r) - \tau (A_a r_t, \wh r) = -\tau (A_a r_t, \wh r).}
Поэтому можно переписать равенство, написанное выше, следующим образом:
\eqn{\hn{\wh r}^2_B - \hn{r}_B^2 + \tau^2\hn{r_t}_B^2 + 2\tau \hn{\wh r}^2_{A_s} - 2\tau^2 (A_a r_t, \wh r) = 0.}

Все операторы у нас ограниченные, поэтому найдутся такие константы $C_1$ и $C_2$, что
\eqn{\hn{A_a y}^2 \le C_1 \hn{y}^2_B, \quad \hn{y}^2 \le C_2 \hn{y}^2_{A_s}.}
Нам потребуется неравенство $2\hm{ab} \le \ep a^2 + \frac1\ep b^2$.
Оценим скалярное произведение с помощью этого неравенства:
\eqn{2\tau^2\hm{(A_a r_t, \wh r)} \le \tau^2 \ep \hn{A_a r_t}^2 + \frac{\tau^2}{\ep} \hn{\wh r}^2 \le
C_1\ep\tau^2\hn{r_t}_B^2 + \frac{C_2\tau^2}{\ep}\hn{\wh r}^2_{A_s},}
поэтому
\eqn{\hn{\wh r}^2_B - \hn{r}_B^2 + \tau^2(1 - C_1\ep)\hn{r_t}_B^2 + \tau\hr{2 - \frac{C_2\tau}{\ep}}\hn{\wh r}^2_{A_s} \le 0.}
Поскольку $\ep$ можно брать любым, положим $\ep := \frac1{C_1}$, тогда третье слагаемое умрёт.
Кроме того, уменьшая при необходимости значение $\tau_0$, можно добиться того, что
$2 - \frac{C_2\tau}{\ep} = 2 - C_1C_2\tau \ge 1$ при $\tau \in (0,\tau_0)$.
После всего этого шаманства получаем неравенство
\eqn{\hn{\wh r}^2_B +\tau\hn{\wh r}_{A_s}^2 \le \hn{r}_B^2.}

В силу эквивалентности норм найдётся константа $C_3$, для которой
\eqn{\hn{\wh r}^2_{A_s} \ge C_3\hn{\wh r}^2_B.}
Следовательно, имеем неравенство
\eqn{(1 + C_3 \tau) \hn{\wh r}_B^2 \le \hn{r}_B^2,}
то есть, в старых обозначениях,
\eqn{\hn{r_{n+1}}_B^2 \le \frac{1}{1 + C_3 \tau} \hn{r_n}_B^2,}
откуда следует сходимость метода со скоростью геометрической прогрессии.
\end{proof}

\begin{petit}
Тут ещё были какие\д то слова про чебышёвское ускорение и про то, что если спектр лежит в эллипсе,
но всё равно ничего не понятно.
\end{petit}


\subsubsection{Метод решения симметричных плохо обусловленных систем}

\begin{petit}
Название придумано самостоятельно. Суть метода состоит в введении переобуславливателя.
Во всяком случае лучше соответствует тексту, чем то, что написано в билетах.
Судя по всему, эквивалентно билету про <<итерационные методы со спектрально эквивалентными операторами>>.

А ещё в книжке на странице примерно 301 транспонирование у матрицы $D$ (см. ниже) всё-таки забыто не было.
Если оно и правда должно там быть (а это правдоподобно), тогда всё становится на свои места.
\end{petit}

Пусть $A$\т симметрическая положительно определённая (но плохая) матрица, и пусть мы решаем
систему методом итераций:
\eqn{\frac{x_{n+1} - x_n}{\tau} + A x_n = b.}
Мы знаем, что если $\frac{M}{\mu} \gg 1$, тогда при реальных вычислениях
всё разлетится. Будем это лечить. Пусть $B$\т симметрическая положительно определённая матрица,
причём такая, чтобы система $By = c$ легко решалась.

Заменим исходную систему на такую систему:
\eqn{B\frac{x_{n+1} - x_n}{\tau} + A x_n = b.}
Большой беды в этом нет, потому что мы легко сможем вернуться к старой системе.
Рассмотрим матрицу $\sqrt{B}$: она, ясное дело, тоже будет симметрической.
Перепишем систему в терминах погрешностей:
\eqn{B\frac{r_{n+1} - r_n}{\tau} + A r_n = 0.}
Теперь сделаем замену переменной: положим $y_n := \sqrt{B} r_n$, $D := \hr{\sqrt{B}}^{-1}$.
Тогда получим следующее:
\eqn{\frac{y_{n+1} - y_n}{\tau} + \ub{D^t A D}_C y_n = 0.}
Совсем легко проверить, что матрица $C$ тоже будет симметрической и положительно определённой.
Таким образом, задача сведена к предыдущей.

А чем это нам помогает? А вот чем. Имеет место равенство
\eqn{
\la(А)_{\min} = \min \frac{\hr{D^t A D y,y}}{(y,y)},\quad \la(А)_{\max} = \max \frac{\hr{D^t A D y,y}}{(y,y)}.}
Сделаем замену $z := Dy$, тогда получим
\eqn{\frac{\hr{D^t A D y,y}}{(y,y)}  = \frac{(Az,z)}{(B z,z)}.}

\begin{df}
Отношение $\frac{(Ax,x)}{(x,x)}$ называется \emph{отношением Релея}.
\end{df}

Поскольку матрицы $A$ и $B$ задают какие\д то нормы, а в конечномерном пространстве все нормы эквивалентны,
всегда найдутся такие константы $\ga_1$ и $\ga_2$, что
\eqn{\ga_1 (B y,y) \le (A y,y)  \le \ga_2 (B y, y).}
Отсюда следует, что $\la(A)_{\min} \ge \ga_1$ и $\la(A)_{\max} \le \ga_2$.
Отсюда следует, что если матрицу $B$ выбрать <<близкой>> к матрице $A$, то после переобуславливания
мы получим систему с хорошим спектром. Оценим скорость сходимости:
имеем
\eqn{\hn{y_n}^2 = (\sqrt B r_n, \sqrt B r_n) = \hn{r_n}^2_B,}
кроме того, применяя те же методы оценки, что и раньше,
получаем
\eqn{\hn{y_{n+1}}^2 \le \frac{\ga_2 - \ga_1}{\ga_2 + \ga_1} \hn{y_n}^2,}
поэтому получаем оценку
\eqn{\hn{r_{n+1}}^2_B \le \frac{\ga_2 - \ga_1}{\ga_2 + \ga_1} \hn{r_n}^2_B.}

\begin{ex}
Пусть $J_k(\la)$\т матрица, у которой числами $\la$ заполнена только $k$\д я строка над диагональю ($k = 0$\т сама диагональ).
Положим $S_k(\la) := \frac{1}{2}\hr{J_k(\la) + J_k^t(\la)}$.
Рассмотрим матрицу $A := S_0(3) \bw+ S_1(1) \bw+ S_2\hr{\frac12} \bw+ S_4\hr{\frac12}$, у неё будет, понятное дело,
$7$ ненулевых диагоналей. Тогда положим $B := S_0(3) \bw+ S_1(1)$ (она будет трёхдиагональной).
Несложно показать, что
\eqn{\frac12(Bx,x) \le (Ax,x) \le 2(B x,x),}
а число $\tau$, стало быть, можно брать равным $\frac{2}{\ga_1 + \ga_2} = \frac{2}{2 + \frac12} = \frac45$.
\end{ex}

\subsubsection{Метод Зейделя}

Этот метод годится для решения систем $Ax = b$, у которых $a_{ii} \ne 0$.
Представим матрицу $A$ в виде $A = B + C$, где $С$\т строго верхнетреугольная (на диагонали стоят нули),
а $B$\т нижнетреугольная. В силу условия на диагональные элементы, матрица $B$ обратима.

Процесс выглядит так:
\eqn{Bx_{n+1} + C x_n = b,}
или, переходя на язык погрешностей,
\eqn{r_{n+1} = -  B^{-1}C r_n.}
Ясно, что сходимость такого метода будет тогда и только тогда, когда $\hn{B^{-1}C} < 1$.

\begin{theorem}
Если $A = A^t > 0$, то метод сходится.
\end{theorem}

\begin{note}
Условие на диагональные элементы обеспечивается автоматически свойством $A > 0$.
\end{note}

\begin{proof}
Мы покажем, что метод Зейделя и метод покоординатного спуска\т это одно и то же.
Рассмотрим функционал
\eqn{F(x) = (Ax,x) - 2(b,x).}
Как мы уже выясняли при изучении метода градиентного спуска, (единственный) минимум этого
функционала и есть искомое решение системы. На каждом шаге будем
постепенно уточнять значения координат решения $(x_1^n\sco x_m^n)$, получая в конце шага
новый вектор $(x_1^{n+1}\sco x_m^{n+1})$.

Пусть мы уже уточнили координаты с номерами $1$ по $k-1$. Уточним $k$\д ю координату.
Продифференцируем функционал $F$ по переменной $x_k$ и приравняем её к нулю,
получим
\eqn{\pf{F}{x_k} = 2\hr{\suml{j=1}{k-1} a_{kj} x^{n+1}_j + a_{kk} x_k + \suml{j=k+1}{m} a_{kj} x_j^n -b_k x_k}= 0,}
то есть это действительно покоординатный спуск.
Теперь осталось понять, почему метод сходится.
Метод работает таким образом, что на каждом шаге имеет место
неравенство $F(x_{n+1}) < F(x_n)$. Пользуясь формулой~\eqref{eqn:OptFDef},
получаем, что его можно переписать в виде
\eqn{\frac{\hn{x_{n+1} - \ol x}_A^2}{\hn{x_n - \ol x}_A^2} < 1.}
Переходя к погрешностям, получаем
\eqn{\frac{\hn{r_{n+1}}_A^2}{\hn{r_n}_A^2} = \frac{\hn{B^{-1}Cr_n}_A^2}{\hn{r_n}_A^2} < 1.}
Выражение в левой части не меняется при замене $r_n$ на $c r_n$, поэтому можно считать, что
$\hn{r_n}_A = 1$. Таким образом, получаем непрерывную функцию на единичной сфере,
поэтому она достигает там своего максимума $M < 1$. Значит, сходимость будет,
но оценить её скорость невозможно.
\end{proof}

\subsubsection{Метод сопряжённых градиентов}

Рассматриваем систему $Ax = b$ с симметричной положительно определённой матрицей.
Снова рассматриваем функционал
\eqn{F(y) := (Ay,y) - 2(b,y).}
Имеем
\mln{F(y) = (Ay, y) - (b,y) - (b,y) + (A\ol x, \ol x) - (A\ol x, \ol x)=\\=
(Ay,y) - (A\ol x, y) - \ub{(\ol x, Ay)}_{(A\ol x, y)} + (A\ol x, \ol x) - (A\ol x, \ol x)=
(A(y - \ol x), y - \ol x) - (A\ol x, \ol x) =\\= \br{A^{-1}(Ay-b), Ay-b} - \hn{x}_A^2 =
\hn{Ay - b}^2_{A^{-1}} - \hn{\ol x}^2_A.}

Положим $\xi_n := x_n - \ol x$. Обычный итерационный процесс устроен примерно так:
$\xi_n = P_n(A)\xi_0$, где $P_n(A)$\т некоторый многочлен степени $n$, у которого $P_n(0) = 1$.
Применим к $\xi_n$ оператор $A$, получим
\eqn{A\xi_n = A P_n(A) \xi_0 = P_n(A) A\xi_0 = P_n(A)r_0,}
а с другой стороны $A\xi_n = r_n$, поэтому $P_n(A) r_0 = r_n$.

Пусть задано начальное приближение $x_0$. Хотим построить процесс,
который при всяком $n$ использует многочлен $P_n(A)$, зависящий, вообще говоря, от $x_0$,
такой что
\eqn{\hn{r_n}_{A^{-1}} \ra \minl{P_n}.}

Положим $v_i := A^i r_0$.
Минимизируем $\hn{r_n}_{A^{-1}}$ (сейчас мы заново докажем, что перпендикуляр к подпространству\т это минимальное расстояние до него).
Пусть $c_0\sco c_n$\т коэффициенты многочлена $P_n$ (здесь $c_0 = 1$).
Тогда имеем
\eqn{r_n = \suml{i=0}{n} c_i v_i.}
Продифференцируем по $c_k$ ($k = 1\sco n$):
\mln{
\pf{}{c_k} \hn{r_n}^2_{A^{-1}} =
\pf{}{c_k} (A^{-1}r_n, r_n) =
\hr{\pf{}{c_k} A^{-1} r_n, r_n} + \hr{A^{-1} r_n, \pf{}{c_k} r_n} =\\=
\hr{A^{-1}v_k, r_n} + \hr{A^{-1} r_n, v_k} =
\hr{A^{-1}v_k, r_n} + \hr{r_n, A^{-1}v_k} =
2\hr{A^{-1}v_k, r_n}.}
Приравнивая частные производные к нулю, находим, что $r_n \bot A^{-1} v_k = v_{k-1} = A^{k-1} r_0$ при $k = 1\sco n$.

\begin{df}
Подпространство Крылова для оператора $A = A^t > 0$, порождённое вектором $e \in V$\т это
вот такое циклическое подпространство:
\eqn{\ha{e, A e, A^2e\etc}.}
\end{df}

Мы знаем, что у всякого симметрического оператора существует собственный базис.
Пусть $r_0$ как\д то разложен по собственным векторам $\hc{e_i}$ оператора $A$, отвечающим различным собственным значениям:
\eqn{r_0 = \suml{i =1}{q} r^i e_i,}
причём все $r_i$ отличны от нуля. Так всегда можно сделать, ибо линейная комбинация
векторов из собственного подпространства тоже является собственным вектором с тем же собственным значением,
значит, если в разложении по собственному базису вектора $r_0$ встретилось несколько векторов с одним собственным значением,
то их можно объединить в один.

Покажем, что векторы $r_0, Ar_0, A^2r_0\sco A^{q-1} r_0$ линейно независимы. В самом деле,
допустим, что имеется нетривиальная линейная зависимость между ними:
\eqn{0 =
\suml{j =0}{q-1} c^j A^j r_0 =
\suml{j=0}{q-1} c^j \suml{i=1}{q} r^i \la_i^j e_i =
\suml{i=1}{q} \bbr{\suml{j=0}{q-1} c^j r^i \la_i^j} e_i.}
Но $e_i$ отвечают различным собственным значениям и потому линейно независимы. Значит,
\eqn{\suml{j=0}{q-1} c^j r^i \la_i^j = 0, \quad i = 1\sco q.}
Это линейная система $q\times q$ на $c^j$ с матрицей $W(\la_1\sco \la_q)\cdot \diag(r^1\sco r^q)$,
а поскольку $\la_i$ различны, то матрица невырождена. Значит, у неё есть только нулевое решение,
поэтому все $c^j = 0$.

Отсюда следует корректность метода, поскольку доказанная линейная независимость означает,
что $r_n$ (или, что то же самое, коэффициенты многочлена $P_n$)
находится из системы уравнений $(r_n,A^{k-1} r_0)$ однозначно.

Рассмотрим подпространства $L_n := \ha{r_0\sco A^n r_0}$ и $R_n = \ha{r_0\sco r_n}$.
В силу того, что $r_n \bot A^k r_0$ при $k = 0\sco n-1$,
получаем, что векторы $r_i$ тоже линейно независимы, значит, они образуют базис пространства $R_n$.
А поскольку $r_k  = P_k(A) r_0$, то они выражаются через базис пространства $L_n$.
Отсюда следует, что $R_k = L_k$ при всех $k$. Мы уже знаем, что $r_n \bot L_{n_1} = R_{n-1}$,
стало быть, $r_n$ ортогонален всем предыдущим $r_i$.

Покажем, что $R_n = \ha{r_0\sco r_{n-1}, A r_{n-1}}$. В самом деле,
\eqn{A r_{n-1} = A P_{n-1}(A) r_0 = \ub{c_{n-1}}_{\ne 0} A^n r_0 + \ub{w_{n-1}}_{\in R_{n-1}}.}

Теперь разложим $r_n$ по этому базису:
\eqn{r_n = \suml{k=0}{n-1} \ga_k r_k  + \ga_n A r_{n-1}.}
Из взаимной ортогональности $r_j$ следует, что при $j < n-2$ имеем $\ga_j =0$.
В самом деле,
\eqn{0 = (r_n, r_j) = \ga_j(r_j,r_j) + \ga_n(A r_{n-1}, r_j).}
Второе скалярное произведение равно нулю: $(A r_{n-1}, r_j) = (r_{n-1}, A r_j) = 0$,
потому что $A r_j \in R_{n-2}$. Значит, $\ga_j = 0$.

Теперь вычислим три старших коэффициента, то есть $\ga_{n-2}$, $\ga_{n-1}$ и $\ga_n$.
Имеем систему из двух уравнений с тремя неизвестными:
\eqn{\begin{aligned}
0 = (r_n, r_{n-2}) &= \ga_{n-2} \hn{r_{n-2}}^2 + \ga_n(r_{n-1}, A r_{n-2}),\\
0 = (r_n, r_{n-1}) &= \ga_{n-1} \hn{r_{n-1}}^2 + \ga_n(r_{n-1}, A r_{n-1}).
\end{aligned}}
Нужно ещё одно. Мы уже получили, что
\eqn{r_n = \ga_{n-2} r_{n-2} + \ga_{n-1} r_{n-1} + \ga_n A r_{n-1}.}
Приравняем коэффициенты при $r_0$ в разложении этого вектора по базису $r_0\sco A^n r_0$,
получим ещё одно уравнение. Имеем
\eqn{P_n(A)r_0 = \ga_{n-2} P_{n-2}(A) + \ga_{n-1} P_{n-1}(A) + \ga_n A P_{n-1} (A) r_0.}
Последнее слагаемое свободного члена не имеет. Поскольку при всех $k$ имеем $P_k(0) = 1$,
получаем ещё одно недостающее уравнение $1 = \ga_{n-2} + \ga_{n-1}$.

Решая полученную систему из трёх уравнений, находим $\ga_{n-2}$, $\ga_{n-1}$ и $\ga_n$.
А теперь остаётся только получить формулу для решения (то есть от невязок перейти к векторам $x_n$):
\eqn{A x_n - b = \ga_{n-2} (A x_{n-2} - b) + \ga_{n-1} (A x_{n-1} - b) + \ga_n A (A x_{n-1} - b),}
а в силу условия $\ga_{n-2} + \ga_{n-1} = 1$ это можно упростить:
\eqn{A x_n = \ga_{n-2} A x_{n-2} + \ga_{n-1} A x_{n-1} + \ga_n A(A x_{n-1} -b).}
Осталось домножить слева на матрицу $A^{-1}$, и мы получим замечательную формулу:
\eqn{x_n = \ga_{n-2} x_{n-2} + \ga_{n-1} x_{n-1} + \ga_n (A x_{n-1} - b).}
Это и есть рекуррентная формула для вычисления итераций решения.

В силу конечномерности пространства, метод сойдётся не более, чем за $\rk A$ шагов (это и называется \emph{конечностью} метода).
На самом деле шагов будет ровно $q$ (напомним, что через $q$ было обозначено количество собственных векторов,
участвующих в разложении~$r_0$).

\subsection{Что делать, когда всё плохо?}

\subsubsection{Метод регуляризации по Тихонову}

Пусть, как обычно, у нас есть система $Ax = b$, причём $A = A^t > 0$ и $\ol x$\т точное решение.
Будем предполагать, что $\la_{\max} \sim 1$, а $\la_{\min} \ll 1$. Более точно, будем предполагать,
что $\la_1 \ge \la_2 \sge \la_m$, причём первые $n$ чисел имеют порядок $O(1)$.

Пусть $e_1\sco e_m$\т собственный ортонормированный базис оператора $A$, и пусть
\eqn{b = \suml{k=1}{m} b_k e_k.}
Сделаем предположение относительно решения:
\eqn{\ol x = \suml{k=1}{n} \frac{b_k}{\la_k} e_k,}
то есть на самом деле $b_{n+1} = \dots = b_m = 0$.

Поскольку в мире ничего идеального не бывает, правая часть системы на самом деле задана с погрешностью,
то есть $\wt b = b + \de b$ (здесь $\de b$\т единый символ). Стало быть, на самом деле мы решаем
систему $Ax = \wt b$, и у неё имеется точное решение $\wt x$.

Будем оценивать погрешность, которая появляется от замены $b$ на $\wt b$ и пытаться её уменьшить.
Имеем
\eqn{\wt x = \suml{k=1}{m} \frac{b_k + \de b_k}{\la_k} e_k =
\ub{\suml{k=1}{n} \frac{b_k}{\la_k} e_k}_{\ol x} + \suml{k=1}{m} \frac{\de b_k}{\la_k} e_k.}
Во второй сумме в знаменателе стоят маленькие числа. Поэтому, даже если погрешность $\de b$ невелика,
при делении почти на ноль она вырастет, и всё испортится.

Поэтому применяется такая идея. Давайте <<испортим>> матрицу $A$. А именно,
заменим матрицу $A$ на матрицу $A_\al := A + \al E$ (как говорят, рассмотрим возмущение).
Тогда, как несложно видеть,
\eqn{x_\al = \suml{k=1}{m} \frac{b_k + \de b_k}{\la_k + \al} e_k.}
Вычислим
\eqn{x_\al - \ol x = \suml{k=1}{m} \frac{b_k + \de b_k}{\la_k + \al} e_k - \suml{k=1}{n} \frac{b_k}{\la_k} e_k=
\ub{\suml{k=1}{n} b_k \hr{\frac{1}{\la_k + \al} - \frac{1}{\la_k}} e_k}_{=:\ph_1} + \ub{\suml{k=1}{m} \frac{\de b_k}{\la_k + \al} e_k}_{=:\ph_2}.}
Заметим, что поскольку $\la_1\sco \la_n$ имеют порядок $O(1)$, разность
\eqn{\frac{1}{\la_k + \al} - \frac{1}{\la_k} = - \frac{\al}{(\la_k + \al)\la_k}}
имеет порядок $O(\al)$. Отсюда
\eqn{\hn{\ph_1}^2 = \suml{k=1}{n} b_k^2 \frac{\al^2}{\la_k^2 (\la_k + \al)^2} \le C\hn{b}^2\al^2,}
где константа $C_1$ как\д то зависит от спектра матрицы $A$.

Для оценки $\ph_2$ вообще забудем про то, что у нас в знаменателях стоят $\la_k$ (мысленно положим их нулями),
от этого сумма только возрастёт.
Тогда получим оценку
\eqn{\hn{\ph_2}^2 \le \frac{1}{\al^2} \hn{\de b}^2.}
Отсюда, полагая $C := \sqrt{C_1}$, получаем оценку для разницы между точным решением и тем,
что получится после регуляризации:
\eqn{\hn{\wt x - \ol x} \le C \hn{b} \al + \frac{1}{\al} \hn{\de b}.}
Осталось её минимизировать за счёт выбора $\al$ (обозначим это оптимальное решение $\al_*$):
\eqn{C \hn{b} - \frac{\hn{\de b}}{\al_*^2}= 0,}
откуда
\eqn{\al_* = \sqrt{\frac{\hn{\de b}}{C\hn{b}}}, \quad \hn{\wt x - \ol x} \le 2\sqrt{C\hn{b}\cdot \hn{\de b}}.}

\subsubsection{Метод Поспелова для решения плохо обусловленных систем}

Этот метод применяется для решения несовместных систем. Что это значит? Это значит, что мы будем искать решение $x$, для которого
норма $\hn{Ax-b}$ минимальна. То есть мы ищем перпендикуляр (то есть кратчайшее расстояние) к гиперплоскости, задающей решение системы.
Сам по себе метод состоит в <<покоординатном спуске>> к этому перпендикуляру.

\begin{petit}
А далее идёт <<Метод Поспелова>> в вольном изложении А.\,Воронцова.

\medskip

Все написанное ниже\т описание того, как я понимаю, что такое метод
Поспелова. Не исключено, что я сильно заблуждаюсь и речь в этом
вопросе должна идти о чем-то совсем другом. Те рассуждения, которые
я додумал сам, я буду выделять \textsl{вот таким шрифтом}.

\end{petit}

Итак, мы хотим научиться решать уравнение $Ax=b$, не предполагая
ничего о матрице $A$. Ясно, что если матрица вырождена, то система
может не иметь решений или иметь не единственное решение.

\textsl{Та же проблема возникает, если мы решаем задачу с невырожденной
матрицей, которая близка к вырожденной, например: $A=C+\varepsilon
I$, $\det C=0$. Такая матрица из-за вычислительных погрешностей
может вести себя как вырожденная.}

Заменим исходную задачу на задачу минимизации функционала
$\Phi=\|Ax-b\|^2$. Оказывается, что эта задача поставлена корректно.
Действительно, рассмотрим вариацию функционала
\eqn{\Phi(x+\delta)=\|Ax-b+A\delta\|^2=\|Ax-b\|^2+2(A^T(Ax-b),
\delta) + \|A\delta\|^2.} Это значит, условие экстремума ---
равенство $A^TAx=A^Tb$. Оказывается, что эта задача всегда имеет
решение. Действительно, по теореме Фредгольма для того чтобы система
имела решение необходимо и достаточно, чтобы $A^Tb$ было
ортогонально ядру $A^TA$.

\begin{stm}
Ядра операторов $A^TA$ и $A$ совпадают.
\end{stm}
\begin{proof}
$Ax=0 \Ra A^TAx=0$, $A^TAx=0$, поэтому $0=(A^TAx,x)=(Ax,Ax)$. Значит, $Ax=0$.
\end{proof}

\begin{stm}
$A^Tb \perp \Ker A^TA$.
\end{stm}
\begin{proof}
Пусть $x\in \Ker A^TA \Ra Ax=0$ (по предыдущему утверждению). Тогда $(A^Tb, x)=(b, Ax)=0$.
\end{proof}

Замена уравнения $Ax=b$ уравнением $A^TAx=A^Tb$ называется \emph{первой трансформацией Гаусса}.

Осталось понять, как решать новую систему. Попытаемся минимизировать
функционал $\Phi$ методом оптимального покоординатного спуска. Для
этого сначала зафиксируем базис, в котором будем искать решение:
выберем $w_1, \dots, w_q$ такие, что $\|Aw_i\|\ne0$ и $Aw_i$ линейно
независимы.

 \textsl{Собственно, насколько я понимаю, весь фокус в том,
как выбирается этот базис. Я думаю, что надо перед тем как добавить
очередной вектор $w_n$ проверить, что $\hn{Aw_n} > \varepsilon$  и
получившаяся система достаточно линейно независима (то есть если
представить $Aw_n=e_n+r$, где $r\in \ha{Aw_1,\dots,Aw_{n-1}}$, а $e_n$ ортогонально этому пространству
$\hn{e_n}$ должна быть больше $\ep$).}

\textsl{В частности, если рассмотреть матрицу, в которой первые $q$
собственных значений имеют порядок 1, а остальные $\lambda_i\ll 1$,
то мы выберем базис $e_1,\dots, e_q$. }

 Далее организуем процесс вычислений следующим образом. Пусть у нас есть значение $x^n$.
 Индекс у $x$ вверху означает номер итерации.  Для каждого $k$ определим $c_k$, которое минимизирует функционал
 $\Phi(x^n+c_kw_k)$. Это значение определяется из уравнений
\eqn{\Phi(x^n+c_kw_k)= \|Ax^n-b\|^2+c_k^2\|Aw_k\|^2+2c_k(Aw_k, Ax^n-b),}
 \eqn{c_k=-\frac{(Aw_k, Ax-b)}{\|Aw_k\|^2}.}

Из всех векторов $w_i$ выбираем тот, спуск вдоль которого дает
наибольший выигрыш (в смысле уменьшения значения функционала
$\Phi$).

Остается показать, что описанный процесс сходится. Для этого нам
понадобится
\begin{lemma}
 Пусть $g_1, \dots, g_k$\т линейно независимые единичные векторы.
 Обозначим $L_k = \ha{g_1, \dots, g_k}$. Тогда существует
 $\gamma$ такое, что для любого $x\in L_k$
 \eqn{\|x-(x, g_i)g_i\|\le\gamma\|x\|.}
 Здесь $i$ выбирается так, чтобы значение $|(x, g_i)|$ было
 максимальным.
\end{lemma}
\begin{proof}
Предположим противное. Пусть существует последовательность $x_k$,
таких что $\|x_k\|=1$ и \eqn{\|x_k-(x_k, g_i)g_i\|>1-\frac{1}{k}.}
На единичной сфере можно выбрать подпоследовательность, которая
сходится к некоторому элементу $x^*$. Для этого элемента имеем
\eqn{\|x^*-(x^*, g_i)g_i\|=\|x^*\|^2-2(x^*, g_i)^2+(x^*,
g_i)^2\ge1.} Поскольку $\|x^*\|=1$ получаем, что $(x^*, g_i)=0$ для всех $i$. А поскольку $g_i$ образуют базис в $L_k$, это означает,
что $x^*=0$. Эта лемма означает, что если рассмотреть проекцию
невязки на подпространство, натянутое на $Aw_1, \dots, Aw_q$, ее
норма будет убывать со скоростью геометрической прогрессии. Что и
требовалось.
\end{proof}

\textsl{От метода Тихонова метод Поспелова, насколько я понимаю,
отличается тем, что здесь мы просто забили на тот кусок матрицы,
который плохо обусловлен, и считаем, что решение имеет вид $(x_1,
\dots, x_q, 0,\dots,0)$.}


\section{Нелинейные и дифференциальные уравнения}

\subsection{Нелинейные уравнения}

Пусть нам задано нелинейное уравнение $f(x) = 0$.

\subsubsection{Метод половинного деления}

Если известно, что функция хорошая,
и известны две точки $x_0$ и $x_1$, для которых $f(x_0) f(x_1) < 0$, то можно запускать метод
половинного деления: делим отрезок пополам, смотрим, на котором из двух новых отрезков
значения разных знаков, выбираем его и так далее.

Он, разумеется, плох тем, что совсем не обобщается на многомерный случай.
Кроме того, на функцию накладываются существенные ограничения.

\subsubsection{Метод простой итерации}

Метод простой итерации заключается в том, что мы переходим от уравнения $f(x) = 0$
к эквивалентному уравнению $x = g(x)$, а затем рассматриваем итерационный процесс $x_{n+1} = g(x_n)$.
Если отображение $g$ сжимающее, то он сойдётся к некоторому решению этого уравнения.

В одномерном случае достаточным условием для сжимающего отображения
будет условие $0 < \hm{g'(x)} < 1$ в том интервале, куда попадают итерации отображения.
В самом деле, пусть $\ol x$\т точное решение. Тогда
\eqn{x_{n+1} - \ol x = g\br{\ol x + (x_n - \ol x)} - g(\ol x).}
Разложим правую часть в ряд Тейлора:
\eqn{x_{n+1} - \ol x = a_1 (x_n - \ol x) + a_2(x_n - \ol x)^2 + \dots}
Если $g'(x) \ne 0$, то $a_1 \ne 0$. А если $\hm{a_1} < 1$,
то, понятное дело, метод будет сходиться со скоростью геометрической прогрессии.

\subsubsection{Метод Ньютона}

Решаем уравнение $f(x) = 0$.
Будем считать, что наша функция достаточно хорошая (что под этим понимается, уточним чуть позже).
Напишем формулу Тейлора:
\eqn{f(\ol x) = f(x) + f'(\xi)(\ol x - x), \quad \xi = \xi(x).}
Имеем $f(\ol x) = 0$, поэтому возникает идея написать такой итерационный метод:
\eqn{f(x_n) + f'(x_n)(x_{n+1} - x_n) = 0,}
то есть
\eqn{x_{n+1} = x_n- \frac{f(x_n)}{f'(x_n)}.}
Это и есть метод Ньютона.

Будем считать, что функция $f$ строго дифференцируема, то есть
\eqn{\hn{ f(x) - f(y) - f'(y)(x-y)} \le M_1 \hn{x-y}^2.}
Чтобы при решении уравнения на $x_{n+1}$ не возникало проблем, естественно требовать
существования $(f'(x))^{-1}$. Более того, нам потребуется, чтобы в некоторой окрестности
она была ограничена некоторой константой $M_2$. Пусть $\hn{x_n - \ol x} \le b$.
Покажем, что при этих условиях отображение будет сжимающим.

В самом деле, имеем
\eqn{\hn{f(\ol x) - f(x_n) - f'(x_n)(\ol x- x_n)} \le M_1\hn{x_n - \ol x}^2.}
Снова замечая, что $f(\ol x) = 0$ и подставляя в эту формулу значение $f(x_n)$, получим
\eqn{\hn{f'(x_n)(x_{n+1} - x_{n} - \ol x +x_n)} \le M_1\hn{x_n - \ol x}^2,}
и после сокращений получаем
\eqn{\hn{f'(x_n)(x_{n+1} - \ol x)} \le M_1\hn{x_n - \ol x}^2,}
Обозначим $z := f'(x_n) (x_{n+1} - \ol x)$, тогда $x_{n+1} - \ol x = (f'(x_n))^{-1}z$,
откуда
\eqn{\hn{x_{n+1} - \ol x} \le M_2\hn{z} \le M_1M_2 \hn{x_n - \ol x}^2 \le M_1M_2 b^2.}
Значит, если мы хотим, чтобы всё это сходилось, нам нужно выбрать такую окрестность,
чтобы выражение справа было меньше $b$, значит, $b < \frac{1}{M_1 M_2}$.

Имеем
\eqn{\de_{n+1} := M_1M_2 \hn{x_{n+1} -\ol x} \le M_1^2 M_2^2 \hn{x_n -\ol x}^2.}
Значит, имеется неравенство $\de_{n+1} \le \de_n^2$. Значит,
\eqn{\de_n\le \de_0^{2^n},}
то есть скорость сходимости просто бешеная.

\begin{note}
Метод Ньютона в одномерном случае имеет довольно наглядный геометрический смысл,
поэтому его часто называют методом касательных.
\end{note}

\begin{ex}
Вычислим $\sqrt{a}$ с помощью метода Ньютона.
Рассмотрим уравнение $x^2 - a = 0$. Итерационный процесс имеет вид
\eqn{x_{n+1} = x_n - \frac{x_n^2-a}{2x_n} = \frac{x_n^2 + a}{2x_n}.}
\end{ex}

Вся проблема метода Ньютона\т это попасть в ту окрестность, в которой он начинает сходиться.

\subsection{Дифференциальные уравнения}

Рассмотрим простейшую задачу Коши:
\eqn{\case{y' = f(x,y),\\ y(0) = y_0.}}

\subsubsection{Метод Эйлера и его модификации}

Пусть нам известно значение $y(x)$. Будем строить значение $y(x+h)$.
Конечно, можно разложить функцию в ряд Тейлора. Но это плохо. Гораздо
лучше (с вычислительной точки зрения) поступать так: проинтегрируем наше уравнение от $x$ до $x+h$.
По формуле Ньютона\ч Лейбница получим
\eqn{y(x+h) = y(x) +\intl{0}{h} f\br{x+t, y(x+t)}\dt.}
Конечно, мы ничего не знаем про то, чему равна функция $y$ на отрезке $[x,x+h]$.
Поэтому мы заменим интеграл самой простой квадратурной формулой.
В итоге получим такое соотношение:
\eqn{y_{n+1} = y_n + h f(x_n,y_n).}
При этом значение интеграла будет вычислено с точностью до $O(h^2)$.
Поскольку нам нужно сделать $\frac{1}{h}$ шагов, то в итоге точность будет довольно низкой: $O(h)$.

Всё зависит от того, насколько точно выбирается квадратурная формула.
Модифицируем этот метод, выбрав другую формулу (формулу прямоугольников):
\eqn{y_{n+1} = y_n + h f\hr{x_n+\frac h2,y\hr{x_n+\frac h2}}.}
Эта формула даёт точность на каждом шаге $O(h^3)$, но есть одна неприятность:
мы ничего не знаем про $y\hr{x_n + \frac h2}$. Поэтому её нельзя использовать без дополнительных
ухищрений. Поступим так: вычислим $y_{n+\frac12}$ по обычному методу Эйлера:
\eqn{y_{n+\frac12} = y_n + \frac{h}{2} f(x_n,y_n).}
Она даёт погрешность $O(h^2)$, но поскольку перед $f$  в основной формуле уже есть один множитель $h$,
в итоге точность $O(h^3)$ не изменится.
Таким образом, схема вычислений в этом случае имеет вид
\eqn{\bcase{
y_{n+\frac12} &= y_n + \frac h2 f(x_n, y_n),\\
y_{n+1} &= y_n + h f(x_{n+\frac12} y_{n+\frac12}).}}

Другая модификация заключается в использовании формулы трапеций,
которая выбирается в качестве основной:
\eqn{y_{n+1} = y_n + \frac h2 \br{f(x_n,y_n) + f(x_{n+1},y_{n+1})}.}
Тут та же трудность: мы не знаем ничего про $y_{n+1}$, поэтому его приходится
приближать опять же с помощью формулы Эйлера
\eqn{y_{n+1} = y_n + h f(x_n,y_n)}
и потом подставлять в предыдущую формулу. Фактически, здесь мы в каком\д то смысле
делаем один шаг метода простой итерации: сначала вычисляем начальное приближение, а потом уточняем его.

Естественно, за повышение точности нам приходится платить: мы дважды
вычисляем значение $f$.


\subsubsection{Метод Рунге\ч Кутта}

Этот метод является естественным обобщением для модификаций метода Эйлера.
Именно, рассмотрим систему поправок
\eqn{\begin{aligned}
k_1(h) &:= h f(x,y),\\
k_2(h) &:= h f(x + \al_2 h, y+ \be_{21} k_1),\\
&\dots\\
k_q(h) &:= h f\Br{x + \al_q h, y + \suml{j=1}{q-1} \be_{qj} k_j}.
\end{aligned}}
После этого вычисление ведётся так:
\eqn{y(x +h) \approx z(h) := y(x) + \suml{j=1}{q} p_j k_j(h).}
Здесь имеется куча параметров, а именно $q$ штук параметров $\al_j$ и ещё столько же $p_j$,
а кроме того, ещё целая строго нижнетреугольная матрица параметров $\be_{kj}$ (их будет $\frac{q(q-1)}{2}$ штук).

\begin{df}
Такой метод называется явным методом Рунге\ч Кутта.
\end{df}


Положим $\ph(h) := y(x+h) - z(h)$.

\begin{df}
Будем говорить, что метод имеет порядок $s$, если
\eqn{\ph(0) = \ph'(0) = \dots = \ph^{(s)}(0) = 0, \quad \ph^{(s+1)}(0) \ne 0.}
\end{df}

Если метод имеет порядок $s$, то, по формуле Тейлора получаем такое выражение для $\ph$:
\eqn{\ph(h) = \frac{\ph^{(s+1)}(0)}{(s+1)!}h^{s+1} + \frac{\ph^{(s+2)}(\ta h)}{(s+2)!}h^{s+2}.}
Отсюда мораль: нужно стараться выбирать параметры так, чтобы $s$ было максимальным.

\begin{ex}
Пусть $q = 1$. Тогда $k_1(h) = h f(x,y)$, $z(h) = y + p_1 hf(x,y)$, отсюда
\eqn{\ph(h) = y(x+ h) - y - p_1 h f(x,y).}
Очевидно, что $\ph(0) = 0$. Вычислим $p_1$ из условия $\ph'(0) = 0$:
\eqn{0 = \ph'(0) = \ub{y'(x)}_{=f(x,y)} - p_1 f(x,y) = 0,}
откуда $p_1 = 1$.
Таким образом, при $q = 1$ получаем обычный метод Эйлера.
\end{ex}


\begin{ex}
Пусть $q = 2$. Как и ранее, $k_1(h) = hf(x,y)$.
Обозначим для краткости
\eqn{\ol x := x + \al_2 h, \quad \ol y := y + \be_{21}k_1,}
тогда $k_2(h) =h f(\ol x, \ol y)$. Функция $\ph$ имеет вид
\eqn{\ph(h) = y(x+h) - y - p_1 h f(x,y) - p_2 h f(\ol x, \ol y).}
Имеем $\ph(0) = 0$. Далее, из условия $\ph'(0) = 0$ получаем
\eqn{0 = \ph'(0) = y' - p_1 f - p_2 f,}
откуда $p_1 + p_2 = 1$.
Продифференцируем второй раз:
\mln{0 = \ph''(0) = y''(x) - 2p_2\br{f_x(x,y)\al_2 + f_y(x,y) \be_{21} f} =\\=
f_x(x,y) + f_y(x,y)f - 2p_2\br{f_x(x,y)\al_2 + f_y(x,y) \be_{21} f}=
f_x\hr{1-2p_2\al_2} + f_yf\hr{1 -p_2\be_{21}}. }
Чтобы это было так, нужно, чтобы коэффициенты при $f_x$ и $f_yf$ были нулевые.
Значит, получается ещё два уравнения:
\eqn{2 p_2 \al_2 = 1, \quad 2 p_2 \be_{21} = 1.}

Однако заметим, что одного уравнения, вообще говоря, не хватает: у нас 3 уравнения и 4 неизвестных.
Так что далее можно рассматривать различные дополнительные условия и получать разные формулы.

Например, возьмём $p_1 = 0$. Тогда $p_2 = 1$,  $\al_2 = \frac12$ и $\be_{21} = \frac12$.
В этом случае получаются в точности формулы для метода прямоугольников.

А если взять $p_1 = p_2 = \frac12$, то получим $\al_2 = 1$ и $\be_{21} = 1$, поэтому возникнут
в точности формулы, полученные из квадратурной формулы трапеций.

При этом возникает естественное желание: раз у нас есть ещё одна степень свободы,
то, быть может, можно выбрать константы так, чтобы метод имел порядок $3$, а не $2$.
Однако нас ждёт неудача: простейшее уравнение $y' = y$ даёт, как несложно показать,
$\ph'''(0) \ne 0$.

Теперь приведём ещё немного результатов (без всяких обоснований).
Если $q = 3$, то можно добиться порядка аппроксимации $s = 3$.  Более того,
если $q = 4$ или $q = 5$, то, как ни странно, можно получить всего лишь $s = 4$.

Для реальных вычислений были построены методы 8\д го порядка\footnote{Остаётся
только догадываться, сколько там коэффициентов :)} (M\'erson).
Кроме того, для специальных систем построены методы 14\д го порядка. Очень недавно какой\д то
хмырь (история не сохранила его фамилии\ldots) построил схему, которая позволяет для полиномиальных уравнений получать методы
сколь угодно высокого порядка.
\end{ex}

\subsubsection{Метод Рунге априорной оценки погрешности}

Возникает вопрос о том, а как оценить погрешность, если мы совсем ничего не знаем
про точное решение?

Мы уже отмечали, что если метод имеет порядок $s$, то $\ph(h) = M h^{s+1} + o(h^{s+1})$.
Здесь $M$\т главный член погрешности. Это число, конечно, зависит от точки $(x,y)$,
но будем считать, что оно не сильно меняется.

Пусть мы уже знаем $y(x)$. Вычислим по методу Рунге\ч Кутта значение $y(x+h)$.
Тогда мы при этом огребём погрешность порядка $M h^{s+1}$. Сделав ещё один шаг длины $h$,
мы вычислим $y(x+2h)$, при этом получим число $y_h$, а суммарная погрешность составит $2M h^{s+1}$.

С другой стороны, можно сразу вычислить $y(x+2h)$, используя для этого удвоенный шаг.
При этом получим некоторое число $y_{2h}$ с погрешностью $M(2h)^{s+1}$. Итого получаем такую систему:
\eqn{
\case{y_h = y(x+2h) + 2Mh^{s+1},\\
y_{2h} = y(x+2h) + M (2h)^{s+1}.}}
В этой системе нам неизвестны $M$ и $y(x +2h)$. Решив её, мы получим хоть какую\д то оценку на величину константы $M$.

\subsubsection{Обобщение метода Рунге\ч Кутта}

Как мы видели из примеров, сколько ни увеличивай число $q$, ощутимых результатов (в смысле повышения
порядка метода) мы не добьёмся. Значит, нужно что\д то ещё.

Модификация метода Рунге\ч Кутта приводит к так называемому неявному методу
Рунге\ч Кутта. Суть его в том, что мы дополняем матрицу $\be_{kj}$ главной диагональю
и несколькими (сколькими именно\т вопрос отдельный, и мы его обсуждать не будем)
строками над ней.

\subsection{Разностные схемы для решения дифференциальных уравнений}

Рассмотрим уравнение $y' = f(x,y(x))$ и проинтегрируем его по отрезку $I := [-nh,0]$:
\eqn{\ints{I} y'(x+t)\dt = \ints{I} f(x+t, y(x+t))\dt.}
Заменим интеграл квадратурной формулой, получим
\eqn{\suml{i=0}{q} \frac{a_{-i} y_{n-i}}{h} = \suml{i =0}{q} b_{-i} f(x_{n-i}, y_{n-i}).}
Получили разностную схему нашего уравнения.

Погрешность такой схемы\т это величина
\eqn{r(x) := \suml{i=0}{q} \frac{a_{-i} y(x-i h)}{h} - \suml{i =0}{q} b_{-i} f\br{x-i h, y(x-i h)}.}

\begin{df}

\pt1. Если $a_0 \ne 0$, а $b_0 = 0$, то схема называется явной.

\pt2. Если $a_0 \ne 0$ и $b_0 \ne 0$, то получается, вообще говоря, нелинейное уравнение на $y_n$
вида
\eqn{\frac{a_0}{h} y_n - b_0f(x_n, y_n) = \dots,}
поэтому этот случай безнадёжен.

\pt3. Если всё\д таки $a_0 = 0$, а $b_0 \ne 0$, то это неявная схема, или, как ещё говорят,
<<схема с забеганием вперёд>>.
\end{df}

Теперь нужно найти условия на коэффициенты $a_{-i}$ и $b_{-i}$, чтобы погрешность была как можно более
высокого порядка по $h$.

\begin{df}
Будем говорить, что схема имеет порядок $s$, если
\eqn{r(x) = E_0 y h^{-1} + E_1 y' + E_2 y'' h \spl E_s y^{(s)} h^{s-1} + E_{s+1} h^s +\dots}
причём $E_0 = E_1 = \dots = E_s = 0$, а $E_{s+1} \ne 0$.
\end{df}

Разложим решение в ряд Тейлора и подставим его в формулу для погрешности:
\eqn{y(x - i h) = y(x) - (i h)y' + \frac{(i h)^2}{2} y'' + \dots. }
Тогда сумма первых двух членов разложения первого слагаемого погрешности будут иметь вид
\eqn{\frac{1}{h} \suml{i=0}{q} a_{-i} y - \suml{i =0}{q} a_{-i} i y'.}
Отсюда следует, что
\eqn{\suml{i =0}{q} a_{-i} = 0, \quad \suml{i=0}{q} ia_{-i} = -1,}
потому что это слагаемое должно (при $h \ra 0$) сходиться к $y'$.
Аналогично поступая со вторым слагаемым погрешности, получаем, что
\eqn{\suml{i=0}{q} b_{-i} =1,}
потому что это слагаемое в пределе должно давать $f(x,y(x))$.

Если мы хотим получать схемы аппроксимации более высокого порядка,
то нужно, чтобы занулялись коэффициенты при $y''$ и так далее.
Для второго порядка, например, появится уравнение
\eqn{\sum a_{-i} \frac{i^2}{2} + \sum b_{-i} i = 0,}
а для третьего порядка\т ещё и уравнение
\eqn{-\sum a_{-i} \frac{i^3}{6} - \sum b_{-i} \frac{i^2}{2} = 0.}

\subsubsection{Устойчивость схем в определениях и примерах}

Рассмотрим уравнение $y' = 0$ и соответствующую схему
\eqn{\suml{i=0}{k} a_{-i} y_{n-i} = 0.}
Это некоторое рекуррентное соотношение, и, как мы знаем, его решение
нужно искать в виде $y_n = \mu^n$. Подставляем в схему, получаем уравнение на $\mu$:
\eqn{\suml{i=0}{k} a_{-i} \mu^{k-i} = 0.}

Найдём решения этого уравнения, получим $k$ корней (с учётом кратности).
Если найдётся корень $\mu_j$, для которого $\hm{\mu_j} > 1$, то существует экспоненциально растущее решение
(при этом неважно, вещественный этот корень или комплексный).

Если $\hm{\mu_i} \le 1$ для всех $i$, но имеются кратные корни, то будет полиномиальный рост
порядка на единицу меньше кратности корня.

Если $\hm{\mu_i} \le 1$ для всех $i$, и на окружности $\hc{\hm{\mu} = 1}$ нет кратных корней, то уже всё хорошо: решения
будут ограниченными (кратные корни строго внутри круга не страшны, потому что их задавит экспонента с отрицательным
показателем).

Сейчас мы рассмотрим несколько примеров схем для решения одного и того же
уравнения и посмотрим, как они себя ведут.
Будем решать уравнение $y' + My = 0$ с начальным условием $y(0) = y_0$ и $M > 0$.

\begin{ex}
Рассмотрим самую простую схему (метод Эйлера). Она имеет вид
\eqn{y_{n+1} = y_n - M y_n h = (1 - M h) y_n.}
Отсюда получаем
\eqn{y_n = y_0 (1-M h)^n,}
значит, по крайней мере нужно, чтобы $0\le 1 - Mh \le 1$, тогда решение, вычисленное по этой схеме,
будет похоже на убывающую экспоненту. Это задаёт условие $h \le \frac{1}{M}$.
\end{ex}

Возникает вопрос: а что, если взять схему с более высоким порядком аппроксимации.
Вот пример, который показывает, что <<больше>>\т не всегда значит <<лучше>>.

\begin{ex}
Рассмотрим такую схему:
\eqn{\frac{y_{n+1} - y_{n-1}}{2h} + M y_n = 0.}
Преобразуя уравнение, получаем
\eqn{y_{n+1} + 2M h y_n - y_{n-1} = 0.}
Соответствующее характеристическое уравнение имеет вид
\eqn{\mu^2 + 2 M h \mu - 1 = 0,}
откуда
\eqn{\mu_{1,2} = -M h \pm \sqrt{M^2h^2 + 1}.}
У этого уравнения всегда есть <<плохой>> корень
\eqn{\mu_1 = -M h - \sqrt{M^2h^2 + 1}, \quad \hm{\mu_1} > 1,}
поэтому наша схема никогда не будет устойчивой.
\end{ex}

Чтобы читателю не показалось, что всё совсем плохо, мы приведём пример хорошей схемы
с аппроксимацией производной второго порядка.

\begin{ex}
Рассмотрим схему
\eqn{\frac{y_{n+1} - y_n}{h} + \frac{M}{2}(y_{n+1} + y_n) = 0.}
Это тоже будет явная схема, но здесь мы аппроксимировали не только производную,
но и саму функцию. Тогда
\eqn{y_{n+1} = \frac{1 - M\frac{h}{2}}{1 + M \frac{h}{2}} y_n,}
значит, решение имеет вид
\eqn{y_n = y_0 \hr{\frac{1 - M\frac{h}{2}}{1 + M \frac{h}{2}}}^n.}
А поскольку $M > 0$, числитель этой дроби будет всегда меньше знаменателя.
Однако и здесь имеется некоторое ограничение на $h$ сверху, поскольку числитель дроби
не должен становиться отрицательным. Отсюда получаем ограничивающее неравенство
$1 - M \frac{h}{2} > 0$, то есть $h < \frac{2}{M}$.

Заметим, что в этом случае мы получили более качественную схему (кстати, и ограничение на $h$ получилось
в два раза менее существенное).
\end{ex}

\subsubsection{Метод Лебедева для решения жёстких систем ОДУ}

Вначале рассмотрим общую ситуацию, когда задача многомерная.
Пусть мы решаем систему $y' + Ay = 0$. Пусть, для простоты,
система распадается, то есть матрица $A$ диагональна:
$A = \diag (\la_1\sco \la_m)$. Пусть $\la_i \ge 0$ при всех $i$.

\begin{note}
Мы специально рассматриваем именно такой простой пример, чтобы объяснить, что такое жёсткая система
и почему при решении возникают трудности.
\end{note}

Решениями этой системы будут, понятное дело, функции такого вида:
\eqn{y^i(x) = y^i(0) e^{-\la_i x}.}
Пусть жизнь сложилась так, что первые $p$ собственных чисел оказались порядка $1$,
а остальные\т по порядку значительно больше $1$. Тогда, как мы уже выясняли,
чтобы схему не разносило при вычислениях, нужно, чтобы шаг $h$ удовлетворял неравенствам
\eqn{h \la_i \le 1.}

Это значит, что при наличии огромных $\la_i$ придётся двигаться с очень маленьким шагом, и в итоге мы уйдём лишь на $\frac nM$.
Конечно, в нашей задаче можно каждое уравнение решать отдельно, но в реальной жизни
всё не так хорошо, поскольку матрица не диагональна.

Про системы говорят, что они слабо жёсткие, если $\la_m \sim 10^6$; средне жёсткие, если $\la_m \sim 10^{12}$;
сильно жёсткие, если $\la_m \sim 10^{18}$.

\begin{petit}
Скорее всего, эта классификация была приспособлена под вполне конкретные вычислительные системы (быть может,
довольно доисторические). Понятно, что для современных вычислительных систем сами константы могут быть другими.
\end{petit}

\begin{df}
Система называется жёсткой, если $\hm{\Img \la_i} < C$ и $\la_1\sco\la_p \sim 1$, а $\Re \la_{p+1}\sco \Re \la_n \gg 1$.
\end{df}

Основная идея метода в переменном шаге $h_j$.
Итерации имеют вид
\eqn{y_{n+1} = y_n(1-Mh_{n+1}),\quad y_n = y_0(1-Mh_1)\sd(1-Mh_n) = y_0(1-M \sum h_j + \dots) = y_0 P(M).}
Мы хотим продвинуться за $n$ шагов как можно дальше. Значит, $\sum h_j$ должна быть минимальной.
Утверждается, что в качестве многочлена $P(M)$ можно взять многочлен Чебышёва. Сумма $\sum h_j$ есть свободный член его
производной в нуле (со знаком <<минус>>), поэтому мы хотим от него таких свойств:
\eqn{P'(0)\ra\min, \quad |P(M)|\le1, \quad P(0)=1.}
Из свойств многочленов Чебышёва это легко следует: пусть многочлен $Q$ круче, тогда рассмотрим разность $R = T-Q$.
Она сначала должна идти вниз, а потому имеет $n+1$ корень. Противоречие.

Таким образом, мы можем вычислить максимальное продвижение:
\eqn{\sum h_j = \frac2M \cdot T'(1) = \frac2M \cdot \cos(n\arccos x)'\evn{x=1} = \frac{2n^2}{M}.}
Это уже гораздо лучше, чем скромное $\frac nM$.

\begin{petit}
Дальше текст набирался очень быстро или выдирался из других источников и документов.
Поэтому полного соответствия лекциям нет.
\end{petit}

\subsection{Простейшая краевая задача}

\subsubsection{Разные определения и теоремы}

Введём обозначение $\de^2 (z) := z_{n+1}-2z_n + z_{n-1}$ (оно будет использовано и дальше).

\begin{petit}
Некоторые из даваемых ниже определений и понятий уже встречались. Но не будет лишним их повторить.
К каким билетам относить те или иные факты, которые здесь изложены\т я до сих пор не понимаю. Но знать их всё равно надо.
\end{petit}

Рассмотрим краевую задачу
\eqb{Lu = f\\u(0)=a,\\u(1)=b.}

Через $[\cdot]$ будем обозначать ограничение аргумента на сетку с шагом $h$.

\begin{df}
$L^h$ аппроксимирует $L$ в точке $x$, если $L^h[u]- [Lu]\ra 0$ при $h\ra 0$.
Погрешностью аппроксимации называется разность $r(x) = L^h[u]-[Lu]$.
\end{df}

\begin{df}
Норма на сетке согласована с нормой на пространстве, если $\hn{[u]_h} \ra \hn{u}$ при $h\ra 0$.
\end{df}

\begin{df}
Схема устойчива, если $\hn{[u]_h} \le C \hn{[f]_h}$ для некоторой универсальной константы $C$.
\end{df}

Будем рассматривать оператор $L = -(\De - p(x))$, где  $p(x) \ge 0$.
Далее оператор Лапласа мы всегда будем приближать разностью

\eqn{\De_h v = \frac{v_{m+1}-2v_m+ v_{n-1}}{h^2}.}

\begin{theorem}[принцип максимума]
Пусть на границе функция $v$ неотрицательна. функция Если ${L v\ge 0}$, то $v\ge 0$.
\end{theorem}
\begin{proof}
Допустим, что $v < 0$. Выберем точку минимума $v$, тогда очевидно, $\min v < 0$.
По условию
\eqn{-\frac{v_{m+1}-2v_m+ v_{n-1}}{h^2}+ p_m v_m \ge 0.}
Перепишем это условие в виде
\eqn{(\ub{v_m -v_{m+1}}_{\le 0}) + (\ub{v_n - v_{m-1}}_{\le 0}) + \ub{p_m v_m}_{< 0} \ge 0.}
Получили противоречие. Значит, $v \ge 0$.
\end{proof}

\begin{imp}[Теорема единственности]
Если два решения дифференциальной задачи с оператором $L$ совпадают на границе, то они совпадают всюду.
\end{imp}


\begin{stm}
Имеет место устойчивость для разностной схемы с оператором $L$.
\end{stm}
\begin{proof}
Покажем, что решение $u(x)$ всегда будет ограничено.
Рассмотрим норму $\hn{f} := \max |f|$ (какую норму брать, это не важно\т в конечномерном\д то пространстве!).
Рассмотрим функцию
\eqn{v(x) := \frac{x-x^2}{2} \hn{f} + |a|\cdot (1-x) + |b|\cdot (x).}
Имеем
\eqn{L v(x) = \hn{f} + p(x) v(x) + |a| \cdot (1-x) + b\cdot (x) \ge \hn{f}.}
Теперь рассмотрим функцию $w(x):= v \pm u$.
Имеем $w(0) > 0$, $w(1) > 0$. Кроме того, $L w = L v \pm L u = \hn{f} \pm f \ge 0$.
Тогда по принципу максимума $w = v \pm u \ge 0$. Значит, $\hm{u}\le v$.
\end{proof}

\subsubsection{Три разностные схемы, спектральный признак}

Будем решать \emph{уравнение переноса}
\eqn{\label{eqn:translate}\pf ut = \pf u x.}

Пусть $L$\т оператор переноса.

$L$\т линейный дифференциальный оператор на
пространстве гладких функций на $\Om$, непрерывных в замыкании
области $\Om$, $l$\т линейный оператор на пространстве функций,
заданных на подмножестве границы области $\Om$, те $\Om_1\subset\pd\Om.$

\begin{df}
Сеткой будем называть множество \eqn{\Om_h^{\tau}=\Om\cap
\Rc_h^{\tau},} где \eqn{\Rc_h^{\tau}=\{(x_m,t_n): x_m=mh,t_n=n\tau,
n,m\in\Z \},} а величины $h$ и $\tau$ задают шаг сетки
соответственно по $x$ и по $t$.
\end{df}
Нетрудно понять, как функциям $f$ и $g$ поставить в соответствие так
называемые сеточные функции $[f]_h^{\tau}$ и $[g]_h^{\tau}$\т
фактически, это просто их ограничения на сетку $\Om_h^{\tau}$. То же
самое касается линейного оператора $l$\т только его область
определения мы ограничиваем на сеточные функции. А линейный оператор
$L_h^{\tau}$ мы построим, используя разделенные разности на сетке
$\Om_h^{\tau}$. Подробнее эта процедура будет описана ниже. В
результате мы получим разностную задачу \eqn{\label{eqn:translate:diff:scheme}\left\{
  \begin{array}{ll}
    L_h^{\tau}u_h^{\tau}=[f]_h^{\tau}, & \hbox{$(x_m,t_n)\in\Om_h^{\tau}$;} \\
    l_h^{\tau}u_h^{\tau}=[g]_h^{\tau}, & \hbox{$(x_m,t_n)\in\Om_{1\ h}^{\ \ \tau}$,}
  \end{array}
\right.} которая является системой линейных уравнений относительно
неизвестных $u_{m,h}^{n,\tau}$, которые задают (являются их набором
значений) сеточные функции $u_h^{\tau}$, называемые решениями
соответствующей разностной схемы.
\begin{df}
Будем говорить, что решение разностной схемы~\eqref{eqn:translate:diff:scheme} сходится к решению
дифференциальной задачи~\eqref{eqn:translate}, если
\eqn{\lim\limits_{h,\tau\ra 0}\bn{u_h^{\tau}-[u]_h^{\tau}}_{\Om_h^{\tau}}=0.}
\end{df}
\begin{df}
Разностная схема~\eqref{eqn:translate:diff:scheme} аппроксимирует дифференциальную задачу~\eqref{eqn:translate} на
функции $u$ с порядком $m$ по пространству и порядком $n$ по
времени, если существуют положительные константы $c_1$, $c_2$, $c_3$, $c_4$, $h_1$, $\tau_1$
такие, что для всех $h$ и $\tau$, таких что $0<h<h_1$ и $0<\tau<\tau_1$ имеет место
\eqn{\left\{
\begin{array}{l}
\bn{L_h^{\tau}[u]_h^{\tau}-[Lu]_h^{\tau}}_{\Om_h^{\tau}}\le c_1h^{m_1}+c_2\tau^{n_1}, \\
\bn{l_h^{\tau}[u]_h^{\tau}-[lu]_h^{\tau}}_{\Om_h^{\tau}}\le
c_3h^{m_2}+c_4\tau^{n_2},
\end{array}
\right.
}
где $n=\min(n_1, n_2), m=\min(m_1, m_2).$
\end{df}

\begin{df}
Разностная схема называется \emph{безусловно устойчивой}, если
существуют положительные константы $c_5, c_6, h_2, \tau_2$ такие,
что для любых правых частей в~\eqref{eqn:translate:diff:scheme} при всех $h$ и $\tau$, таких что
$0<h<h_2, 0<\tau<\tau_2$ выполнены условия:

\begin{nums}{-3}
\item Существует и единственно решение $u_h^{\tau}$ задачи
(7);
\item Имеет место неравенство \eqn{\label{eqn:translate:stable}\bn{u_h^{\tau}}_{\Om_h^{\tau}}\le c_5\bn{f_h^{\tau}}_{\Om_h^{\tau}}+c_6\bn{g_h^{\tau}}_{\Om_{1\ h}^{\ \
\tau}}.}
\end{nums}
\end{df}

Разностная схема называется \emph{условно устойчивой}, если
существуют последовательности $h_k\ra0,\ \tau_k\ra0$, для которых
выполнено неравенство~\eqref{eqn:translate:stable}. Разностная схема называется
\emph{(безусловно) неустойчивой}, если таких последовательностей не
существует.

\subsubsection{Спектральный признак устойчивости}

Для простоты все дальнейшие рассуждения относятся лишь к уравнению переноса.

Положим
\eqn{\label{eqn:spectral}u_n^m=c\la^ne^{im\ph}}
и будем подставлять эти значения
в нашу разностную схему (что именно это означает\т см. далее).
Получим зависимость \equ{\la=\la(\ph)=\la(\ph, h, \tau).} Согласно
спектральному признаку устойчивости, наша схема будет устойчивой
тогда и только тогда, когда \eqn{\label{eqn:spectral:stable}|\la|\le1,} или, точнее,
\eqn{|\la|\le\kappa\tau,} где $\kappa$\т некоторая неотрицательная
константа. Но такие тонкости нам едва ли понадобятся. Возможны
следующие случаи:
\begin{itemize}
  \item неравенство~\eqref{eqn:spectral:stable} выполнено при всех $\tau$ и $h$\т тогда
  схема безусловно устойчива;
  \item неравенство выполнено только в случае, если $\tau$ и $h$
  удовлетворяют некоторым условиям\т такая схема является условно
  устойчивой;
  \item неравенство вообще никогда не выполняется\т схема
  безусловно неустойчива.
\end{itemize}

\begin{ex}
Вот наиболее простой пример разностной схемы для уравнения переноса:
\eqn{\frac{u_{m}^{n+1}-u_m^n}{\tau}=a\frac{u_{m+1}^n-u_m^n}{h}.}
Будем исследовать эту схему на устойчивость. Подставляя \eqref{eqn:spectral},
получим: \eqn{\frac{\la^{n+1}e^{im\ph}-\la^n
e^{im\ph}}{\tau}=a\frac{\la^n e^{i(m+1)\ph}-\la^n e^{im\ph}}{h}.}
Сократим на $\la^n e^{im\ph}$:
\eqn{\frac{\la-1}{\tau}=a\frac{e^{i\ph}-1}{h},} откуда выразим
$\la$:
\eqn{\la=1+\frac{a\tau}{h}(e^{i\ph}-1)=1+\frac{a\tau}{h}(\cos\ph+i\sin\ph-1).}
Преобразуем это выражение, обозначая для краткости $r=a\tau/h$ и
используя формулу $1-\cos\ph=2\sin^2(\ph/2)$:
\eqn{\la=(1-2r\sin^2\frac{\ph}{2})+ir\sin\ph,} откуда
\eqn{|\la|^2=(1-2r\sin^2\frac{\ph}{2})^2+r^2\sin^2\ph=1-4r\sin^2\frac{\ph}{2}+4r^2\sin^4\frac{\ph}{2}+4r^2\sin^2\frac{\ph}{2}\cos^2\frac{\ph}{2}.}
Теперь уже несложно понять, что нужное нам неравенство $|\la|\le1$
эквивалентно условию \eqn{0\le r\le1\Lra0\le\frac{a\tau}{h}\le1.}
Итак, рассмотренная схема является условно устойчивой при $a\ge0$\т
причем нами было найдено условие на ее устойчивость, а при $a<0$ она
является неустойчивой.
\end{ex}

\begin{note}
Очевидно, что если рассмотреть схему
\eqn{\frac{u_{m}^{n+1}-u_m^n}{\tau}=a\frac{u_{m}^n-u_{m+1}^n}{h},}
она будет условно устойчива при $a\le0$ с тем же условием на
устойчивость и неустойчива при $a>0$.
\end{note}

А вот пример безусловно устойчивой схемы:
\eqn{\frac{u^{n+1}_m-u^n_m}{\tau}=a\frac{u^{n+1}_{m+1}-u^{n+1}_{m-1}}{4h}+a\frac{u^{n}_{m+1}-u^{n}_{m-1}}{4h}.}
Вычислить $\la$ и убедиться, что $|\la|=1$, предоставляется читателю.

\subsection{Схемы с весами}

\subsubsection{Явная схема}

Будем рассматривать следующую краевую задачу для уравнения
теплопроводности с постоянными коэффициентами. В области $\{0<x<1,0<t\le T\}$
требуется найти решение уравнения
\eqn{\pf{u}{t}=\pf{^2u}{x^2}}
удовлетворяющее начальному условию
\eqn{u(x,0)=u_0(x)}
и граничным условиям
\eqn{u(0,t)=0,\quad u(1,t)=0.}
Примером явной разностной схемы для данной задачи может служить следующая схема:
\eqn{\frac{u_m^{n+1}-u_m^n}{\tau}=\frac{u^n_{m+1}-2u_m^n+u^n_{m-1}}{h^2}.}
При этом краевые уловия дают дополнительные уравнения:
\equ{u_0^n=u_M^n=0,\quad u_m^0=u_0(mh).}
Спектральный признак устойчивости для указанной схемы даёт следующее условие:
\eqn{\hm{1-\frac{4\tau}{h^2}\sin^2\frac{\pi kh}2}\le1\Ra\frac{\tau}{h^2}\le\frac12.}
Итак, построенная схема даёт первый порядок аппроксимации по времени и второй по пространству (это будет доказано позже),
однако, для использования этой схемы шаг по времени следует брать достаточно малым ($\tau\le\frac12h^2$).

\subsubsection{Неявная схема}
Использование неявной схемы позволяет снять условие на зависимость шагов по времени и по пространству.
Для этого нужно взять схему
\eqn{\frac{u_m^{n+1}-u_m^n}{\tau}=\frac{u^{n+1}_{m+1}-2u_m^{n+1}+u^{n+1}_{m-1}}{h^2}.}
Эта схема, имеет тот же порядок аппроксимации, что и предыдущая (явная) схема (это будет доказано позже), однако
условие устойчивости для неё (согласно спектральному признаку) имеет вид
\eqn{\hm{1+\frac{4\tau}{h^2}\sin^2\frac{\pi kh}2}^{-1}\le1,}
откуда немедленно следует, что эта схема устойчива при любых $\tau$ и $h$.

\subsubsection{Схема с весами}

Схема с весами является обобщением уже рассмотренных схем. А именно, вводя обозначение
\eqn{\La u^n_m=\frac{u^n_{m+1}-2u_m^n+u^n_{m-1}}{h^2},}
введём схему вида
\eqn{\label{eqn:WeightScheme}\frac{u_m^{n+1}-u_m^n}{\tau}=\si\La u^{n+1}_m+(1-\si)\La u^n_m,}
где $\si$\т фиксированная константа из отрезка $[0,1]$. Таким образом, при каждом значении
константы $\si$ получаем некоторую разностную схему; в частности, при $\si=1$ имеем чисто неявную схему,
а при $\si=0$\т явную. Отдельного упоминания заслуживает случай $\si=\frac12$. Такая схема (она носит название <<схема Кранка\ч Николсон>>) является
абсолютно устойчивой и имеет второй порядок аппроксимации по времени и по пространству (это будет доказано позже). При вычислениях по этой схеме
требуется решить систему с трёхдиагональной матрицей, в частности, это можно можно сделать методом прогонки.

Исследуем погрешность аппроксимации схемы \eqref{eqn:WeightScheme} для произвольной константы $\si$ из отрезка $[0,1]$.
Представим решение задачи \eqref{eqn:WeightScheme} в виде
\equ{y_i^n=u(x_i,t_n)+z_i^n,} где $u(x_i,t_n)$\т точное решение исследуемой дифференциальной задачи.
Тогда для погрешности получим систему уравнений:
\eqn{\frac{z_i^{n+1}-z_i^n}{\tau}=\si\La z_i^{n+1}+(2-\si)\La z_i^n+\psi_i^n,}
где сеточная функция $\psi$, входящая в правую часть уравнения, равна
\eqn{\psi_i^n=\si\La u_i^{n+1}+(1-\si)\La u_i^n-\frac{u_i^{n+1}-u_i^n}{\tau}}
и называется погрешностью аппроксимационной схемы \eqref{eqn:WeightScheme}.
Разлагая выражение для $\psi_i^n$ по формуле Тейлора в точке $(x_i,t_n+\frac12\tau),$ получим
\eqn{\psi_i^n=(u''-\dot{u})+\hr{\si-\frac12}\tau u''+\frac{h^2}{12}u^{IV}+O(\tau^2+h^4).}
Учитывая исходное уравнение, окончательно можем записать
\eqn{\psi_i^n=\hs{\hr{\si-\frac12}\tau+\frac{h^2}{12}}\dot{u}''+O(\tau^2+h^4).}
Отсюда, в частности, следуют указанные порядки аппроксимации для схем при $\si=0,1,\frac{1}{2}.$
Кроме того, следует отметить, что при $\si=\frac12-\frac{h^2}{12\tau}$ схема имеет повышенный порядок аппроксимации\т второй по
времени и четвёртый по пространству.

\subsection{Сеточные теоремы вложения}

Сейчас мы докажем простой аналог одной теоремы из курса УрЧП, относящейся к теории пространства Соболева.

Рассмотрим две нормы на пространстве сеточных функций на отрезке $[0,X]$.
\eqn{\hn{u_n}^2_{1,h} := h \sum \hr{\frac{u_{n+1}-u_n}{2}}^2.}
\eqn{\hn{u_n}^2_{0,h} := h \sum |u_n|^2.}

\begin{theorem}[вложения] Пусть $u_0 = u_N = 0$. Имеют место следующие неравенства:
\eqn{\hn{u_n}^2_{0,h} \le X \hn{u_n}^2_C,}
\eqn{\hn{u_n}^2_C \le \frac{X}2 \hn{u_n}^2_{1,h}.}
\end{theorem}
\begin{proof}
Первое неравенство очевидно (оценка интеграла максимумом модуля функции, помноженной на длину отрезка),
а второе тривиально. Пусть $k$\т точка, в которой достигается максимум модуля функции $u_n$
(можно считать, что $k \le \frac N2$).
Тогда
\eqn{u_k = \suml{n=0}{k-1} (u_{n+1} - u_n) = \suml{n = 0}{k-1} \sqrt h \cdot \frac{u_{n+1}-u_n}{\sqrt h}.}
Применим к произведению в правой части неравенство Коши, Буняковского, Шварца и ещё многих товарищей:
\eqn{|u_k|^2 \le \ub{k \cdot (\sqrt h)^2}_{X/2} \cdot h \suml{n=0}{k-1} \hr{\frac{u_{n+1}-u_n}{\sqrt h}}^2 = \frac X2 \hn{u_n}^2_{1,h}.}
Ну вроде мы именно этого и добивались.
\end{proof}

\subsection{Методы стрельбы и прогонки}

\subsubsection{Метод прогонки}

Для решения трёхдиагональных систем, часто возникающих при построении разностных схем, можно использовать \emph{метод прогонки}.

Пусть нам дана трёхдиагональная система $Ax=b$, где $\al_1\sco \al_n$\т числа на диагонали,
$\be_1\sco\be_{n-1}$\т числа над диагональю, а $\ga_2\sco\ga_{n}$\т числа под диагональю. Обратите внимание
на индексы\т они именно такие, поскольку мы хотим, чтобы в каждой строке они были одинаковые.

Попробуем рекуррентно выразить $x_i$ друг через друга. А именно, будем искать решение в виде
\eqn{x_{i-1} = A_i x_i + B_i.}
Числа $A_i$ и $B_i$ называются \emph{прогоночными коэффициентами}.
Попробуем найти формулы для $A_i$ и $B_i$.

Запишем систему, используя эти соотношения:
\eqb{
\al_1 x_1 + \be_1 x_2 & = b_1,\\
\dots\\
\ga_i x_{i-1} + \al_i x_i & + \be_i x_{i+1} = b_i,\\
\dots\\
\ga_n x_{n-1}& + \al_n x_n = b_n.\\
}

Перепишем общее уравнение через прогоночные коэффициенты:
\eqn{\ga_i (A_i x_i + B_i) + \al_i x_i  + \be_i x_{i+1} = b_i.}
Ура! Нам повезло, и мы избавились от третьей переменной. Выразим $x_i$:
\eqn{x_i = \ub{-\frac{\be_i}{\ga_i A_i + \al_i}}_{A_{i+1}} x_{i+1} + \ub{\frac{b_i - \ga_i B_i}{\ga_i A_i + \al_i}}_{B_{i+1}}.}
А теперь мы видим, что нам ещё больше повезло, поскольку дроби содержат только переменные с индексами $i$.
Обозначая их соответственно $A_{i+1}$ и $B_{i+1}$, получаем как раз формулу нужного вида:
\eqn{x_i = A_{i+1} x_{i+1} + B_{i+1}.}

Итак, теперь уже ясно, как надо решать систему. Вычислим сначала первые два коэффициента:
$A_2 = -\frac{\be_1}{\al_1}$ и $B_2 = \frac{b_1}{\al_1}$.
Далее вычисляем все остальные коэффициенты по формулам
\eqn{A_{i+1} = -\frac{\be_i}{\ga_i A_i + \al_i}, \quad B_{i+1} = \frac{b_i - \ga_i B_i}{\ga_i A_i + \al_i},}
не забывая складывать нажитое непосильным трудом в массивы. Кстати говоря, можно смело использовать память, в которой
у нас лежат коэффициенты, поскольку она нам больше не пригодится.
Когда все $A_i$ и $B_i$ вычислены, можно приступать к нахождению $x_i$.

Из последних  двух уравнений выразим $x_n$. Имеем
\eqb{x_{n-1}  = A_n x_n + B_n\\
\ga_n x_{n-1} + \al_n x_n = b_n.}
Поэтому
\eqn{x_n = -\frac{B_n - \frac{b_n}{\ga_n}}{A_n+\frac{\al_n}{\ga_n}}.}
Все остальные переменные вычисляем по формуле $x_{i-1} = A_i x_i + B_i$.

Вот такой простенький метод. Работает он за линейное время, точнее говоря, за $8n$ операций.
Остаётся только доказать, что он работает. А именно, есть проблема с делением на нуль при вычислении $A_i$ и $B_i$.

\begin{theorem}
Пусть $\al_1 = \al_n=1$ (этого всегда можно добиться). Пусть имеется диагональное преобладание
$|\al_i \ge |\ga_i| + |\be_i|$, причем $\ga_i, \be_i \neq 0$. Пусть $|\be_1| + |\ga_n|\ < 2$.
Тогда в алгоритме не возникнет деления на нуль, и $|A_i|\le 1$, то есть погрешность не будет расти.
\end{theorem}
\begin{proof}
Индукция. База: $|A_2| = \hm{\frac {\be_1}{\al_1}} \le 1$. Пусть уже доказано, что $|A_i|\le1$. Тогда
\eqn{|A_i\ga_i + \al_i|-|\be_i| \ge |\al_i| - |A_i|\cdot |\ga_i| - |\be_i| \ge |\ga_i|(\ub{1-|A_i|}_{\ge 0}) \ge 0.}
Значит, знаменатель дробей не меньше числителя (по модулю), и потому дробь меньше 1.

Далее, рассмотрим два случая.

\pt{1} Если $|\be_1| < 1$, то $|A_2| <1$, а потому предыдущее неравенство строгое, и все $|A_i| < 1$.
А если $\be_1 \ge 1$, то по условию $|\ga_n| < 1$. Поэтому $\frac{\al_n}{\ga_n} + A_n \neq 0$.

\pt{2} Если $|\be_1| \ge 1$, то по условию $|\ga_n| < 1$. Опять неравенства строгие, и $\hm{\frac{\al_n}{\ga_n}}<1$,
а потому $\frac{\al_n}{\ga_n} + A_n \neq 0$.
\end{proof}

\subsubsection{Метод стрельбы}

Напомним, что $\de^2 (z) := z_{n+1}-2z_n + z_{n-1}$.

Для решения краевых задач вида
\eqb{\De y -py = f,\\y(0)=a,\\y(X)=b}
можно использовать \emph{метод стрельбы}.
Он заключается в том, что мы строим обычную разностную схему для исходного уравнения, и для однородного (то есть когда $f\equiv 0$).
А именно,
\eqn{\begin{aligned}
\frac{\de^2 y}{h^2} -p_n y_n &= f_n,\\
\frac{\de^2 z}{h^2} -p_n z_n &= 0.
\end{aligned}}

Тогда зададим краевые условия для $y$ и $z$: $y_0 := a$, $z_0 := 0$.
А теперь зададим (от фонаря) значения функции во втором узле:
$y_1 := q$, $z_1 := r \ne 0$.
Из уравнений, написанных выше, мы найдём все остальные $y_i$ и $z_i$. Естественно ожидать, что краевое условие для $y$ будет нарушено.
Тогда, исходя из общей теории дифуров, можно найти  константу $C$ из уравнения $y_N + C z_N = b$.
После этого остаётся лишь вычислить <<правильное>> решение по формуле
$\wt y_n := y_n + C z_n$.

\begin{note}
Насчёт уточнения крайне абстрактного математического термина <<от фонаря>>: чтобы схему не разнесло, лучше брать $y_1 = a + O(h)$.
Да и $z_1$ лучше взять порядка $h$. Естественно, условие $z_1\neq 0$ нужно, чтобы не получить тривиальное решение.
\end{note}

\subsection{Повышение порядков аппроксимации. Метод баланса}

Основная идея повышения порядка заключается в получении некоторых соотношений на погрешности в силу самой системы.

\subsubsection{Пример номер раз}

Рассмотрим пример:
\eqb{
&y''(x) - p(x)y(x) = f(x),\\
&y(0)=a,\\
&y(X)=b.}

Легко видеть, что самая тупая аппроксимация этой схемы даст 2-й порядок. А мы сейчас сделаем 4-й.

Имеем
\eqn{y''(x_n) = \frac{\de^2 y}{h^2} + \frac{y^{(4)}(x_n)}{12}h^2 + O(h^4).}
А вот теперь применяем трюк: в силу системы
\eqn{y^{(4)} = \br{p(x) y(x)+f(x)}''  = \frac{\de^2 \br{p(x_n) y(x_n) + f(x_n)}}{h^2} + O(h^2).}
Итого
\eqn{\frac{\de^2 y_n}{h^2} -p_n y_n - \frac1{12} \de^2(p_ny_n+f_n) = f_n.}
\eqn{\frac{\de^2 y_n}{h^2} -p_n y_n - \frac1{12} \de^2(p_ny_n) = f_n + \frac1{12}\de^2(f_n).}
Заметим, что мы не вылетели из класса трёхдиагональных систем, но получили схему 4 порядка.
Весь фокус\т во <<вкусовых добавках>> с коэффициентом $\frac{1}{12}$.

\subsubsection{Пример номер два}

\eqb{
&y''(x) - p(x)y(x) = f(x),\\
&y'(0)-\al y(0)=a,\\
&y(X)=b.}

Здесь нам кроме самой системы ещё и краевое условие нужно аппроксимировать.
Самая простая схема даёт первый порядок:
\eqn{\frac{y_1-y_0}{h} -\al y_0 = a.}
Фигово. А можно сделать вот что: разложить $y(0)$ по Тейлору до 2 порядка:
\eqn{\frac{y_1-y_0}{h} -\al y_0 - a = y'(0) + \frac h2 y''(0) - \al y(0) -a + O(h^2) = \frac h2 y''(0) + O(h^2).}
В силу системы имеем $y''(0) = p(0) y(0) + f(0)$.
Поэтому схема
\eqn{\frac{y_1-y_0}{h} -\al y_0 - \frac h2 (p_0 y_0 + f_0) = a.}
имеет второй порядок.

\subsubsection{Метод баланса}

\begin{petit}
Про него всё равно ничего доказано не было. Он есть в книжке Бахвалова\ч Жидкова\ч Кобелькова
на страницах 464--466.

Точнее говоря, на странице 464 всё начинается с последнего абзаца и заканчивается формулой $(8)$ на странице 466.
Приведённая там схема и есть <<ответ>>.
\end{petit}

Ну ладно, настало время написать этот бред.

Рассмотрим уравнение
\eqn{\bcase{&\br{k(x)y'(x)}'-p(x)y(x)=f(x),\\
&y(0)=a,\\
&y(l)=b.}}

Здесь $p(x)\ge 0$, а $k(x)\ge k_0 > 0$, причём $k(x)\in \Cb^3$, а $p,f\in \Cb^2$ за исключением конечного числа точек.

Казалось бы, можно раскрыть производную произведения, но этого как раз делать не нужно, а то сходиться будет совсем плохо.

Обозначим $w(x) := k(x) y'(x)$.
Будем считать, что узлы сетки $x_i$ не попадают на точки разрыва наших функций.
Имеем
\eqn{(ky')'\evn{x_n} \approx \frac{(ky')'\evn{x_{n+\frac12}} - (ky')'\evn{x_{n-\frac12}}}{h} =
\frac{k(x_{n+\frac12})\cdot \frac{y_{n+1}-y_n}{h} - k(x_{n-\frac12})\cdot \frac{y_n-y_{n-1}}{h}}{h}.}
Но это нам ничего не даст. А именно, данная схема будет иметь порядок $O(h)$, и аппроксимация будет порядка $O(1)$.
Это полная лажа, надо придумывать что-то ещё (кому нужна аппроксимация с точностью до константы?).
Таким образом, это уравнение доставляет пример того, когда схема <<аппроксимация $+$ устойчивость>> не даёт сходимости.

Предлагаемый ниже метод называется \emph{методом баланса} и позволяет построить схему порядка $h^2$.
Правда, доказать, что она имеет именно такой порядок, весьма непросто, и мы этого делать не будем.

\def\tm{t_{n-\frac12}}
\def\xm{x_{n-\frac12}}
\def\xp{x_{n+\frac12}}

Ну-с, приступим: имеем
\eqn{w(\xp)-w(\xm) = \intl{\xm}{\xp}\br{p(x)y(x)+f(x)}\dx.}
Далее, $y(x) = y(x_n) + O(h)$, если $x-x_n = O(h)$.
Тогда
\eqn{\intl{\xm}{\xp}\br{p(x)y(x)+f(x) + O(h)}\dx = h\br{\ol p_n y_n + \ol f_n} + O(h^2),}
где
\eqn{\ol p_n := \frac1h\intl{\xm}{\xp}p(x)\dx, \quad \ol f_n := \frac1h\intl{\xm}{\xp}f(x)\dx.}
Делить на $h$ в этих формулах не так уж страшно, так как в пределе при $h\ra0$ получим $p_n$ и $f_n$ соответственно.

Рассмотрим схему
\eqn{\frac{w(\xp)-w(\xm)}{h} -\ol p_n y(x_n)= \ol f_n + O(h).}
А вот теперь начинаем хитрить. Введём новую переменную
\eqn{t(x) := \intl{0}{x}\frac{dx}{k(x)}, \quad t_n := \intl{0}{x_n}\frac{dx}{k(x)}.}
Тогда
\eqn{w(\xm) = k(x) \frac{dy}{dx}\evn{\xm} = \frac{dy}{dt}\evn{\tm} = \frac{y(x_n)-y(x_{n-1})}{t_n-t_{n-1}} + O(t_n-t_{n-1}).}
Введём ещё две функции
\eqn{\ol k_+ := \frac1h \intl{x_n}{x_{n+1}}\frac{dx}{k(x)}, \quad \ol k_- := \frac1h \intl{x_{n-1}}{x_n}\frac{dx}{k(x)}.}
Окончательно, схема имеет вид
\eqn{\frac1h\hr{\frac{y_{n+1} -y_n}{h \ol k_+} - \frac{y_n-y_{n-1}}{h\ol k_-}} -\ol p_n y_n = \ol f_n.}

\begin{petit}
Почему-то в конспекте чёрточки над $f_n$ не было. Что-то мне подсказывает, что это больше похоже на опечатку Арушаняна.
Иначе какого чёрта мы эту функцию вводили?!
\end{petit}

Далее читателю предлагается поверить в тот факт, что эта схема имеет порядок $O(h^2)$.
Можно про это прочесть в книжке БЖК, но  на лекциях этого не было, да и в программу не вошло.

\begin{petit}
Кстати, как вычислять те интегралы, которые тут фигурируют в большом количестве, не было сказано ни слова.
Видимо, по квадратурным формулам. С другой стороны, если использовать только узлы $x_i$, то будет как-то не очень точно,
а если использовать больше узлов, то это фактически означает уменьшение шага $h$.
\end{petit}






\subsection{Метод конечных элементов (проекционный метод)}

Тот метод, который мы рассмотрим сейчас, является очень частным случаем метода конечных элементов.
Точнее говоря, даже сам метод конечных элементов не есть чёткий алгоритм решения той или иной дифференциальной задачи,
а скорее представляет собой некоторую идею (или технологию). Ничего принципиально нового он в себе не несёт.

Ключевая идея метода (\те её вольная интерпретация автором конспекта) излагается ниже мелким текстом.

\begin{petit}

\textsl{Философское отступление о разных базисах в функциональных пространствах}

Разложение функций по какому\д нибудь базису было придумано очень давно. Но вот какой базис брать\т этот вопрос
часто бывает не праздным. Например, в некоторых случаях хорош тригонометрический базис, однако в задачах сжатия
информации уже давно поняли, что он не очень удачен. А вот зато базис Хаара (или как там по-русски пишут его фамилию?),
о котором речь пойдет ниже, себя оправдывает. Вообще, идея использования в качестве базиса функций-<<всплесков>> (<<wavelets>>),
то есть функций, которые имеют ярко выраженный экстремум, оказалась очень плодотворной, например, в задачах сжатия.
В нашем случае носители базисных функций будут очень маленькими, что позволит получить систему линейных уравнений с ленточной
матрицей. Конечно, хочется сказать так: <<давайте возьмём ортогональный базис и не будем мучаться>>, но беда в том, что по настоящему
ортогональному базису функция будет разлагаться плохо. Поэтому приходится жертвовать диагональностью матриц. Но это не так страшно,
потому что трёхдиагональные системы мы всё равно умеем решать за линейное время.

Вообще, метод конечных элементов заключается в том, что мы разлагаем функцию по базисным функциям с конечным носителем, и для тех точек,
где носители пересекаются, мы получаем какие-то уравнения. Этот метод можно использовать и для многомерных задач, и даже для областей
произвольной формы, но вид получаемой системы будет зависеть от способа нумерации наших <<конечных элементов>>, то есть кусочков области,
где сосредоточены носители базисных функций.
\end{petit}


Не претендуя на оригинальность и новизну, рассмотрим до боли знакомую одномерную задачу
\eqn{\case{-u''+pu=f,\\
u'+\al u\evn0=0,\\
u(l)=0.}}

Но мы будем искать не классическое решение, а обобщённое. А именно, перейдём к интегральной задаче, для начала спроецировав всё на сетку
на отрезке $[0,l]$ с шагом $h$ и $N$ узлами:
\eqn{a(u,v):=\intl{0}{l}(u'v'+puv)\dx + \al u(0) v(0) = (f,v),}
где
\eqn{(f,v):=\suml{i=1}{N-1} h u_i v_i.}
Будем говорить, что мы нашли приближённое решение, если данное уравнение выполняется при любой функции~$v$.
От <<настоящего>> обобщённого решения оно отличается лишь тем, что мы рассматриваем функции из подпространства
сеточных (На самом деле не совсем сеточных, а кусочно-линейных, продолженных линейным образом между узлами) функций.

Введём в нашем пространстве вот такой прикольный базис:
\eqn{v_i = \case{1,x=x_i;\\0,x\ne x_i.}}
Носитель $k$\д й функции есть отрезок $[x_{i-1},x_{i+1}]$. Очевидно, что если $|i-j| \ge 2$, то $\int v_i(x) v_j(x) \dx = 0$.
Отсюда следует, что при разложении по наших функций по базису $\hc{v_j}$ мы получим ленточную (точнее даже трёхдиагональную матрицу).
Как она получается, наверное, читатель догадается сам.

\begin{petit}
Для формалистов: на странице примерно 479 книжки БЖК (и далее ещё на 10 страницах) излагается <<варианционно-разностный метод Ритца>>,
в котором делается всё примерно то же самое. Там же (на стр. 480) явно вычислены коэффициенты разложения.
Пункты 4, 5 и 6 данного параграфа представляют собой обобщение рассмотренного нами случая (для сведения к частному случаю надо
в формуле (13) на странице 481 взять $k(x)\equiv1$.
В пункте 6 рассматривается в точности данная задача.

Тут была ещё теорема о том, что этот метод вообще сходится, и что матрица получится положительно определённой.
Но доказательство было неправильное, а в книжке написано хреново.
Но кому интересно, почитайте пункт 5 на странице 484.
\end{petit}



\subsection{Интегральные уравнения второго рода}

Будем рассматривать интегральные уравнения следующего вида
\eqn{\intl{0}{1}K(x,y)u(y)\dy = f(x).}
\eqn{\la\intl{0}{1}K(x,y)u(y)\dy + u(x) = f(x).}
Они называются интегральными уравнениями первого и второго рода  соответственно.

Предполагаем, что $\iint k\dx\dy < \bes$.

Эта задача, вообще говоря, является некорректно поставленной. Нужно определить, на каком классе функций мы ищем решения.

Первый метод решения состоит в приближении конечномерными интегральными операторами. Можно разложить ядро в ряд по системе функций $\ph_j$:
\eqn{K_n = \suml{i,j=1}{n}a_{ij}\ph_i(x)\ph_j(y).}
Если система полная, то $\hn{K_n-K}\ra 0$. Подынтегральное выражение и правую часть тоже разложим в ряды:
\eqn{u_n = \suml{i=1}{n}c_i\ph_i(x).}
\eqn{f_n = \suml{i=1}{n}d_i\ph_i(x).}
Можно доказать, что $u_n$ сходятся к точному решению $u$.
Тогда уравнение запишется в виде
\eqn{\la\suml{i,j=1}{n}a_{ij}\intl{0}{1}\ph_i(x)\ph_j(x)\suml{l=1}{n}c_l\ph_l(y)\dy + \suml{i=1}{n}c_i\ph_i(x) = \suml{i=1}{n}d_i\ph_i(x).}
Рассмотрим это уравнение при каждом фиксированном $i$.
Сумму по $l$ можно вытащить:
\eqn{\la\suml{i,j=1}{n}a_{ij}\hr{\suml{l=1}{n}\intl{0}{1}\ph_j(x)\ph_l(y)\dy}u_i + u_i = f_i.}
Получилась система линейных уравнений. Одно плохо: даже если система ортогональна, то матрица получается полная. То есть придётся решать
методом Гаусса, по-честному. А вот когда уравнение имеет вид свёртки, то становится легче: матрица становится <<полосатой>>.
При этом можно применить быстрое преобразование Фурье, и тогда свёртка перейдёт в произведение, и всё будет хорошо.

Второй способ\т замена интеграла квадратурной формулой. От этого тоже легче не станет, поскольку матрица-то будет полная.
Сходимость метода обеспечивается сходимостью квадратурной формулы. Но проблема в том, что система будет плохо обусловленная, и к ней надо
применять всякие улучшающие приемы (типа добавления скалярного оператора).

\end{document}
