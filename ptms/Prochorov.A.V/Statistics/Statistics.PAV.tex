\documentclass[a4paper,draft]{article}
\usepackage{dmvn}

\newcommand{\raP}{\stackrel{\Pf}{\ra}}
\newcommand{\rad}{\xrightarrow{\mathrm{\:d\:}}}
\newcommand{\raPnrai}{\xrightarrow[n\rightarrow\infty]{\Pf}}
\newcommand{\radnrai}{\xrightarrow[n\rightarrow\infty]{\mathrm{d}}}
\newcommand{\ranrai}{\xrightarrow[n\rightarrow\infty]{}}
\newcommand{\nrai}{n\ra\infty}

\newcommand{\iii}{\int\limits_{-\infty}^{+\infty}}

% Буквы с крышкой
\newcommand{\hatFn}{\widehat{F}_n}
\newcommand{\hatRn}{\widehat{R}_n}
\newcommand{\hattheta}{\hat{\theta}}

% Жирная тэта
\newcommand{\btheta}{\boldsymbol{\theta}}

% Нормальное распределение
\newcommand{\Norm}{\mathcal{N}}

\tocsubsectionparam{2.6em}
\tocsubsubsectionparam{3.5em}

% Для интервальных оценок
\def\utheta{{\underline\theta}}
\def\otheta{{\overline\theta}}
\def\ulp{\underline p}
\def\olp{\overline p}

% Modified by Steve L. Kuznetsov and Alexander V. Kharitonov
% Minor lazha correction by Timofey A. Arkhangelsky, January 2007

\begin{document}
\dmvntitle{Курс лекций по}{математической статистике}{Лектор\т Александр Васильевич Прохоров}
{III курс, 5 семестр, поток математиков}{Москва, 2006 г.} \pagebreak

\tableofcontents

\section*{Предисловие}

Ну что Вам рассказать про Сахалин?\dots Это незавершённый курс, который,
вообще  говоря, можно до\TeX ать. Но у нас нет на это времени.
Если у кого-то из читателей найдутся силы и желание это делать,
это будет просто замечательно. Исходные тексты будут выданы всякому,
кто захочет набрать остатки.

\subsubsection*{Дополнение к предисловию}
Текст существенно дополнен С.\,Л.\,Кузнецовым и А.\,В.\,Харитоновым.
Работа постепенно продвигается к завершению. Нам ещё много нужно сделать,
но многое мы уже сделали. Спасибо Тиме Архангельскому и Владимиру Гаврилину
за обнаружение и исправление лажи.
\begin{flushright}\it С.К., А.Х.\end{flushright}



\medskip\dmvntrail


\pagebreak

\section{Краткий обзор курса}

Математическая статистика\т это наука, посвященная разработке
оптимального вывода, основанного на неизвестных закономерностях.

Напомним некоторые основные определения из курса теории
вероятностей.

\begin{df}
\emph{Пространством элементарных событий} называется множество исходов
некоторого эксперимента. Элементарным событием называется любой
элемент пространства элементарных событий. Событием называется
любое подмножество пространства элементарных событий.
Экспериментом называется функция, принимающая значение на
пространстве элементарных событий.
\end{df}

\begin{df}
\emph{Генеральной совокупностью} называется достаточно большое, быть
может, бесконечное подмножество элементарных событий.
\end{df}

\begin{df}
\emph{Случайной величиной} называют функцию от элементарного события.
\end{df}

\subsection{Модель конечного случайного выбора}

Рассмотрим модель \лк Выбор без возвращения\пк. Пусть $N$ ---
общее число элементов генеральной совокупности, $M$ --- число
отмеченных (каким-то свойством) элементов, $n$ --- размер
выборки, \те число элементов, выбранных из генеральной
совокупности, $m$
--- число отмеченных элементов в выборке.

Вероятностная задача рассматривает случай, когда $n$, $M$ и $N$
заданы, а $m \in \hc{0 \sco \min(n,M)}$.
Тогда вероятность того, что среди выборки размера $n$ окажется
ровно $m$ отмеченных элементов, может быть вычислена по известной
формуле
$$Q_{n,m}(N,M)=\frac{\Cb_M^m \Cb_{N-M}^{n-m}}{\Cb_N^n}.$$

Статистическая задача ставится несколько иначе. Например:

а) Допустим, что $n$, $m$, $N$ известны, а $M$ --- неизвестно.
Требуется оценить $M$. Это в некотором смысле задача, обратная
вероятностной. Решить ее не так-то просто. Простейшее (но довольно
грубое) приближение для $M$ можно найти, например, из соотношений
$$\frac{M}{N}\approx\frac{m}{n},\quad M\approx\frac{m}{n}N.$$
Для того, чтобы найти более точные оценки, нужны специальные
методы, которыми и занимается математическая статистика.

б) Пусть заданы $n$, $m$ и $M$, а $N$ неизвестно. Требуется
оценить $N$. Пример такой задачи\т оценка числа рыб в водоеме:
производится выборка размера $M$, помечаются все рыбы из этой
выборки, а спустя некоторое время производится еще одна выборка
размера $n$ и подсчитывается число помеченных рыб $m$ из этой
выборки. По этим данным требуется оценить число рыб в водоеме.
Для решения этой задачи рассматривается вероятность
$\wt{Q}_{n,m}(N)$ как функция переменной $N$. Оказывается, что
функция $\wt{Q}$ сначала возрастает, а затем убывает. В качестве
оценки искомого значения $N$ выбирается такое целое $N_*$, для
которого $\wt{Q}_{n,m}(N)$ максимально. Можно показать, что
$$N_*=\hs{M\frac{n}{m}}\le M\frac{n}{m}.$$


Рассмотрим следующий эксперимент: два раза независимо друг от
друга бросается монетка. Можно рассматривать две модели этого
эксперимента:

1) 4 исхода: выпали последовательно орел--орел, орел--решка,
решка--орел, решка--решка. Каждому исходу приписывается
вероятность $\frac{1}{4}$.

2) 3 исхода: 2 орла, 2 решки, 1 орел и 1 решка; каждому исходу
приписывается вероятность $\frac{1}{3}$.

Практика показывает, что первая модель более соответствует
действительности, чем вторая: при большом числе испытаний каждый
из четырех исходов появляется с частотой, близкой к $\frac{1}{4}$,
в то время как во второй модели последний исход появляется с
частотой, близкой к $\frac{1}{2}$, а первые два --- с частотой
$\frac{1}{4}$, что плохо соответствует приписанным вероятностям.

В некотором смысле задача математической статистики обратна задаче
теории вероятностей. В теории вероятностей в каждой конкретной
ситуации вероятность считается полностью определенной и основной
задачей теории вероятностей является разработка методов нахождения
вероятностей различных сложных событий (исходя из известных
вероятностей более простых событий) для данной вероятностной
модели. В математической статистике рассматривается статистическая
модель, которая описывает такие ситуации, когда в вероятностной
модели изучаемого эксперимента имеется та или иная
неопределенность в задании вероятности, и задача математической
статистики состоит в том, чтобы уменьшить эту неопределенность,
уточнить (выявить) структуру статистической модели по результатам
проводимых наблюдений.

\subsection{Статистическая модель схемы Бернулли}

Зафиксируем число $n\in\mathbb{N}$. Рассмотрим случайные величины
$\xi_1$, $\xi_2$, \ldots, $\xi_n(\om)$ на некотором общем
вероятностном пространстве $(\Om,\As,\Pf)$, $\om\in\Om$. Их
совместное распределение:
$$\Pf(\xi_1=a_1,\xi_2=a_2,\ldots,\xi_n=a_n)=p^{a_1+\ldots+a_n}q^{n-(a_1+\ldots+a_n)},\quad
a_k\in\{0,1\}, \quad p,q\ge0,\quad p+q=1.
$$

Значение случайной величины $\xi_1$ --- исход первого испытания,
$\Pf(\xi_1=1)=p$, $\Pf(\xi_1=0)=q=1-p$, и аналогично для
$\xi_2,\ldots,\xi_n$. Отсюда $\Pf(\xi_1=a_1)=p^{a_1}q^{1-a_1}$, и
т.д. Значит,
$$\Pf(\xi_1=a_1,\xi_2=a_2,\ldots,\xi_n=a_n)=p^{a_1+\ldots+a_n}q^{n-(a_1+\ldots+a_n)}=
\prod\limits_{k=1}^n
\bbr{p^{a_k}q^{1-a_k}}=\Pf(\xi_1=a_1)\ldots\Pf(\xi_n=a_n).$$

Отсюда следует, что $\xi_1$, $\xi_2$, \ldots, $\xi_n$~---
независимые испытания.

Рассмотрим случайную величину $S_n=\xi_1+\ldots+\xi_n$. Она имеет
биномиальное распределение:
$$\Pf(\xi_1+\ldots+\xi_n=m)=\Cb_n^m p^m q^{n-m},\quad
S_n=\xi_1+\ldots+\xi_n,\quad \Mf S_n=np, \quad \Df S_n=npq.$$

Задача математической статистики~--- оценить неизвестное значение
$p$. Для этого используются три подхода~--- точечная оценка,
интервальная оценка и выбор из двух гипотез. Продемонстрируем
каждый из них на примере схемы Бернулли.

\subsubsection{Точечная оценка}

Запишем закон больших чисел для схемы Бернулли:
$$\frac{S_n}{n}=\frac{\xi_1+\ldots+\xi_n}{n}\raP \Mf\frac{S_n}{n}=\frac{np}{n}=p,
\quad n\to\infty.$$
\те частота появления успешного исхода $\frac{S_n}{n}$ сходится
по вероятности к параметру $p$: $\frac{S_n}{n}\raP p$,
$n\to\infty$.

Возьмем в качестве оценки параметра $p$ эту частоту
$\frac{S_n}{n}=:\hat{p}_n$. Это случайная величина со значениями
$\frac{m}{n}$, $m=0,\ldots,n$.

\begin{theorem} Эта оценка обладает следующими свойствами:

1) Несмещенность: $\Mf \hat{p}_n=p$.

2) Состоятельность: $\hat{p}_n\raP p$, $n\to\infty$.

3) Эффективность: Дисперсия частоты $\hat{p}_n$ является
наименьшей среди дисперсий всех других оценок, которые обладают
свойствами 1) и 2).
\end{theorem}

\begin{proof}
Выше уже было показано, что оценка $\hat{p}_n$ несмещенная (это
следует из того, что $\Mf S_n=np$), а в силу закона больших чисел
для схемы Бернулли она состоятельна; тем самым свойства 1) и 2)
доказаны.

Докажем свойство 3)~--- эффективность. Пусть $\tilde{p}_n$~---
любая оценка параметра $p$, удовлетворяющая условиям 1) и 2)
(несмещенность и состоятельность). Рассмотрим величину $\Mf
(\tilde{p}_n-p)^2$. Она называется средней квадратической ошибкой
оценки $\tilde{p}_n$.

Для несмещенных оценок средняя квадратическая ошибка совпадает с
дисперсией, в частности для нашей оценки $\hat{p}_n$: $\Mf
(\hat{p}_n-p)^2=\Df \hat{p}_n$.

Обозначим
$$\Pf(\xi_1=a_1,\xi_2=a_2,\ldots,\xi_n=a_n)=p^{a_1+\ldots+a_n}
(1-p)^{n-(a_1+\ldots+a_n)}=g(p;a_1,\ldots,a_n).$$
Для любого $p\in(0,1)$ имеет место равенство
$\sum\limits_{(a_1,\ldots,a_n)}g(p;a_1,\ldots,a_n)\equiv 1.$
Условие несмещенности оценки означает, что
$\Mf\tilde{p}_n=\sum\limits_{(a_1,\ldots,a_n)}\tilde{p}_n(a_1,\ldots,a_n)g(p;a_1,\ldots,a_n)=p.$
Рассмотрим $g(p;a_1,\ldots,a_n)=g(p)$ как функцию параметра $p$.
Тогда наши условия могут быть записаны в следующем виде
(суммирование ведется по всем наборам $(a_1,\ldots,a_n)$):
$$\left\{\begin{array}{l}
  \sum g(p)\equiv 1, \\
  \sum \tilde{p}_n\cdot g(p)\equiv p;
\end{array}\right.\quad 0<p<1.
$$

Продифференцируем каждое из этих соотношений по $p$, а затем,
умножив первое на $p$, вычтем его из второго; получим:
$$\sum (\tilde{p}_n-p) g'_p(p)\equiv 1.$$
Теперь представим $g'_p(p)$ как логарифмическую производную:
$g'_p(p)=g(p)\frac{\partial\ln g(p)}{\partial p}$, а затем
применим неравенство Коши--Буняковского, представив $g(p)$ в виде
$g(p)=\sqrt{g}\sqrt{g}$:
$$1\equiv \hr{\sum (\tilde{p}_n-p)g(p)\frac{\partial\ln g(p)}{\partial p}}^2
\le\hr{\sum (\tilde{p}_n-p)^2 g(p)}\hr{\sum \hs{\frac{\partial\ln
g(p)}{\partial p}}^2 g(p)}.$$

Так как $\Mf\tilde{p}_n=p$ (условие несмещённости оценки
$\tilde{p}_n$), то первый из множителей в правой части этого
неравенства~--- это дисперсия $\tilde{p}_n$. Обозначим второй
множитель через $I(p)$, тогда неравенство перепишется в виде
$1\le\Df\tilde{p}_n\cdot I(p)$, или
$\Df\tilde{p}_n\ge\frac{1}{I(p)}$. Найдем $I(p)$ в явном виде:
$$I(p)=\Mf\hs{\frac{\sum\xi_k}{p}-\frac{n-\sum\xi_k}{1-p}}^2=
\frac{\Mf\hr{\sum\xi_k-np}^2}{p^2(1-p)^2}=\frac{\Df\hr{\sum\xi_k}}{p^2(1-p)^2}=
\frac{np(1-p)}{p^2(1-p)^2}=\frac{n}{p(1-p)}.$$ Подставляя
найденное значение $I(p)$ в неравенство для дисперсии, получаем:
$\displaystyle\Df\tilde{p}_n\ge\frac{p(1-p)}{n}=\Df\hat{p}_n,$
\те оценка~$\hat{p}_n$ действительно обладает наименьшей
дисперсией из всех несмещенных состоятельных оценок $\tilde{p}_n$.
Теорема доказана.
\end{proof}

Если задана произвольная оценка $\hat{p}_n$ параметра $p$, то
представим ее математическое ожидание $\Mf \hat{p}_n$ в виде $\Mf
\hat{p}_n=p+\Delta_n$. Тогда $\Delta_n$ называется смещением
оценки $\hat{p}_n$. Несмещенные оценки обладают нулевым смещением:
$\Delta_n=0$.

Для нашей оценки $\hat{p}_n=\frac{S_n}{n}$, очевидно, $\Df
\hat{p}_n\to0$, $n\to\infty$, \те частота обладает наименьшим
рассеянием, если рассеяние измеряется с помощью дисперсии.

\subsubsection{Интервальная оценка}

Интервальной оценкой параметра $p$ называется интервал $[\bar
p_n,\bar{\bar{p}}_n]$, который обладает следующим свойством:
$\Pf\bc{p\in[\bar p_n,\bar{\bar{p}}_n]}\ge 1-\al$, $0<\al<1$;
$\bar p_n=\bar p_n(\al)$, $\bar{\bar{p}}_n=\bar{\bar{p}}_n(\al)$.
При этом длина интервала должна быть наименьшей.

\begin{ex}
Рассмотрим $n=100$ бросаний правильной монеты (схема Бернулли с
параметром $p=0{,}5$), $x_k$~--- исход $k$-го испытания (значение
бернуллиевской случайной величины $\xi_k$);
$S_n=\xi_1+\ldots+\xi_n$. Очевидно, $\Pf(0\le S_n\le 100)=1$.
Прямой подсчет вероятностей показывает, что $$\Pf(35\le S_n\le
65)=0{,}99822;\quad \Pf(39\le S_n\le 61)\approx 0{,}98.$$

Таким образом, $\bs{\frac{35}{100},\frac{65}{100}}$~---
доверительный интервал для параметра $p$ с доверительной
вероятностью $0{,}99822$; а
$\bs{\frac{39}{100},\frac{61}{100}}$~--- доверительный интервал
для параметра $p$ с доверительной вероятностью $\approx 0{,}98$.
\end{ex}


Для построения доверительного интервала для схемы Бернулли запишем
для оценки $\hat{p}_n=\frac{S_n}{n}$ неравенство Чебышёва:
$$\Pf\hr{\bbm{\frac{S_n}{n}-\Mf\frac{S_n}{n}}\ge\ep}\le\frac{\Df\frac{S_n}{n}}{\ep^2}
\quad \Longrightarrow \quad
\Pf\hr{\bbm{\frac{S_n}{n}-\Mf\frac{S_n}{n}}\le\ep}\ge
1-\frac{\Df\frac{S_n}{n}}{\ep^2},$$
$$\Pf\br{\bm{\hat{p}_n-p}\le\ep}\ge 1-\frac{pq}{n\ep^2}\ge
1-\frac{1}{4n\ep^2}\ge 1-\al, \quad \al=\frac{1}{4n\ep^2}.$$

Теперь зададим произвольное $\al\in(0,1)$. Тогда для
$\ep_\al=\sqrt{\frac{1}{4n\al}}$ получим:
$$\Pf\br{\bm{\hat{p}_n-p}\le\ep_\al}\ge 1-\al, \quad\mbox{ \те }
\quad\Pf\br{\hat{p}_n-\ep_\al\le p\le\hat{p}_n+\ep_\al}\ge
1-\al.$$

Таким образом, мы построили интервал $[\hat{p}_n-\ep_\al,\hat{p}_n+\ep_\al]$, в котором с задаваемой нами
вероятностью ошибки $\al$ находится неизвестный параметр $p$. Он называется доверительным интервалом для
параметра $p$ с доверительной вероятностью $1-\al$ (или с вероятностью ошибки $\al$). Чем меньше мы выбираем
$\al$, тем больше этот интервал. Для заданного $\al$ длину интервала можно уменьшить за счёт увеличения числа
испытаний $n$.

Укажем еще один (более точный) способ нахождения интервальной
оценки в схеме Бернулли. По теореме Муавра--Лапласа число <<успехов>> схемы Бернулли с ростом $n$ стремится к нормальной
случайной величине:
\eqn{\frac{S_n-\Mf S_n}{\sqrt{\Df S_n}}=\frac{S_n-np}{\sqrt{n pq}}\stackrel{\Dc}{\to}
\Nc(0,1).}

Используя это, можно оценить вероятность
$$\textstyle\Pf\br{\bm{\hat{p}_n-p}\le\ep}=\Pf\bbr{\bm{\frac{S_n-np}{n}}\sqrt{\frac{n}{pq}}
\le\ep\sqrt{\frac{n}{pq}}}\simeq
\Phi\bbr{\ep\sqrt{\frac{n}{pq}}}-\Phi\bbr{-\ep\sqrt{\frac{n}{pq}}}=2\Phi(u)-1,
\quad u=\ep\sqrt{\frac{n}{pq}}.
$$

Здесь $\Phi(u)=\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^u
e^{-\frac{t^2}{2}}\,dt$~--- функция распределения стандартной
нормальной случайной величины.

Фиксируем $0<\al<1$. Нам нужно, чтобы $2\Phi(u)-1=1-\al$, \те
$\Phi(u)=1-\frac{\al}{2}$. Обозначим такое значение $u$, при
котором это выполнено, через $u_{1-\frac{\al}{2}}$ (квантиль
порядка $1-\frac{\al}{2}$ нормального распределения, находится из
таблицы квантилей). Тогда искомое $\ep$ найдем из условия
$\ep\sqrt{\frac{n}{pq}}=u_{1-\frac{\al}{2}}$;
$\ep=u_{1-\frac{\al}{2}}\sqrt{\frac{p(1-p)}{n}}$. Итак,
неравенство
\eqn{\bm{\hat{p}_n-p}\le u_{1-\frac{\al}{2}}\sqrt{\frac{p(1-p)}{n}}}
выполняется с вероятностью $\simeq 1-\alpha$. Осталось найти границы
доверительного интервала. Возведем неравенство в квадрат:

$${\br{\hat{p}_n-p}}^2\le
u_{1-\frac{\al}{2}}^2\frac{p(1-p)}{n}.$$

Получили квадратное уравнение на $p$. В качестве $\bar p_n$ и
$\bar{\bar{p}}_n$ берут корни этого квадратного уравнения (можно
показать, что всегда $D>0$ и корней действительно два).

\subsubsection{Выбор из двух гипотез}

Пусть задано $p_0$. Рассмотрим две (взаимоисключающих) гипотезы о
параметре $p$: $H_0$ (основная, или нулевая, гипотеза) и $H_1$
(альтернативная, или конкурирующая, гипотеза). (Например,
$H_0\colon p=p_0$, $H_1\colon p\neq p_0$.) Наша задача: выбрать из
этих двух гипотез ту, которой соответствует наименьшая вероятность
ошибки.

\begin{df}
Вероятность ошибки I рода~--- это вероятность отклонить верную
гипотезу $H_0$. Вероятность ошибки II рода~--- это вероятность
принять неверную гипотезу $H_0$.
\end{df}

Критерий проверки гипотезы $H_0$~--- это правило, на основании
которого мы можем считать, что она верна или неверна (\те
принимаем ее или не принимаем). Составим таблицу:
$$\begin{array}{r|c|c|}
  & H_0 & H_1 \\ \hline
  \mbox{\small принимаем } H_0 &  & \quad\beta\quad \\ \hline
  \mbox{\small отклоняем } H_0 & \quad\alpha\quad &
\end{array}$$

Здесь вероятность ошибки I рода (отклоняем $H_0$ в то время как
она верна) обозначена через $\al$, а вероятность ошибки II рода
(принимаем неверную гипотезу $H_0$)~--- $\beta$.

Рассмотрим две гипотезы $H_0\colon p=p_0$, $H_1\colon p\neq p_0$
($p_0$ задано). Пусть для параметра $p$ получена интервальная
оценка для заданной вероятности ошибки $\al$~--- доверительный
интервал $[\bar p_n,\bar{\bar{p}}_n]$. Тогда можно предложить
такой критерий:

\quad 1) Если $p_0\in[\bar p_n,\bar{\bar{p}}_n]$, то $H_0$
принимаем (и соответственно, отклоняем $H_1$);

\quad 2) Если $p_0\not\in[\bar p_n,\bar{\bar{p}}_n]$, то $H_0$
отклоняем (и тем самым принимаем $H_1$).

Поскольку $[\bar p_n,\bar{\bar{p}}_n]$~--- доверительный интервал
с доверительной вероятностью $1-\al$, то вероятность ошибки I рода
не превосходит $\al$.

Рассмотрим еще один пример гипотез о параметре $p$ схемы Бернулли.
Пусть $H_0\colon p=p_0$, $H_1\colon p=p_1$, где $p_0<p_1$~---
заданы, и пусть $\al$~--- вероятность ошибки I рода, $\beta$~---
вероятность ошибки II рода. Как всегда, обозначаем
$S_n=\xi_1+\ldots+\xi_n$. Тогда существует такой критерий: если
$S_n>m_*$, то $H_0$ отклоняем (тем самым принимая $H_1$), а если
$S_n\le m_*$, то $H_0$ принимаем ($H_1$ отклоняем). Число $m_*$
называется критическим значением и находится из соображений
минимизации при фиксированном $n$ сумм (вероятностей ошибок I и II
рода)
$$\al=\sum_{m=m_*+1}^n C_n^m p_0^m (1-p_0)^{n-m},\quad
\beta=\sum_{m=0}^{m_*}C_n^m p_1^m (1-p_1)^{n-m}.$$

\begin{problem}
Пусть заданы $0<\al'<1$ и $0<\beta'<1$. Найти наименьшее значение
$n$ и соответствующее ему $m_*(n)$, такие что данный критерий
различает гипотезы $H_0$ и $H_1$ с вероятностями ошибок I и II
рода, не превосходящими соответственно $\al'$ и $\beta'$.
\end{problem}

На этом мы завершаем обзор. Далее речь пойдёт подробнее о точечных оценках,
интервальных оценках и проверке гипотез.

\section{Точечные оценки}
\subsection{Общие понятия математической статистики}

\subsubsection{Статистическая модель}

Фундаментальным понятием теории вероятностей является
вероятностная модель (вероятностное пространство)~--- это тройка
$(\Om,\As,\Pf)$, где $\Om$ --- пространство элементарных событий,
$\As$~--- $\sigma$-алгебра подмножеств этого пространства
(событий), $\Pf$~--- вероятностная мера на $\sigma$-алгебре $\As$.

Основным объектом исследования математической статистики является
статистическая модель. Определим это понятие.

Результатом статистического эксперимента являются вещественные
числа $x_1,\ldots,x_n$~--- статистические данные. Это
значения случайных величин $\xi_1,\ldots,\xi_n$. Их совокупность
$x^{(n)}=(x_1,\ldots,x_n)$ называется \emph{выборкой} размера (порядка) $n$.

\begin{df}
\emph{Статистической моделью} называется тройка $(\Xs,\Bs(\Xs),\Ps)$, где
$\Xs=\{x^{(n)}\}$~--- выборочное пространство, \те совокупность
всевозможных выборок размера $n$, $\Bs(\Xs)$~--- $\sigma$-алгебра на
выборочном пространстве, $\Ps=\{\Pf\}$~--- некоторое \emph{семейство}
распределений вероятностей, заданное на $\Bs(\Xs)$.
\end{df}
В простейшем случае
считаем, что $\Xs\! \subseteq \mathbb{R}^m$, а $\Bs(\Xs)$ --- борелевская
$\sigma$-алгебра.

Примерами семейств распределений могут служить, например,
семейство бернуллиевских распределений, $p\in[0,1]$; семейство
пуассоновских распределений, $\lambda\in(0,\infty)$; семейство
биномиальных распределений с параметрами $(n,p)$, где $n$
фиксировано, а $p\in(0,1)$ и т.д.


Наша цель\т выделить из семейства распределений то единственное
распределение, которое наилучшим образом соответствует нашим
запросам, точнее, полученной выборке (после этого мы сможем работать
с вероятностной моделью).

Если $\Ps=\{\Pf_\theta \mid \theta\in\Theta\}$, где $\theta$~---
параметр, $\Theta\subset\R^m$~--- параметрическое множество, то
говорят, что $\Ps$~--- \emph{параметрическое семейство распределений},
а $(\Xs, \Bs(\Xs), \Ps)$~--- \emph{параметрическая модель}.

Пусть имеется случайный вектор $\xi^{(n)}=\{\xi_1,\ldots,\xi_n\}$
со значениями $(x_1,\ldots,x_n)$ в выборочном пространстве
$\Xs\!\!$.
В соответствии с определением статистической модели, $\Ps$~---
семейство распределений случайного вектора $\xi^{(n)}$.
Чтобы не путать набор случайных величин (случайный вектор)
$\xi_1,\ldots,\xi_n$ с его конкретными значениями
$x_1,\ldots,x_n$, говорят, что $x_1,\ldots,x_n$\т выборка, а
$\xi_1,\ldots,\xi_n$ $(\xi_i=\xi_i(\om))$\т случайная выборка.

\subsubsection{Обоснование предельного перехода при стремлении размера
выборки к бесконечности}

%Обозначим выборочные пространства буквой $R$.
Считаем, что $R^{(n)}$~---
$n$-мерное евклидово пространство\footnote{Почему-то лектор
обозначил его нестандартно... --- \emph{примеч. С.\,К.}}.
 Рассмотрим последовательность
выборочных пространств
$$
(R^{(1)}, \Bs(R^{(1)})), \dots, (R^{(n)}, \Bs(R^{(n)})), \dots
$$
с вероятностными мерами $\Pf_1, \dots, \Pf_n, \dots$. Исследуем их
предельные свойства при $n \to \infty$.

\begin{df}
Вероятностные меры $\Pf_1, \dots, \Pf_n, \dots$ называются
\emph{согласованными}, если
$$
\forall\, n \in \mathbb{N} \quad \forall\, B \in \Bs(R^{(n)}) \quad
\Pf_{n+1}(B \times \mathbb{R}) = \Pf_n(B)
$$
\end{df}

Введём пространство $(R^{(\infty)}, \Bs(R^{(\infty)}))$, где
$R^{(\infty)} = \{ x = (x_1, \dots, x_n, \dots) \mid x_k \in \mathbb{R} \}$,
$\Bs(R^{(\infty)})$~--- борелевская $\sigma$-алгебра.

\begin{df}
Пусть $B \in \Bs(R^{(n)})$. Тогда \emph{борелевским цилиндром} называется
следующее множество:
$$
Z_n(B) = \{ x = (x_1, \dots) \in R^{(\infty)} \mid (x_1, \dots x_n) \in B \}.
$$
\end{df}

\begin{theorem}[Колмогоров]
Если меры на $R^{(1)}, \dots, R^{(n)}, \dots$ согласованы, то
существует единственная вероятностная мера $\Pf$ на
$(R^{(\infty)}, \Bs(R^{(\infty)}))$ такая, что $\Pf(Z_n(B)) =
\Pf_n(B)$ для всех $B \in \Bs(R^{(n)})$ и для всех натуральных
$n$.
\end{theorem}

Эта теорема обосновывает законность перехода к пределу при $n \to \infty$
($n$ --- размер выборки).

\subsubsection{Модель повторных испытаний}

\begin{df}
\emph{Моделью повторных испытаний} называется статистическая модель, в
которой случайные величины $\xi_1,\ldots,\xi_n$ (со значениями
$x_1,\ldots,x_n$ соответственно, $(x_1,\ldots,x_n)\in \Xs$)
независимы и одинаково распределены.
\end{df}

В дальнейшем мы будем рассматривать только модели повторных
испытаний.

\begin{ex}
Рассмотренная выше статистическая модель схемы Бернулли~--- модель
повторных испытаний. Действительно, в этом случае рассматриваются
независимые испытания с одним и тем же распределением вероятности
$\Pf(\xi=0)=p$, $\Pf(\xi=1)=1-p$, где $p\in(0,1)$~--- параметр.
\end{ex}

\begin{ex}
Рассмотрим эксперимент по измерению температуры. Мы считаем, что
измерения независимы и результаты измерений~--- значения одинаково
распределенных случайных величин $\xi_1,\ldots,\xi_n\sim\xi$. На
практике обычно результаты измерений колеблются около некоторого
постоянного значения $a$, поэтому удобно рассматривать $\xi$ в
виде $\xi=a+\Delta$, где $\xi=(\xi_1,\ldots,\xi_n)$,
$\Delta=(\Delta_1,\ldots,\Delta_n)$~--- случайная ошибка, или в
координатах: $\xi_k=a+\Delta_k$, $k=1,\ldots,n$. Случайные
величины $\Delta_1,\ldots,\Delta_n$ также независимы и одинаково
распределены; при этом $\Mf\Delta_k=0$, $\Df\Delta_k=\sigma^2$
$\fa k$. Средняя температура вычисляется как среднее
арифметическое результатов измерений:
$$\bar\xi_n=\frac{\xi_1+\ldots+\xi_n}{n}=a+\frac{\Delta_1+\ldots+\Delta_n}{n},\quad
\bar\Delta_n=\frac{\Delta_1+\ldots+\Delta_n}{n} \mbox{ --- средняя
ошибка, } \quad\Mf\bar\Delta_n=0,\quad
\Df\bar\Delta_n=\frac{\sigma^2}{n}.$$
\end{ex}

\subsubsection{Выборочные характеристики} \label{char}

Пусть в некоторой статистической модели имеется выборка порядка
$n$: $x_1,\ldots,x_n$.

\begin{df}
\emph{Статистикой} называется произвольная измеримая функция
$f(x_1,\ldots,x_n)$ от элементов выборки $x_1,\ldots,x_n$.
\end{df}

\begin{df}
Если случайная величина $\xi$ имеет распределение $F(x)$, то
\emph{медианой} распределения называется такое число $\mu$, что
$F(\mu)=\frac{1}{2}$.
\end{df}
Медиана распределения обладает тем свойством, что $\Pf(\xi\ge\mu)=\Pf(\xi\le\mu)$.

Рассмотрим примеры наиболее часто встречающихся статистик (или \emph{выборочных характеристик}):

\begin{items}{-2}
\item Выборочное среднее:
\eqn{\ol x_n := \suml{i=1}{n} x_i;}

\item Выборочная дисперсия:
\eqn{S_n^2:= \frac{1}{n}\suml{i=1}{n}(x_i-\ol x_n)^2;}

\item Выборочный момент порядка $k$:
\eqn{\wh{\mu}_k= \frac{1}{n}\suml{i=1}{n} x_i^k}

\item Выборочный центральный момент порядка $k$:
\eqn{\wh{M}_k=\frac{1}{n}\suml{i=1}{n} (x_i-\ol x_n)^k;}

\item Порядковые статистики: упорядочим элементы выборки по возрастанию, получим последовательность
\eqn{x_{(1)}\sco x_{(n)}.}
Она называется вариационным рядом выборки, а её элементы\т порядковыми статистиками.
Случайные величины $\xi_{(n)}$ со значениями $x_{(n)}$ также называются порядковыми статистиками.
Более формально,
\begin{gather}
x_{(1)}:= \min(x_1,\ldots,x_n),\\
x_{(2)}:=\max\bs{\minl{k}(x_1 \sco \wh{x_k}\sco x_n)},\\
x_{(3)}:=\max\bs{\minl{i\neq j}(x_1 \sco \wh{x_i} \sco \wh{x_j} \sco x_n)},\\
\ldots\ldots\ldots\\
x_{(n)}:=\max(x_1 \sco ,x_n),
\end{gather}
где <<крышка>>, как обычно, означает пропуск этого элемента.

\item Выборочная медиана:
\eqn{\wh\mu=\case{x_{(m)}, & n=2m-1; \\ \frac12\hr{x_{(m)}+x_{(m+1)}}, & n=2m.}}
\end{items}

\begin{ex}
Рассмотрим равномерное распределение на $[0,\theta]$, $\theta$~--- неизвестный
параметр. Параметр $\theta$ можно оценить двумя способами:
\begin{nums}{-2}
\item $\wh\theta^1 = 2 \ol x_n$ --- несмещённая оценка
\item $\wh\theta^2 = x_{(n)}$ (оценка по крайней точке) --- смещённая,
но средне-квадратичная ошибка меньше, чем у $\wh\theta^1$.
\end{nums}
\end{ex}

\subsection{Эмпирическая функция распределения}
\subsubsection{Определение и свойства}

\begin{df}
\emph{Эмпирической функцией распределения} для данной выборки
$x_1,\ldots,x_n$ называется функция
$$\hatFn(x;x_1,\ldots,x_n)=\frac{1}{n}\sum_{k=1}^n I_{(-\infty,x]}(x_k)=
\frac{1}{n}\sum_{k=1}^n I_{(x_k\le x)},$$ где
$I_A(x)=\left\{\begin{array}{cl}
  1, & x\in A; \\
  0, & x\not\in A.
\end{array}\right.$~--- индикатор множества $A$.
\end{df}

Перейдем от выборки $x_1,\ldots,x_n$ к вариационному ряду
(совокупности порядковых статистик); иными словами, упорядочим
выборку по возрастанию: $x_{(1)},\ldots,x_{(n)}$. Тогда, очевидно,
эмпирическая функция распределения может быть записана в виде
$$\hatFn(x;x_1,\ldots,x_n)=\hatFn(x;x_{(1)},\ldots,x_{(n)})=\left\{\begin{array}{cl}
  0, & x<x_{(1)}; \\
  \frac{k}{n}, & x_{(k)}\le x<x_{(k+1)}, 1\le k\le n-1; \\
  1 & x\ge x_{n}.
\end{array}\right.$$

Если выборка $x_1,\ldots,x_n$ фиксирована, то эмпирическая функция
распределения~--- это функция от переменной $x\in\R$: $\hatFn%
(x;x_1,\ldots,x_n)=\hatFn(x).$ Она является функцией
распределения некоторой случайной величины $\xi$ (проверьте это!):
$\hatFn(x)=F_\xi(x)=\Pf(\xi\le x)$.

Если же выборка не фиксирована, а случайные величины
$\xi_1,\ldots,\xi_n$, породившие эту выборку, независимы и
одинаково распределены с функцией распределения $F(x)$, то можно
рассматривать $\hatFn(x;\xi_1,\ldots,\xi_n)$. Для каждого
$x\in\R$ это случайная величина:
$\hatFn(x;\xi_1,\ldots,\xi_n)=\hatFn(x;\om).$

\begin{theorem}
1) Случайная величина $n\hatFn(x;\xi_1,\ldots,\xi_n)$ имеет
биномиальное распределение с параметрами $(n,p=F(x))$ при любом
фиксированном $x\in\R$;

2) $\hatFn(x;\xi_1,\ldots,\xi_n)$ является несмещенной
состоятельной оценкой $F(x)$;

3) $\Pf\bbr{\lim\limits_{n\to\infty}\hatFn%
(x;\xi_1,\ldots,\xi_n)=F(x),\fa x\in\R}=1$;

4) $n\hatFn%
(x;\xi_1,\ldots,\xi_n)\stackrel{d}{\to}\Nc\br{F(x),\frac{F(x)(1-F(x))}{n}}$,
$n\to\infty$.
\end{theorem}

\begin{proof}
Найдем распределение случайной величины $\hatFn%
(x;\xi_1,\ldots,\xi_n)$ при любом фиксированном $x\in\R$. Если
$\xi_k\le x$, то $I_{(-\infty,x]}(\xi_k)=1$, а если $\xi_k>x$, то
$I_{(-\infty,x]}(\xi_k)=0$. Значит, при каждом $x\in\R$ $\hatFn%
(x;\xi_1,\ldots,\xi_n)$~--- это частота наступления события
$\{\xi_k\le x\}$, вероятность которого равна $\Pf(\xi_k\le
x)=F_{\xi_k}(x)=F(x)$. Отсюда получаем, что случайная величина
$n\hatFn(x;\xi_1,\ldots,\xi_n)$ имеет биномиальное распределение с
параметрами $(n,p=F(x))$ при любом фиксированном $x\in\R$
(утверждение 1) теоремы). Её математическое ожидание и дисперсия
соответственно равны $np$ и $np(1-p)$, поэтому получаем
\begin{equation} \label{matdisp_emp}
\Mf\hatFn(x;\xi_1,\ldots,\xi_n)=F(x), \quad \Df\hatFn%
(x;\xi_1,\ldots,\xi_n) =\frac{F(x)\br{1-F(x)}}{n}\quad \fa
x\in\R.
\end{equation}

Таким образом, эмпирическую функцию распределения $\hatFn%
(x;\xi_1,\ldots,\xi_n)$ можно рассматривать как оценку
(теоретической) функции распределения $F(x)$. Поскольку $\Mf\hatFn%
(x;\xi_1,\ldots,\xi_n)=F(x)$, то эта оценка несмещенная, а в
силу неравенства Чебышева $\Pf\br{|\hatFn%
(x;\xi_1,\ldots,\xi_n)-F(x)|\ge\ep}\le\frac{\Df\hatFn%
}{\ep^2}=\frac{p(1-p)}{n\ep^2}\to0,$ $n\to\infty$ $\fa x\in\R$,
\те $\hatFn(x;\xi_1,\ldots,\xi_n)\raP F(x)$, $n\to\infty$ $\fa
x\in\R$, значит эта оценка состоятельная. Таким образом,
утверждение 2) также доказано.

Утверждение 3) следует из УЗБЧ для схемы Бернулли (теорема
Бореля), а утверждение 4)~--- из формул (\ref{matdisp_emp}) и
центральной предельной теоремы для независимых одинаково
распределенных случайных величин, примененной к сумме $n\hatFn%
(x;\xi_1,\ldots,\xi_n)=\sum\limits_{k=1}^n
I_{(-\infty,x]}(\xi_k)$.
\end{proof}

\begin{problem}
Доказать, что $\hatFn(x;\xi_1,\ldots,\xi_n)$ является
эффективной оценкой $F(x)$ $\fa x\in\R$.
\end{problem}

На самом деле имеет место еще более сильное утверждение, чем
утверждение 3) доказанной теоремы, а именно равномерная сходимость
$\hatFn(x;\xi_1,\ldots,\xi_n)$ к $F(x)$ с вероятностью 1, что и
составляет содержание следующей теоремы.

\subsubsection{Теорема Гливенко\ч Кантелли}

\begin{theorem}[Гливенко\ч Кантелли]
Пусть $\xi_1,\ldots,\xi_n$~--- взаимно независимые, одинаково
распределенные случайные величины с функцией распределения $F(x)$;
$\hatFn(x;\xi_1,\ldots,\xi_n)=\frac{1}{n}\sum\limits_{k=1}^n
I_{(-\infty,x]}(\xi_k)$~--- их эмпирическая функция распределения.
Тогда $$\Pf\bbr{\liml{n\ra\bes} \supl{x}\biggl|{\wh{F}_n(x;\xi_1,\ldots,\xi_n)-F(x)}\biggr|=0}=1$$ (\те $\hatFn(x;\xi_1,\ldots,\xi_n)$ равномерно
 сходится к $F(x)$ с вероятностью 1).
\end{theorem}

\begin{note}
Для определения эмпирической функции распределения в теореме
Гливенко\ч Кантелли не требуется понятия выборки: она определяется
для заданного (известного) набора взаимно независимых одинаково
распределенных случайных величин.
\end{note}

\begin{proof}
Для краткости будем обозначать эмпирическую функцию распределения
через $\hatFn(x)=\hatFn(x;\om)$. По условию теоремы,
$F(x)$~--- функция распределения случайных величин
$\xi_1,\ldots,\xi_n$. Рассмотрим два случая.

1) Пусть $F(x)$~--- непрерывная и строго монотонная функция.
Фиксируем произвольное $\ep>0$ и $k\in\N$, $k\neq1$:
$\frac{1}{k}\le\ep$. Поскольку функция $F(x)$ непрерывна и строго
монотонна, то для каждого $i=0,\ldots,k$ найдется $x_i$:
$F(x_i)=\frac{i}{k}$ (возможно, $x_1=-\infty$ или $x_k=+\infty$),
причем такие $x_1,\ldots,x_k$ определены однозначно. Для соседних
точек, по определению,
\begin{equation} \label{1kan}
F(x_{i+1})-F(x_i)=\frac{1}{k}\le\ep.
\end{equation}

Зафиксируем $i$ и произвольную точку $x$: $x_{i}<x<x_{i+1}$. В
силу монотонности функций $F$ и $\widehat F$ имеем:
\begin{equation}\label{2kan}
\hatFn(x_i)-F(x_{i+1})\le \hatFn(x)-F(x)\le \hatFn%
(x_{i+1})-F(x_i),
\end{equation} и используя неравенство
(\ref{1kan}), отсюда получаем:
$$\hatFn(x_i)-F(x_i)-\ep\le \hatFn(x)-F(x)\le \hatFn%
(x_{i+1})-F(x_{i+1})+\ep,$$
$$\bm{\hatFn(x)-F(x)}\le\max\limits_{0\le i\le k}\bm{\hatFn(x_i)-F(x_i)}+\ep,
\quad \fa x\in\R, \quad\Longrightarrow$$
\begin{equation} \label{3kan}
\sup\limits_{x\in\R}\bm{\hatFn(x)-F(x)}\le\max\limits_{0\le i\le
k}\bm{\hatFn(x_i)-F(x_i)}+\ep,\quad \fa\ep>0,\mbox{ где }
\frac{1}{k}\le\ep.
\end{equation}

Рассмотрим событие $A_i=\bc{\om:\enskip \hatFn(x_i;\om)\to
F(x_i), n\to\infty}.$ По УЗБЧ для схемы Бернулли $\Pf(A_i)=1$.
Далее, рассмотрим событие $A^{(k)}=\bigcap\limits_{i=1}^k A_i$.
Его вероятность равна $\Pf(A^{(k)})=1$ (проверьте!). Очевидно,
событие $A^{(k)}$ равносильно тому, что $\max\limits_{0\le i\le k}
\bm{\hatFn(x_i;\om)-F(x_i)}\to0$, $n\to\infty$. Определим
события $$\tilde A=\bigcap_{k=2}^\infty A^{(k)}, \quad B=\bbc{\om:
\enskip \sup_{x\in\R}\bm{\hatFn(x_i;\om)-F(x_i)}\to0,
n\to\infty}.$$

В силу неравенства (\ref{3kan}) $\tilde A\subset B$, а поскольку
$\Pf(\tilde A)=1$, то и $\Pf(B)=1$.

2) Пусть теперь $F(x)$~--- произвольная (неубывающая) непрерывная
функция. Тогда определим $x_i$ так:
$$x_i=\inf\bc{x: \enskip F(x-0)\le\frac{i}{k}\le F(x)}.$$
Далее рассуждаем аналогично первому случаю. Осталось заметить, что
при применении УЗБЧ для схемы Бернулли в данном случае нужно
представить событие $A_i$ в виде $A_i=A_i'\cap A_i''$, где
$$A_i'=\bc{\om: \enskip \hatFn(x_i;\om)\to F(x_i),n\to\infty},\quad
A_i''=\bc{\om: \enskip \hatFn(x_i-0;\om)\to
F(x_i-0),n\to\infty}.$$ Вероятность каждого из этих событий равна
$\Pf(A_i')=\Pf(A_i'')=1$, поэтому и $P(A_i)=1$; дальнейшие
рассуждения в точности такие же, как и в случае 1).
\end{proof}

%\subsection{Критерий Колмогорова}

\subsubsection{Статистика Колмогорова}

Пусть дана случайная выборка $\xi_1,\ldots,\xi_n$~--- независимые
одинаково распределенные случайные величины с непрерывной функцией
распределения $F(x)$.

\begin{df} Случайная величина $D_n=D_n(\xi_1,\ldots,\xi_n)=
\sup\limits_{x\in\R}\bm{\hatFn(x;\xi_1,\ldots,\xi_n)-F(x)}$ называется
статистикой Колмогорова.
\end{df}

В терминах статистики Колмогорова теорему Гливенко\ч Кантелли можно
переформулировать так: $D_n$ сходится к нулю при $n\to\infty$ с
вероятностью 1 (\те $\Pf$-п.н.).

Вид асимптотической функции распределения статистики $\sqrt{n}D_n$
дает следующая теорема.

\begin{theorem}[Колмогоров]
Если функция $F(x)$ непрерывна, то при любом $y>0$
$$\lim_{n\to\infty}\Pf(\sqrt{n}D_n\le y)=K(y):=\sum_{m=-\infty}^{\infty}(-1)^m e^{-2m^2 y^2}.$$
\end{theorem}

\begin{note}
Для $y\le 0$, очевидно, $\Pf(\sqrt{n}D_n\le y)=0$.
\end{note}

Участвующая в теореме функция
$K(y)=\sum\limits_{m=-\infty}^{\infty}(-1)^m e^{-2m^2 y^2}$, $y>0$
называется функцией Колмогорова.

Мы докажем только часть теоремы Колмогорова, а именно следующую
лемму:

\begin{lemma}
Распределение статистики Колмогорова $D_n(\xi_1,\ldots,\xi_n)$,
где $\xi_1,\ldots,\xi_n$~--- независимые одинаково распределенные
случайные величины с непрерывной функцией распределения $F(x)$, не
зависит от вида функции $F(x)$.
\end{lemma}

\begin{proof}
Рассмотрим два случая.

1) Пусть $y=F(x)$~--- непрерывная и строго монотонная функция.
Тогда существует обратная функция: $x=F^{-1}(y)$. Рассмотрим
случайные величины $Y_1,\ldots,Y_n$, $Y_k=F(\xi_k)$,
$k=1,\ldots,n$. Они независимы и имеют одинаковое равномерное
распределение на отрезке $[0,1]$:
$$R(y)=\Pf(Y_k\le y)=\Pf(F(\xi_k)\le y)=\Pf(\xi_k\le F^{-1}(y))=F(F^{-1}(y))=y,\quad 0<y<1.$$

Эмпирическая функция распределения $Y_1,\ldots,Y_n$:
%% HAT R_N FIXME
$$\hatRn(y;Y_1,\ldots,Y_n)=\frac{1}{n}\sum_{k=1}^n I_{\{Y_k\le y\}}=
\frac{1}{n}\sum_{k=1}^n I_{\{\xi_k\le F^{-1}(y)\}}=\hatFn%
(F^{-1}(y)),$$ где $\hatFn(x)=\hatFn%
(x;\xi_1,\ldots,\xi_n)$~--- эмпирическая функция распределения
случайной выборки $\xi_1,\ldots,\xi_n$.

Рассмотрим очевидное равенство $$\sup_{x:\enskip
0<F(x)<1}\bm{\hatFn(x)-F(x)}=\sup_{y:\enskip
0<y<1}\bm{\hatRn(y)-R(y)}.$$ Его левая часть с вероятностью~$1$
совпадает с $D_n(\xi_1,\ldots,\xi_n)$, а правая часть~--- с
$D_n(Y_1,\ldots,Y_n)$. Статистика $D_n(Y_1,\ldots,Y_n)$ от вида
функции $F(x)$ не зависит, поскольку от $F(x)$ не зависит
распределение случайных величин $Y_1,\ldots,Y_n$.

Для завершения доказательства осталось показать, что на множестве
$C=\bc{x:\, F(x)=0 \mbox{ или } F(x)=1}$ эмпирическая функция
распределения $\hatFn(x)$ и теоретическая $F(x)$ совпадают с
вероятностью 1. Для этого достаточно проверить, что
$\Pf\bbr{\sup\limits_{x\in C}\bm{\hatFn(x)-F(x)}=0}=1$. Проверку
этого факта мы предоставляем читателю.

2) Если функция $F(x)$~--- произвольная (неубывающая) непрерывная
функция, то рассуждения аналогичны предыдущему случаю, только в
этом случае нужно положить $F^{-1}(y)=\sup\bc{x:\, F(x)=y}$.
Читателю рекомендуется аккуратно провести рассуждения для этого
случая самостоятельно.
\end{proof}

\subsubsection{Критерий Колмогорова}

Рассмотрим две гипотезы о функции распределения $F(x)$: $H_0\colon
F(x)=F_0(x)$ (нулевая гипотеза), где $F_0(x)$~--- заданная
непрерывная функция распределения; $H_1\colon F(x)\neq F_0(x)$
(альтернативная гипотеза).

Статистика Колмогорова позволяет сформулировать критерий, согласно
которому выбирается одна из этих двух гипотез. А именно:

{\bf Критерий Колмогорова.} {\it Если $\sqrt{n}D_n>y_*$, то $H_0$
отклоняем ($H_1$ принимаем), если же $\sqrt{n}D_n\le y_*$, то
$H_0$ принимаем ($H_1$ отклоняем). Здесь число $y_*$ называется
критическим значением и равно $y_*=y_{1-\al}$~--- квантиль уровня
$(1-\al)$ функции Колмогорова $K(y)$ (\те решение уравнения
$K(y)=1-\al$).}

\smallskip

На практике для заданного $\al$ квантиль $y_{1-\al}$ находится по
таблице квантилей функции Колмогорова.

Действительно, по теореме Колмогорова
$\Pf_{H_0}(\sqrt{n}D_n>y_*)=1-\Pf_{H_0}(\sqrt{n}D_n\le
y_{1-\al})\xrightarrow[n\to\infty]{}
1-K(y_{1-\al})=1-(1-\al)=\al,$ \те вероятность ошибки I рода
приближенно равна $\al$ (если $n$ достаточно велико).

\subsubsection{Выборочные характеристики как характеристики эмпирической функции распределения}

Напомним некоторые определенные в п. \ref{char} выборочные
характеристики~--- выборочное среднее и выборочную дисперсию:
$$\bar x=\frac{x_1+\ldots+x_n}{n},\quad s^2=\frac{(x_1-\bar x)^2+\ldots+(x_n-\bar x)^2}{n}.$$

\begin{stm}
$\bar x$ и $s^2$~--- соответственно математическое ожидание и
дисперсия эмпирического распределения (\те распределения,
определяемого функцией распределения
$\hatFn(x;x_1,\ldots,x_n)=\hatFn(x)$).
\end{stm}

\begin{proof}
Обозначим эмпирическое распределение $\hat\xi$, $\hatFn(x)$~---
функция распределения $\hat\xi$. Тогда доказательство следует из
соотношений
$$\Mf \hat\xi=\ints{\R}x\,d\hatFn(x)=\frac{1}{n}\sumkun\ints{\R}x\,dI_{\{x\ge x_k\}}=
\frac{1}{n}\sumkun x_k=\bar x;$$
$$\Df \hat\xi=\ints{\R}(x-\bar x)^2\,d\hatFn(x)=
\frac{1}{n}\sumkun\ints{\R}(x-\bar x)^2\,dI_{\{x\ge
x_k\}}=\frac{1}{n}\sumkun (x_k-\bar x)^2=s^2.$$ Здесь мы
воспользовались определением эмпирической функции распределения,
линейностью интеграла Стилтьеса и формулой для интеграла Стилтьеса
$\ints{X}f(x)\,dg(x)=f(\xi)c$, где $g(x)$~--- функция одного
скачка (в точке $\xi$), $c=g(\xi+0)-g(\xi-0)$~--- величина скачка.
\end{proof}

Аналогично можно показать, что выборочные моменты порядка $k$
являются моментами порядка $k$ эмпирического распределения.
Покажем, что выборочные моменты можно рассматривать как хорошие
оценки моментов теоретического распределения.

Пусть $x_1\sco x_k$~--- выборка, порожденная независимыми
одинаково распределенными случайными величинами
$\xi_1,\ldots,\xi_n\sim\xi$, $F(x)$~--- их (теоретическая) функция
распределения (неизвестная, или известно в каком классе лежит, но
неизвестно какая именно). Её $k$-тый момент равен
$$\mu_k=\intl{-\infty}{+\infty}x^k\,dF(x),\quad k\in\N.$$
($\mu_1$~--- математическое ожидание, $\mu_2$~--- второй момент,
$\sigma^2=\mu_2-\mu_1^2$~--- дисперсия, и т.д.). Рассмотрим
выборочные моменты~--- моменты эмпирического распределения:
$$\hat\mu_{k,n}=\frac{1}{n}\sum_{i=1}^k x_i^k=\frac{x_1^k+\ldots+x_n^k}{n}=
\intl{-\infty}{+\infty}x^k\,d\hatFn(x).$$

Если рассматривать $\hat\mu_{k,n}$ как оценки $\mu_k$, то легко
получаем следующие её свойства:

1) Несмещённость: $\displaystyle\Mf\hat\mu_{k,n}=\Mf\hr{\frac{\xi_1^k+\ldots+\xi_n^k}{n}}=\Mf \xi^k=\mu_k$;

2) Состоятельность: по закону больших чисел
$\displaystyle\hat\mu_{k,n}\convas\Mf\hat\mu_{k,n}=\mu_k\quad\Longrightarrow
\quad\hat\mu_{k,n}\raP \mu_k$, $n\to\infty$.

\subsubsection{Распределение порядковых статистик}

Пусть $\xi_1,\ldots,\xi_n$~--- случайная выборка с теоретической
функцией распределения $F(x)$, $\xi_{(1)},\ldots,\xi_{(n)}$~--- её
порядковые статистики.

Найдем распределение $\xi_{(k)}$, $k\in\N$. Пусть
$G_{\xi_{(k)}}(x)=\Pf(\xi_{(k)}\le x)$~--- функция распределения
$\xi_{(k)}$. При каждом фиксированном $x\in\R$ имеем:
$$G_{\xi_{(k)}}(x)=\Pf\br{\xi_{(k)}\le x}=\Pf\bbr{\hat F_n(x)\ge \frac{k}{n}}=
\suml{i=k}{n}C_n^i {\br{F(x)}}^i {\br{1-F(x)}}^{n-i}.$$

\begin{ex}
Пусть $F(x)$~--- функция распределения равномерного распределения
на отрезке $[0,1]$: $F(x)=x$, $0<x<1$. Тогда
$$G_{\xi_{(k)}}(x)=\Pf\br{\xi_{(k)}\le x}=\suml{i=k}{n}C_n^i x^i{(1-x)}^{n-i}.$$

Найдем плотность этого распределения. Для этого продифференцируем
функцию распределения:
$$\br{G_{\xi_{(k)}}(x)}_x'=\suml{i=k}{n}iC_n^i x^{i-1}{(1-x)}^{n-i}-
\suml{i=k}{n-1}(n-i)C_n^i
x^i{(1-x)}^{(n-1)-i}=$$$$=\suml{i=k}{n}nC_{n-1}^{i-1}
x^{i-1}{(1-x)}^{(n-1)-(i-1)}-\suml{i=k}{n-1}nC_{n-1}^i
x^i{(1-x)}^{(n-1)-i}=$$$$=nC_{n-1}^{k-1}x^{k-1}(1-x)^{(n-1)-(k-1)}+nC_{n-1}^k
x^k(1-x)^{n-k-1}+\ldots-nC_{n-1}^k
x^k(1-x)^{n-k-1}-\ldots=nC_{n-1}^{k-1}x^{k-1}(1-x)^{n-k}.$$ (во
втором равенстве воспользовались тождествами
$iC_n^i=nC_{n-1}^{i-1}$, $(n-i)C_n^i=nC_{n-1}^i$). Таким образом,
плотность распределения $\xi_{(k)}$ равна $\bs{\Pf(\xi_{(k)}\le x
)}'=nC_{n-1}^{k-1}x^{k-1}{(1-x)}^{n-k}=\dfrac{n!}{(k-1)!(n-k)!}x^{k-1}{(1-x)}^{n-k},$
а функция распределения~---
\begin{equation} \label{ras_ps}
G_{\xi_{(k)}}(x)=\Pf(\xi_{(k)}\le
x)=\frac{n!}{(k-1)!(n-k)!}\intl{0}{x}t^{k-1}{(1-t)}^{n-k}\,dt,\quad
0<x<1.\end{equation}
\end{ex}

\begin{df} Пусть $a>0$, $b>0$. Распределение с плотностью $$p(x)=\left\{\begin{array}{cl}
  \dfrac{1}{B(a,b)}x^{a-1}{(1-x)}^{b-1}, & 0<x<1; \\
  0, & x\le 0\mbox{ или }x\ge 1,
\end{array}\right.$$
где $B(a,b)=\intl{0}{1}t^{a-1}{(1-t)}^{b-1}\,dt$~--- бета-функция
(эйлеров интеграл I рода), называется бета-распределением с
параметрами $a>0$, $b>0$. Функция распределения бета-распределения
$I_x(a,b)=\dfrac{1}{B(a,b)}\intl{0}{x}t^{a-1}{(1-t)}^{b-1}\,dt$
($0<x<1$) называется неполной бета-функцией.
\end{df}

Из математического анализа известно, что
$B(a,b)=\dfrac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$, где
$\Gamma(\lambda)=\intl{0}{\infty}x^{\lambda-1}e^{-x}\,dx$~---
гамма-функция Эйлера (эйлеров интеграл II рода), и для $n\in\N$
$\Gamma(n+1)=n!$, поэтому формулу (\ref{ras_ps}) можно переписать
в виде

$$G_{\xi_{(k)}}(x)=\Pf(\xi_{(k)}\le
x)=\frac{1}{B(k,n-k+1)}\intl{0}{x}t^{k-1}{(1-t)}^{n-k}\,dt=I_x(k,n-k+1),\quad
0<x<1.$$

Таким образом, нами доказан следующий результат.

\begin{stm}
Распределение порядковой статистики $\xi_{(k)}$, $1\le k\le n$,
для случайной выборки $\xi_1,\ldots,\xi_n$ с равномерным
распределением на отрезке $[0,1]$ является бета-распределением с
параметрами $a=k$, $b=n-k+1$.
\end{stm}

\begin{note}
Функция распределения порядковых статистик в случае произвольной
непрерывной функции распределения $F(x)$ случайной выборки
$\xi_1,\ldots,\xi_n$ имеет вид $\Pf(\xi_{(k)}\le
x)=I_{F(x)}(k,n-k+1)$.
\end{note}

\subsection{Функция правдоподобия. Регулярные модели}

Пусть в некоторой статистической модели
$(X,\As_X,\Ps=\{\Pf_\theta,\theta\in\Theta\})$ имеется выборка
$x=(x_1,\ldots,x_n)$, порожденная случайной выборкой
$\xi_1,\ldots,\xi_n\sim\xi(\theta)$, где случайные величины
$\xi_1,\ldots,\xi_n$ независимы и одинаково распределены с
функцией распределения $F_\theta(x)$, $\theta\in\Theta$~---
параметр распределения.

Оценка параметра $\theta$~--- это подходящая статистика (измеримая
функция от выборочных данных): $\hattheta=\hattheta(x)$. Пусть
имеются две оценки параметра $\theta$. Определим, что значит, что
одна оценка \лк лучше\пк другой.

\begin{df}
Пусть $\hattheta_1$ и $\hattheta_2$~--- две оценки параметра
$\theta$. Говорят, что оценка $\hattheta_1$ лучше (или
предпочтительней) оценки $\hattheta_2$, если
$$\Mf_\theta{(\hattheta_1-\theta)}^2\le \Mf_\theta{(\hattheta_2-\theta)}^2\quad
\fa\theta\in\Theta,$$
$$\mbox{ и }\exi\theta_0\cln\quad\Mf_{\theta_0}{(\hattheta_1-\theta_0)}^2<
\Mf_{\theta_0}{(\hattheta_2-\theta_0)}^2.$$
\end{df}

\begin{df} Эффективной оценкой параметра $\theta$ называется несмещенная
оценка с минимальной дисперсией, \те такая оценка $\hattheta^*$,
для которой выполнены следующие свойства:

1) $\Mf_\theta\hattheta^*=\theta$ $\fa\theta\in\Theta$;

2)
$\Mf_\theta{(\hattheta^*-\theta)}^2=\min\limits_{\hattheta\cln\Mf_\theta\hattheta=\theta}
\Mf_\theta{(\hattheta-\theta)}^2.$

Напомним, что для несмещенных оценок среднеквадратичное отклонение совпадает с дисперсией:
$$\Mf_\theta{(\hattheta-\theta)}^2=\Mf_\theta{(\hattheta-\Mf\hattheta)}^2=\Df\hattheta.$$
\end{df}
Таким образом, эффективная оценка~--- это наилучшая из всех
несмещенных оценок. Аналогично можно определить эффективную оценку
в классе оценок с заданным смещением $\Delta$:
$\Mf_\theta\hattheta_1=\Mf_\theta\hattheta_2=\Delta$.

Нас будут интересовать два случая: распределение $\xi$ дискретно
(с распределением вероятностей $\Pf_\theta$) или абсолютно
непрерывно (с плотностью $p(\theta;x)$). Чтобы в дальнейшем не
рассматривать эти случаи отдельно, введем следующее удобное
обозначение:
$$f(\theta;x)=\left\{\begin{array}{cl}
  \Pf_\theta(\xi=x), & \mbox{ если модель дискретна; } \\
  p(\theta;x), & \mbox{ если модель абсолютно непрерывна. }
\end{array}\right.$$

В дальнейшем придется интегрировать по выборочному пространству,
поэтому отметим, что если модель дискретна, то интегрирование
заменяется суммированием (для краткости, мы будем проводить все
выкладки для абсолютно непрерывной модели).

\begin{df}
Функцией правдоподобия (для данной выборки $x_1,\ldots,x_n$)
называется следующая функция (параметра $\theta$):
$$L(\theta;x_1,\ldots,x_n):=\prod_{i=1}^n f(\theta;x_i),\quad \theta\in\Theta.$$
\end{df}

В дальнейшем для краткости мы будем писать
$L(\theta;x)=L(\theta;x_1,\ldots,x_n)$, $x=(x_1,\ldots,x_n)$.

\begin{df}
Статистическая модель называется регулярной (по Рао--Крамеру),
если выполнены следующие условия (регулярности):

\quad 1) $L(\theta;x)>0$ и дифференцируема по $\theta$
$\fa\theta\in\Theta$ и $\fa x\in X$;

\quad 2) Случайная величина $U(\theta;x)=\dfrac{\partial\ln
L(\theta;x)}{\partial\theta}=\sumiun\dfrac{\partial\ln
f(\theta;x_i)}{\partial\theta}$ (которая называется функцией
вклада выборки) имеет ограниченную дисперсию:
$$0<\Mf^2_\theta U(\theta;x)<\infty,\quad \fa\theta\in\Theta.$$

\quad 3) Для любой статистики $\hattheta=\hattheta(x)$ имеет
место равенство
$$\frac{\partial}{\partial\theta}\ints{X} \hattheta(x) L(\theta;x)\,dx=
\ints{X} \hattheta(x) \frac{\partial
L(\theta;x)}{\partial\theta}\,dx$$ (Это означает, что выборочное
пространство $X$ не зависит от параметра $\theta$).
\end{df}

\begin{ex} (нерегулярной модели).
Рассмотрим модель $R(0,\theta)$ (равномерное распределение на
отрезке $(0,\theta)$, $\theta>0$). Условие 3) регулярности для
этой модели не выполнено:
$$\frac{\partial}{\partial\theta}\bbr{\intl{0}{\theta}\frac{1}{\theta}\,dx}=
\frac{\partial}{\partial\theta}\bbr{\frac{1}{\theta}\theta}=\frac{\partial}{\partial\theta}(1)=0,\quad\mbox{
но
}\quad\intl{0}{\theta}\frac{\partial}{\partial\theta}\bbr{\frac{1}{\theta}}\,dx=
-\intl{0}{\theta}\frac{1}{\theta^2}dx=-\frac{1}{\theta}.$$ Таким
образом, эта модель не является регулярной.

\end{ex}

\begin{problem}
Проверить условие регулярности 3) для экспоненциального
распределения с плотностью
$$p_\theta(x)=\left\{
\begin{array}{cl}
  e^{-(x-\theta)}, & x\ge0; \\
  0, & x<0.
\end{array}\right.$$
\end{problem}

\subsection{Количество информации Фишера. Неравенство Рао\ч Крамера}

\subsubsection{Информация Фишера}

Пусть модель регулярна. Рассмотрим тождество
$\ints{X}L(\theta;x)\,dx\equiv1$ (оно выполнено, так как
$L(\theta;x)$~--- плотность распределения случайного вектора
$\xi_1,\ldots,\xi_n$). Продифференцируем его по $\theta$:
$$\frac{\partial}{\partial\theta}\ints{X}L(\theta;x)\,dx=
\ints{X} \frac{\partial\ln
L(\theta;x)}{\partial\theta}L(\theta;x)\,dx=0.$$

Отсюда следует, что математическое ожидание функции вклада равно
0: $\Mf_\theta U(\theta;x)=\Mf_\theta \frac{\partial \ln
L(\theta;x) }{\partial\theta}=0.$

\begin{df}
Количеством информации Фишера (или просто информацией Фишера)
называется дисперсия функции вклада:
$$I_n(\theta):=\Df_\theta U(\theta;x)=\Mf_\theta U^2(\theta;x)=
\ints{X}{\bbr{\frac{\partial \ln L(\theta;x)}{\partial\theta}}}^2
L(\theta;x)\,dx.$$

Количество информации для одного наблюдения равно $I_1(\theta)=
\ints{X}{\br{\frac{\partial \ln f(\theta;x)}{\partial\theta}}}^2
f(\theta;x)\,dx$, а поскольку наблюдения (случайные величины)
$\xi_1,\ldots,\xi_n$ независимы, то информация Фишера о выборке
размера $n$ равна $I_n(\theta)=n I_1(\theta)$.
\end{df}

\subsubsection{Неравенство Рао\ч Крамера}

Пусть имеется регулярная модель с параметрическим семейством распределений
${\Ps=\{\Pf_\theta,\enskip\theta\in\Theta\}}$. Следующая теорема
дает нижнюю границу дисперсий оценок произвольной дифференцируемой
функции от параметра~$\theta$ в классе оценок с заданным
смещением.

\begin{theorem}[Неравенство Рао\ч Крам\'ера\footnote{а не Кр\'амера!}]
Пусть модель с параметрическим семейством распределений $\Ps$
регулярна, $x=(x_1,\ldots,x_n)$~--- выборка, и пусть некоторая
статистика $\hattheta(x)$ оценивает дифференцируемую функцию
$\tau(\theta)$ параметра $\theta$. Обозначим $b(\theta)=\Mf_\theta
\hattheta(x)-\tau(\theta)$~--- смещение оценки $\hattheta(x)$.
Если $b(\theta)$~--- дифференцируемая функция, то справедливо
неравенство
$$\Df_\theta\br{\hattheta(x)-\tau(\theta)-b(\theta)}\ge
\frac{[\tau'(\theta)+b'(\theta)]^2}{I_n(\theta)},$$ где
$I_n(\theta)$~--- количество информации Фишера о выборке $x$. При
этом неравенство обращается в равенство тогда и только тогда,
когда оценка $\hattheta(x)$ является линейной функцией вклада
выборки, \те
$\hattheta(x)-\tau(\theta)-b(\theta)=a(\theta)U(\theta;x)$.
\end{theorem}

В частности, если оценка $\hattheta(x)$ несмещенная,
$\Mf_\theta\hattheta(x)=\tau(\theta)$, то $b(\theta)=0$, и с
учетом $I_n(\theta)=nI_1(\theta)$ неравенство принимает вид
$\Df_\theta\hattheta(x)\ge\dfrac{\br{\tau'(\theta)}^2}{nI_1(\theta)}$.

\begin{proof}
По определению смещения оценки $\hattheta(x)$ имеем
$\tau(\theta)+b(\theta)=\Mf_\theta\hattheta(x)$.
Продифференцируем это равенство, записав $\Mf_\theta\hattheta(x)$
в виде интеграла и пользуясь условием регулярности 3):
$$\tau'(\theta)+b'(\theta)=\frac{\partial}{\partial\theta}\ints{X} \hattheta(x) L(\theta;x)\,dx=
\ints{X} \hattheta(x) \frac{\partial \ln
L(\theta;x)}{\partial\theta}L(\theta;x)\,dx=\ints{X} \hattheta(x)
U(\theta;x)L(\theta;x)\,dx=\Mf_\theta\br{\hattheta(x)U(\theta;x)}.$$

Учитывая, что $\Mf_\theta U(\theta;x)=0$, получаем:
$$\tau'(\theta)+b'(\theta)=\Mf_\theta\br{\hattheta(x)U(\theta;x)}=
\Mf_\theta\bs{\br{\hattheta(x)-\tau(\theta)-b(\theta)+\tau(\theta)+b(\theta)}U(\theta;x)}=
\Mf_\theta\bs{\br{\hattheta(x)-\tau(\theta)-b(\theta)}U(\theta;x)}+$$$$+\br{\tau(\theta)+b(\theta)}
\Mf_\theta
U(\theta;x)=\Mf_\theta\bbs{\br{\hattheta(x)-\Mf_\theta\hattheta(x)}
\br{U(\theta;x)-\Mf_\theta U(\theta;x)}}
=\cov_\theta(\hattheta(x),U(\theta;x)).$$

Применим к $\cov_\theta(\hattheta(x),U(\theta;x))$ неравенство
Коши\ч Буняковского $(\cov(\xi,\eta))^2\le \Df\xi\cdot\Df\eta$:

$$\br{\tau'(\theta)+b'(\theta)}^2=\br{\cov_\theta(\hattheta(x),U(\theta;x))}^2\le
\Df_\theta\hattheta(x)\Df_\theta
U(\theta;x)=\Df_\theta\br{\hattheta(x)-\tau(\theta)-b(\theta)}\cdot
I_n(x).$$

Отсюда следует требуемое неравенство. А так как неравенство
Коши\ч Буняковского превращается в равенство тогда и только тогда,
когда функции (в нашем случае случайные величины) линейно связаны,
то и наше неравенство превращается в равенство в том и только том
случае, когда (при каждом $\theta$) $\hattheta(x)$ и
$U(\theta;x)$ линейно связаны:
$\hattheta(x)-\tau(\theta)-b(\theta)=a(\theta)U(\theta;x)$.
Теорема доказана. \end{proof}

%%% NEW INFORMATION

\subsubsection{Семейства распределений экспоненциального типа}
Если мы будем рассматривать несмещённые оценки параметра $\theta$ ($b(\theta) = 0$, $\tau(\theta) =
\theta$), то равенство в теореме Крамера\ч Рао ($\Df \hattheta = \frac 1
I_n(\theta)$) достигается при
$$
\frac{\partial \ln L(\theta; x_1, \dots, x_n)}{\partial\theta} = K(\theta)(\hattheta - \theta),
$$
где $K$ не зависит от выборки (это условие пропорциональности, при котором неравенство
Коши\ч Буняковского обращается в равенство). Отсюда
$$
L(\theta; x_1, \dots, x_n) = C(x_1, \dots, x_n) e^{a_1(\theta) \hattheta(x_1, \dots, x_n) +
a_2(\theta)}.
$$
Мы получили явный вид функции правдоподобия. Это плотность параметрического семейства распределений,
принадлежащих к так называемому \emph{экспоненциальному типу}. Примерами таких распределений служат
биномиальное, показательное, нормальное и другие.

\subsubsection{Многомерный случай}
Пусть $\theta = (\theta_1, \dots, \theta_s)^T$~---
вектор-параметр, $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$.
Рассматривается оценка $\hattheta = (\hattheta_1, \dots,
\hattheta_s)$, $\hattheta_k = \hattheta_k(x_1, \dots, x_n)$.
Положим
$$
\lambda_j = \frac{\partial\ln L(\theta; x_1, \dots, x_n)}{\partial\theta_j},
\qquad
\Lambda = (\lambda_1, \dots, \lambda_s)^T.
$$
В многомерном случае роль дисперсии играет \emph{ковариационная матрица}
$\Sigma = \Sigma_{\hattheta(\xi)} = \Mf_\theta \bigl( (\hattheta(\xi) -
\Mf\hattheta(\xi))(\hattheta(\xi) - \Mf\hattheta(\xi))^T \bigr)$;
$\Sigma = (\sigma_{ij})$, $\sigma_{ij} = \Mf \bigl( (\hattheta_i - \Mf\hattheta_i)
(\hattheta_j - \Mf\hattheta_j) \bigr) = \cov (\hattheta_i, \hattheta_j )$;
$\sigma_{ii} = \sigma_i^2$ --- дисперсия, остальные --- попарные
ковариации. Несмещённость оценки задаётся равенством $\Mf_\theta \hattheta(\xi) = \theta$.

Аналогом количества информации Фишера является \emph{информационная матрица Фишера}
$J(\theta) = \Mf_\theta(\Lambda \Lambda^T)$. Предположим, что эта матрица обратима
(существует $J^{-1}(\theta)$). Тогда имеет место аналог теоремы Крамера\ч Рао:
матрица $\Sigma_{\hattheta(\xi)} - J^{-1}(\theta)$ является неотрицательно
определённой.

% FIXME: problems

\subsection{Методы получения оценок}
\subsubsection{Метод моментов}
Рассматривается статистическая модель с $s$-мерным параметром $\theta = (\theta_1, \dots, \theta_s)$.
$m_k(\theta) = \Mf \xi_i^k$ ($i = 1,\dots,n$, выборка повторная) --- $k$-й момент (истинный);
$\widehat{m}_k(\theta) = \frac{\xi_1^k + \dots + \xi_n^k}{n}$ --- эмпирический момент. Предположим, что
$\Mf \xi_i^s = m_s(\theta) < \infty$. Эмпирические моменты являются оценками для истинных. Запишем
систему \emph{моментных уравнений}:
$$
\begin{cases}
m_k(\theta_1, \dots, \theta_s) = \widehat{m}^k\\
1\leqslant k\leqslant s
\end{cases}
%\qquad \text{(\textasteriskcentered)}
$$
Рассмотрим полученную систему относительно переменных $\theta_1, \dots, \theta_s$. Пусть существует
единственное решение ${\hattheta_j} \bw= f_j(\widehat{m}_1, \dots, \widehat{m}_s)$
($1\leqslant j\leqslant s$).
Мы получили некоторую оценку для $\theta$. Следующая теорема сообщает, что оценка не совсем
плохая.

\begin{theorem}[О~состоятельности статистических оценок, полученных методом моментов]
Пусть ${\hattheta}_1, \dots, {\hattheta}_s$ есть решение системы
моментных уравнений и пусть функции $f_j$ непрерывны. Тогда оценки
${\hattheta}_j = f_j(\widehat{m}_1, \dots, \widehat{m}_s)$ для
всех $j$ являются состоятельными оценками параметров $\theta_j$
(т.е. $\hattheta_j \raPnrai \theta_j$ для всех $j$).
\end{theorem}

\begin{proof}
Это следует из непрерывности $f_j$ и асимптотического свойства моментов: $\widehat{m}_k(n)
\raPnrai m_k(\theta)$.
\end{proof}

\begin{ex}
Схема Бернулли. $p = \Mf \xi_i$ --- параметр (вероятность удачи);
$\xi_i$~--- 1 или 0. $\overline{x} = \frac 1n (x_1 + \dots +
x_n)$~--- первый выборочный момент. $\widehat{p} = \overline{x}$
--- оценка $p$ по методу моментов. Это хорошая оценка
(несмещённая, состоятельная, эффективная в смысле неравенства
Крамера\ч Рао).
\end{ex}

\begin{ex}
$\xi_1, \dots, \xi_n \sim \Norm(a, \sigma^2)$, $\Mf \xi_i = a$,
$\Df \xi_i = \sigma^2$. $\overline{x} = \widehat{a}$~--- хорошая
оценка. $\widehat{\sigma^2} = s^2 = \frac 1n ((x_1 -
\overline{x})^2 + \dots + (x_n - \overline{x})^2)$~--- выборочная
дисперсия\т смещённая оценка $\sigma^2$. Это оценки, полученные
по методу моментов. А вот такая оценка:
$\widehat{\widehat{\sigma^2}} = s^2 \frac{n}{n-1}$ является
несмещённой.
\end{ex}

\subsubsection{Асимптотические свойства оценок}
\begin{nums}{-2}
\item \emph{Состоятельность оценки}. $\{ \Pf_\theta \mid \theta \in \Theta \}$~--- параметрическая
модель, ${\hattheta}_n$~--- оценка по выборке длины $n$,
$\theta_0$~--- истинное значение параметра. Оценка состоятельна,
если ${\hattheta}_n \raPnrai \theta_0$.

\item \emph{Асимптотическая несмещённость}. $\Mf_\theta {\hattheta}_n \ranrai \theta$
(\те смещение $b_n(\theta) = \Mf_\theta {\hattheta}_n  - \theta
\ranrai 0$).

\item \emph{Асимптотическая нормальность}. ${\hattheta}_n$ асимптотически нормальна, если
существует монотонно сходящаяся к нулю последовательность
положительных чисел $\left\{ c_n \right\}_{n=1}^{\infty}$ такая, что
$$
\frac{{\hattheta}_n - \theta}{c_n} \xrightarrow{\mathrm{d}} Z \sim \Norm(0,1)
$$
(сходимость по распределению к некоторой случайной величине со стандартным нормальным
распределением). Говорят, что $\frac{\hattheta_n - \theta}{c_n}$ асимптотически
нормальна с $\Norm(0,1)$, а $\hattheta_n$ асимптотически нормальна с $\Norm(\theta,c_n^2)$;
$\theta$ называется асимптотическим средним, а $c_n^2$ --- асимптотической дисперсией оценки
$\hattheta_n$. Асимптотически нормальная оценка автоматически является асимптотически
несмещённой. В качестве $c_n$ обычно берут $c_n^2 = \frac{\sigma^2(\theta)}{n}$ ($\sigma^2$
не зависит от выборки).
\begin{ex}
Схема Бернулли с параметром $p$ (вероятность успеха).
$\xi_1, \dots, \xi_n$ --- выборка.
$$
\widehat{p} = \frac{\xi_1 + \dots + \xi_n} n,
\qquad \Mf \widehat{p} = p, \qquad \Df \widehat{p} =
\frac{p(1-p)}{n} = \frac{\sigma^2(p)}{n}.
$$
$\widehat{p}$ асимптотически нормальна в силу теоремы Муавра --
Лапласа (ЦПТ). (Обычно через эту теорему асимптотическую
нормальность и доказывают.)
\end{ex}

\begin{ex}
$\xi_1, \dots, \xi_n$, $\xi_i \sim \Norm(a, \sigma^2)$.
$\Df \overline{\xi} = \frac{\sigma^2}{n}$.
\end{ex}

\item \emph{Асимптотическая эффективность}. Рассмотрим семейство распределений, подчинённых
условиям регулярности, для оценок параметров которого имеет место
неравенство Крамера\ч Рао. $1/{I_n(\theta)}$~--- нижняя граница
дисперсий всех оценок. Коэффициент эффективности оценки: $
e_n(\hattheta_n) = \frac{1/{I_n(\theta)}}{\Df\hattheta_n}. $ $0 <
e_n(\hattheta_n) \leqslant 1$. Если $e_n(\hattheta_n) = 1$, то
оценка называется эффективной. $e_\infty(\hattheta_n) =
\lim\limits_{\nrai} e_n(\hattheta_n)$. Если $e_\infty(\hattheta_n)
= 1$, то оценка называется асимптотически эффективной.
\par\emph{Асимптотическая эффективность в рамках асимптотической нормальности.}
Пусть $\hattheta_n$ асимптотически нормальна с $\Norm(\theta,
\frac{\sigma^2(\theta)}{n})$. Она
 называется асимптотически эффективной (в рамках
асимптотической нормальности), если
$$
\frac{1/I_n(\theta)}{\sigma^2(\theta)/n} \ranrai 1.
$$
\end{nums}

\subsubsection{Метод максимального (наибольшего) правдоподобия}
\paragraph{Принцип МП в простейшем случае}
Принцип МП был рассмотрен ещё Гауссом в следующей форме: найдём такие значения параметров, чтобы вероятность
получить данную выборку была максимальной (берём $\max\limits_{\theta\in\Theta} \Pf_\theta(\xi_1 = x_1,
\dots, \xi_n = x_n) = \Pf_{\theta^{*}}(\xi_1 = x_1, \dots, \xi_n = x_n)$). $\theta^{*}$ --- оценка МП
(наиболее правдоподобное значение $\theta$).

\paragraph{Общая ситуация}
$x_1, \dots, x_n$ --- выборка в $(\Xs\!\!, \As, \Ps)$, $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$.
$L(\theta; x_1, \dots, x_n)$ --- функция правдоподобия. Возьмём $\theta^{*}$ такое, что
$$
\sup\limits_{\theta\in\Theta} L(\theta; x_1,\dots,x_n) = L(\theta^{*}; x_1, \dots, x_n)
$$
(если точная верхняя грань достигается). $\theta^{*}$ называется \emph{оценкой максимального правдоподобия
(ОМП)}.

\begin{ex}
$\xi_1, \dots, \xi_n$ взаимно независимы и равномерно распределены на
$[0, \theta]$; $\theta \in \Theta = (0, +\infty)$.\\
$L(\theta; x_1, \dots, x_n) = f_\theta(x_1)\cdot \dots\cdot f_\theta(x_n)$,
где $f_\theta(x) = \frac 1\theta \mathrm{I}_{[0, \theta]} (x)$ --- плотность
равномерного распределения. $L \ne 0$ тогда и только тогда, когда
для всех $i$ $x_i \in [0, \theta]$.
$$
L = \begin{cases}
\frac 1 {\theta^n} & x_{(n)} \leqslant \theta \\
0 & \text{в ином случае}
\end{cases}
$$
Ясно, что максимум $L$ достигается при $\theta = \theta^* = x_{(n)} =
\max\limits_i x_i$. $\theta^*$ --- ОМП.
\end{ex}

Предположим, что функция $L(\theta; x_1, \dots, x_n)$ дифференцируема по параметру $\theta$
($\theta\in\Theta\subset\mathbb{R}$). Тогда ОМП
можно найти, решая \emph{уравнение правдоподобия}:
$
\frac{\partial L(\theta)}{\partial \theta} = 0
$
(или переходим к логарифмам: $\frac{\partial \ln L}{\partial \theta} = 0$, если это возможно).
В случае $\Theta\subset\mathbb{R}^k$ получаем систему уравнений.

\begin{theorem}[о свойствах ОМП --- теорема Дюге] При выполнении условий регулярности
{\rm (\textasteriskcentered)}
ОМП обладает следующими асимптотическими свойствами:
\begin{nums}{-4}
\item состоятельность;
\item асимптотическая нормальность и асимптотическая эффективность.
\end{nums}
\end{theorem}

{\bf Условия регулярности} (\textasteriskcentered):
\begin{nums}{-2}
\item $\Theta$ --- невырожденный замкнутый интервал на $\mathbb R$; существуют
$\frac{\partial \ln f}{\partial \theta}$, $\frac{\partial^2 \ln f}{\partial \theta^2}$,
$\frac{\partial^3 \ln f}{\partial \theta^3}$ для $\theta\in\Theta$.

\item Для всех $\theta\in\Theta$:
$$
\left| \frac{\partial \ln f}{\partial \theta} \right| \leqslant g_1(x), \qquad
\left| \frac{\partial^2 \ln f}{\partial \theta^2} \right| \leqslant g_2(x), \qquad
\left| \frac{\partial^3 \ln f}{\partial \theta^3} \right| \leqslant H(x),
$$
где $g_1$ и $g_2$ интегрируемы на $\mathbb R$, а $H$ обладает следующим свойством:
$$
\Mf_\theta H = \int\limits_{-\infty}^{+\infty} H(x) f_\theta(x) \, dx < M
$$
($M$ не зависит от $\theta$).

\item
$$
0 < I_1(\theta) = \int\limits_{-\infty}^{+\infty}
\left(\frac{\partial \ln f}{\partial \theta}\right)^2
f_\theta(x) \, dx < \infty.
$$
\end{nums}
Эти условия содержат условия Крамера\ч Рао.

\begin{lemma}\label{L:OMP1}
Пусть
$$
B_0 = \frac 1n \sum_{i=1}^{n} \left.\left(\frac{\partial\ln f_\theta(\xi_i)}{\partial\theta}%
\right)\right|_{\theta=\theta_0} \qquad
B_1 = \frac 1n \sum_{i=1}^{n} \left.\left(\frac{\partial^2\ln f_\theta(\xi_i)}{\partial\theta^2}%
\right)\right|_{\theta=\theta_0} \qquad
B_2 = \frac 1n \sum_{i=1}^{n} H(\xi_i)
$$
(все $B_j$ являются функциями от $\xi_1,\dots,\xi_n$). Тогда $B_0 \raP \Mf B_0 = 0$;
$B_1 \raP \Mf B_1 = -k^2$, где $k^2 = I_1(\theta)$; $B_2 \raP \Mf H(\xi_1) < M$ (при
$n\to\infty$).
\end{lemma}

\begin{proof}
Предельные свойства $B_1$, $B_2$ и $B_3$ следуют из ЗБЧ ($B_j$ являются средними арифметическими
независимых одинаково распределённых случайных величин с конечными математическими ожиданиями).

В условиях регулярности меняем дифференцирование по $\theta$ и интегрирование:
$$\Mf \frac{\partial\ln f_\theta(\xi)}
{\partial\theta} =
\iii \frac{\partial\ln f_\theta(x)}
{\partial\theta} f_\theta(x) \, dx =
\iii \frac{1}{f_\theta(x)} \cdot
\frac{\partial f_\theta(x)}{\partial\theta} \cdot f_\theta(x) \, dx =
\frac\partial{\partial\theta} \underbrace{\iii
f_\theta(x) \, dx}_{= 1} = 0
$$

Отсюда $\Mf B_0 = \frac 1n \sum\limits_{i=1}^{n}
 \Mf \frac{\partial\ln f_\theta(\xi_i)}
{\partial\theta} = 0$.

%\frac{\partial}{\partial\theta} \Mf\left(\ln f_\theta(\xi)\right) =
%\frac{\partial}{\partial\theta} \int\limits_{-\infty}^{+\infty}
%(\ln f_\theta(x)) \cdot f_\theta(x) \, dx =
%\int\limits_{-\infty}^{+\infty} \ln f_\theta(\xi)
%\frac{\partial f_\theta(\xi)}{\partial\theta} \, dx
%$$

%$\Mf \frac{\partial\ln f_\theta(\xi_i)}{\partial\theta} = 0$. Меняем местами дифференцирование по
%$\theta$ и интегрирование (взятие математического ожидания) в условиях регулярности и получаем, что
%$\Mf B_0 = 0$.

Утверждение $\Mf B_1 = -k^2$ вытекает из следующей выкладки\footnote%
{Лектор на неё забил, но я всё же приведу её. --- \emph{примеч. С.\,К.}}:

\begin{multline*}
\Mf \frac{\partial^2 \ln f_\theta(\xi)}{\partial\theta^2} =
\iii \frac\partial{\partial\theta}
\frac{\partial \ln f_\theta(x)}{\partial\theta}
f_\theta(x) \, dx =
\iii \frac\partial{\partial\theta}
\left(\frac 1 {f_\theta(x)} \cdot \frac{\partial f_\theta(x)}{\partial\theta}
\right) f_\theta(x) \, dx ={}\\=
\iii \frac{-1}{f^2_\theta(x)} \cdot \frac{\partial f_\theta(x)}{\partial\theta}
\cdot \frac{\partial f_\theta(x)}{\partial\theta} f_\theta(x) \, dx +
\iii \frac{\partial^2 f_\theta(x)}{\partial\theta^2} \cdot \frac 1
{f_\theta(x)} f_\theta(x) \, dx =
- \iii \left( \frac{\partial f_\theta(x)}{\partial\theta} \right)^2
\frac 1 {f_\theta(x)} \, dx +{}\\+
\underbrace{\frac{\partial^2}{\partial\theta^2}
\iii f_\theta(x) \, dx}_{=0} =
- \iii \left( \frac{\partial \ln f_\theta(x)}{\partial\theta} f_\theta(x)
\right)^2 \frac 1 {f_\theta(x)} \, dx =
- \iii \left( \frac{\partial \ln f_\theta(x)}{\partial\theta} \right)^2
f_\theta(x) \, dx ={}\\=
- \Mf \left(\frac{\partial\ln f_\theta(\xi)}{\partial\theta}
\right)^2 = - I_1(\theta) = -k^2.
\end{multline*}


%$$
%\Mf B_1 = \Mf \frac{\partial^2 \ln f_\theta(\xi_i)}{\partial\theta^2} = \{ \mbox{соотн. между
%вторым моментом и дисперсией} \} = - \Mf \left(\frac{\partial \ln f_\theta(\xi_i)}{\partial\theta}
%\right)^2 = -I_1(\theta) = -k^2.
%$$
\end{proof}

\begin{lemma}\label{L:OMP2}
\begin{nums}{-4}
\item $X_n \radnrai X$, $Y_n \raPnrai 0$ $\Rightarrow$ $X_n + Y_n \radnrai X$.
\item $X_n \radnrai X$, $Z_n \raPnrai 1$ $\Rightarrow$ $X_n Z_n \radnrai X$, $X_n/Z_n \radnrai X$.
\end{nums}
\end{lemma}
(Это --- задача по теории вероятностей)

\begin{proof}[\emph{Доказательство теоремы Дюге}]
$$
\frac{\partial \ln f_\theta(x)}{\partial\theta} = \{ \mbox{разложение Тейлора} \} =
\left. \frac{\partial \ln f_\theta}{\partial\theta} \right|_{\theta=\theta_0} +
(\theta-\theta_0) \left. \frac{\partial^2 \ln f_\theta}{\partial \theta^2}\right|_{\theta=\theta_0} +
\frac 12 \tau (\theta-\theta_0)^2 H(x)
$$
($x\in\mathbb R$, $|\tau| < 1$). Вспоминаем, что $L(\theta; x_1, \dots, x_n) = f_\theta(x_1)\cdot\dots
\cdot f_\theta(x_n)$, и переписываем уравнение правдоподобия
$$
\frac 1n \frac{\partial \ln L(\theta; x_1, \dots, x_n)}{\partial \theta} = 0
$$
в виде:
$$
\mbox{Ы}(\theta) = B_0 + (\theta-\theta_0) B_1 + \frac 12 \tau (\theta-\theta_0)^2 B_2 = 0.
$$
Зададим $\varepsilon>0$, $\delta > 0$ --- малые числа. Выберем $n > n_0(\delta, \varepsilon)$ так, чтобы
$$ \Pf\left( |B_0| \geqslant \delta^2 \right) < \frac \varepsilon 3 \qquad
\Pf \left( B_1 \geqslant -\frac{k^2}2 \right) < \frac \varepsilon 3 \qquad
\Pf(|B_2| \geqslant 2M) < \frac \varepsilon 3.
$$
Положим $S = \{ x \mid |B_0| < \delta^2, B_1 < -\frac 12 k^2, |B_2| < 2M \}$. При $n > n_0$ $\Pf(S) > 1 -
\varepsilon$. Пусть $x \in S$. Положим $\theta = \theta_0 \pm \delta$.
Так как $|B_0 + \frac 12 \tau \delta^2 B_2| \leqslant \delta^2 (1+M)$, при малых $\delta$ знак
выражения $\mbox{Ы}(\theta) = B_0 \mp \delta B_1 + \frac 12 \tau \delta^2 B_2$ определяется
вторым слагаемым. Поэтому (при малых $\delta$) если $\theta = \theta_0 - \delta$, то
$\frac{\partial \ln L}{\partial \theta} > 0$, а если $\theta = \theta_0 + \delta$, то
$\frac{\partial \ln L}{\partial \theta} < 0$. Отсюда по непрерывности $\exists \theta^{*}\in
(\theta_0 - \delta, \theta_0 + \delta)$ т.,ч. $\left.\frac{\partial \ln L}{\partial \theta}\right|_%
{\theta = \theta^{*}} = 0$, \те $\theta^{*}$ --- точка максимума.
Итак, с вероятностью, не меньшей, чем $1-\varepsilon$, точка
максимума лежит в $(\theta_0 - \delta, \theta_0 + \delta)$, откуда
следует состоятельность.

Докажем асимптотическую нормальность и асимптотическую эффективность. Имеем $\mbox{Ы}(\theta^{*}) = 0$.
Отсюда
$$
\theta^{*} - \theta_0 = \frac{B_0}{-B_1 - \frac 12 \tau (\theta^{*} - \theta_0)B_2}.
$$
Далее ($k^2 = I_1(\theta)$),
$$
\frac{\theta^{*} - \theta_0}{\frac{1}{\sqrt{nk^2}}} = \frac{\frac{1}{\sqrt{nk^2}}
\sum\limits_{i=1}^{n} \frac{\partial \ln f(\xi_i)}{\partial\theta}}%
{-\frac{B_1}{k^2} + \frac 12 \tau \frac{\theta^{*} - \theta_0}{k^2}B_2}.
$$
$Y_i = \frac{\partial \ln f(\xi_i)}{\partial\theta}$
 --- независимые и одинаково распределённые с $\Mf Y_i = 0$ и $\Df Y_i = \Mf Y_i^2 = k^2$. В силу
ЦПТ числитель
$$
\frac 1 {\sqrt{nk^2}} \sum \frac{\partial \ln f(\xi_i)}{\partial \theta} \radnrai
Z \sim \Norm(0,1).
$$
%Очевидно, что знаменатель $\raPnrai 1$.
В силу леммы \ref{L:OMP1} $B_1 \raP -k^2$, $B_2 \raP \Mf H(\xi_1) < M$, откуда
знаменатель
$$
-\frac{B_1}{k^2} + \frac 12 \tau \frac{\theta^{*} - \theta_0}{k^2} B_2
\raP 1
$$
(второе слагаемое оценивается через $\frac 12 \frac M {k^2}
(\theta^* - \theta_0)$, а $\theta^* - \theta_0 < \delta \to 0$).
Тогда (в силу леммы \ref{L:OMP2}) $\sqrt{nk^2}(\theta^{*} -
\theta_0)$ асимптотически нормальна с $\Norm(0,1)$. Отсюда
$\theta^{*}$ асимптотически нормальна с $\Norm(\theta_0,
\frac{1}{nk^2})$, а т.\,к. асимптотическая дисперсия $nk^2 =
I_n(\theta)$, то $\theta^{*}$ асимптотически эффективна (в рамках
асимптотической нормальности).
\end{proof}

% FIXME: probl. 1

\begin{problem}
Пусть $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$ --- регулярное параметрическое семейство
распределений. Докажите, что если существует эффективная (в смысле неравенства Крамера\ч Рао)
оценка $\hattheta$, то $\hattheta$ есть ОМП.
\end{problem}

\subsection{О свойствах информации Фишера}
$\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$, $x_1, \dots, x_s$ --- выборка.
$L(\theta; x_1, \dots, x_n)$ --- функция правдоподобия.
Информация Фишера:
$$
I_n(\theta) = \Mf_\theta \left( \frac{\partial\ln L(\theta; \xi_1, \dots, \xi_n)}{\partial\theta} \right)^2
=: I^\xi(\theta).
$$
$I^{\xi_1}(\theta) = I_1(\theta)$ ($I^\xi(\theta)$ --- информация в выборке $\xi$ --- это просто такое
обозначение). Для информации Фишера выполняются все обычные свойства информации:

\begin{stm}[свойства информации Фишера]
\begin{nums}{-2}
\item $I^\xi(\theta) = I^{\xi_1}(\theta) + \dots + I^{\xi_n}(\theta)$, если $\xi_1, \dots, \xi_n$
независимы.
\item Для повторной выборки $I^\xi(\theta) = n I^{\xi_1}(\theta)$.
\item Пусть $(\mathfrak{T}, \Cs)$ --- измеримое пространство значений статистики $T$, $\Qf_\theta$ ---
распределение статистики $T$, $\widetilde{L}(\theta; T(x))$ --- функция правдоподобия для $T(x)$
(статистика $T$ не обязательно одномерна),
$
I^{T(\xi)}(\theta) = \Mf_\theta
\left(\dfrac{\partial\ln\widetilde{L}(\theta; T(\xi))}{\partial\theta}\right)^2
$ --- информация Фишера в статистике $T$.
Тогда $I^\xi(\theta) \geqslant I^{T(\xi)}(\theta)$.
\end{nums}
\end{stm}

\begin{proof}
Докажем только пункт 3 (остальные тривиальны), да и то лишь в дискретном случае.

$L(\theta; x) = \Pf_\theta(\xi = x)$.
$$\widetilde{L}(\theta;\, T(x)) = \widetilde{L}(\theta;\, t) = \sum\limits_{x: T(x) = t} \Pf_\theta(\xi = x)$$

% FIXME: macro for \widetilde{L}

$$
0 \leqslant \Mf_\theta\left(\frac{\partial\ln L(\theta;
x)}{\partial\theta} - \frac{\partial\ln \widetilde{L}(\theta;
T(x))}{\partial\theta} \right)^2 = I^\xi(\theta) - 2\Mf_\theta
\left(\frac{\partial\ln L(\theta; x)}{\partial\theta} \cdot
\frac{\partial\ln \widetilde{L}(\theta; T(x))}{\partial\theta}
\right) + I^{T(\xi)}(\theta).
$$

Далее (здесь штрих означает производную по $\theta$),
$$
\widetilde L'(\theta; t) = \sum_{x:T(x)=t} \Pf_\theta'(\xi = x) =
\sum_{x:T(x) = t} L'(\theta; x),
$$
откуда
\begin{multline*}
\Mf_\theta\left( \frac{\partial \ln L}{\partial\theta} \cdot \frac{\partial \ln \widetilde{L}}%
{\partial\theta} \right) =
\sum_t \sum_{x: T(x) = t} \frac{L'(\theta;\, x)}{L(\theta;\,x)} \cdot \frac{\widetilde{L}'(\theta;\,T(x))}%
{\widetilde{L}(\theta;\,T(x))} L(\theta;\,x) = \sum_t
\frac{\widetilde{L}'(\theta;\,t)}{\widetilde{L}(\theta;\,t)} \cdot
\sum_{x:T(x)=t} L'(\theta;\,x) =\\= \sum_t \frac{\widetilde L'
(\theta;\,t)}{\widetilde L (\theta;\,t)} \widetilde L'(\theta;\,t)
= \sum_t \left( \frac{\widetilde L' (\theta;\,t)}{\widetilde
L(\theta;\,t)} \right)^2 \widetilde L(\theta;\,t) = \Mf\left(
\frac{\widetilde L' (\theta;\,T(\xi))}{\widetilde L
(\theta;\,T(\xi))} \right)^2 = I^{T(\xi)}(\theta),
\end{multline*}
поэтому $0 \leqslant I^\xi(\theta) - 2 I^{T(\xi)}(\theta) +
I^{T(\xi)}(\theta) = I^\xi(\theta) - I^{T(\xi)}(\theta)$, то есть
$I^\xi(\theta) \geqslant I^{T(\xi)}(\theta)$.
\end{proof}




\subsection{Достаточные статистики}
$(\Xs, \As, \Ps = \{ \Pf_\theta \mid \theta\in\Theta \})$ --- параметрическая модель; $x_1, \dots,
x_n$ --- выборка.

\begin{df}
Статистика $T(x) = T(x_1, \dots, x_n)$ называется \emph{достаточной} (для данной модели), если
для всех $\theta\in\Theta$ и для всех $A\in\As$ условная вероятность
 $\Pf_\theta(A \mid T(\xi) = t)$ не зависит от
$\theta$ для всех $t$, для которых определена условная вероятность.
\end{df}

Найти достаточную статистику по определению не представляется возможным.

\begin{ex}
Схема Бернулли. $T(x) = \sum x_i$. Пусть $t$ и $x$ таковы, что $T(x) = t$. Тогда
$$
\Pf_\theta(\xi = x \mid T(\xi) = t) = \frac{\Pf_\theta(\xi = x)}{\Pf_\theta(T(\xi) = t)} =
\frac{\theta^t (1-\theta)^{n-t}}{C_n^t \theta^t (1-\theta)^{n-t}} = \frac 1 {C_n^t}
\quad \text{--- не зависит от $\theta$.}
$$
Следовательно, $T(x) = \sum x_i$~--- достаточная статистика.
\end{ex}

\begin{theorem}[критерий достаточности~--- теорема Неймана\ч Фишера~--- теорема о факторизации]
$T$~--- достаточная статистика тогда и только тогда, когда имеет
место следующее представление функции правдоподобия:
$$
L(\theta;x) = g_\theta(T(x)) h(x) \qquad g_\theta(T(x)) = g(\theta; T(x)),
$$
где $g_\theta$ и  $h$~--- неотрицательные измеримые функции
(каждая~--- в своей области).
\end{theorem}

\begin{note}
$L(\theta; x)$~--- плотность совместного распределения $\xi_1,
\dots, \xi_n$ по некоторой мере $\mu$; семейство $\{ \Pf_\theta
\}$ абсолютно непрерывно относительно меры $\mu$, плотность~---
производная Радона\ч Никодима. Нам важны два случая: а) $\mu$~---
мера Лебега, б) $\mu$~--- считающая мера.
\end{note}

\begin{proof} \emph{[Доказательство для дискретного случая]}
$x\in\Xs$ --- выборка.
$$
\Pf_\theta(\xi = x \mid T(\xi) = t) =
\begin{cases}
\frac{\Pf_\theta(\xi = x)}{\Pf_\theta(T(\xi) = t)}, & T(x) = t \text{\quad\footnotemark}\\
0, & T(x) \ne t
\end{cases}
$$\footnotetext{Лектор использовал
странное обозначение $x \in \{ x \mid T(x) = t \}$~---
\emph{примеч. С.\,К.}}

\emph{Необходимость.} Пусть $T$~--- достаточная статистика. Тогда
положим
$$
h(x) =
\frac{\Pf_\theta(\xi = x)}{\Pf_\theta(T(\xi) = t)} \text{ --- не зависит от $\theta$
в силу достаточности $T$,}
$$
откуда
$ L(\theta; x) = \Pf_\theta(\xi = x) = \Pf_\theta(T(\xi) = t) \cdot h(x) = g_\theta(T(x))\cdot  h(x) $.

\emph{Достаточность.} Пусть $T(x) = t$. Тогда
$$
\Pf_\theta(\xi = x \mid T(\xi) = t) =
\frac{g_\theta(t) h(x)}{\sum\limits_{T(x) = t} g_\theta(t) h(x)} =
\frac{h(x)}{\sum h(x)} \text{ --- не зависит от $\theta$,}
$$
поэтому $T$ --- достаточная статистика.
\end{proof}

%%% FIXME: куча примеров

\subsection{Условное математическое ожидание}
\begin{df}
Пусть $(\Omega, \As, \Pf)$ --- вероятностное пространство, $\xi$ --- $\As$-измеримая функция
(случайная величина), $\Cs$ --- $\sigma$-подалгебра в $\As$. Тогда \emph{условным математическим
ожиданием (УМО; обозначение: $\Mf(\xi \mid \Cs)$)}
 $\xi$ относительно $\Cs$ (более точно, \emph{вариантом} УМО) называется случайная
величина $\tilde\xi(\omega)$, удовлетворяющая следующим условиям:
\begin{nums}{-2}
\item $\tilde\xi$ $\Cs$-измерима;
\item для всех $C\in\Cs$
$$
\int\limits_C \xi \, d\Pf = \int\limits_C \tilde\xi \, d\Pf
\text{, т.е. }  \Mf\xi\mathrm{I}_C = \Mf\tilde\xi\mathrm{I}_C.
$$
\end{nums}
\end{df}

\begin{note}
$\xi$ не обязана быть $\Cs$-измеримой (иначе определение тривиально: возьмём $\tilde\xi = \xi$).
УМО есть осреднение случайной величины, чтобы та стала измеримой относительно более грубой
$\sigma$-алгебры.
\end{note}

Следующие два утверждения обосновывают корректность определения:
\begin{stm}
Варианты УМО существуют, если $\Mf|\xi| < \infty$.
\end{stm}

\begin{proof}
Сначала рассмотрим случай неотрицательной случайной величины $\xi$.
$Q(C):= \int\limits_C \xi \, d\Pf$ определяет меру на $(\Omega, \Cs)$, абсолютно
непрерывную относительно меры $\Pf$, рассмотренной на $(\Omega, \Cs)$. По теореме
Радона\ч Никодима существует $\Cs$-измеримая функция $g(\omega) \geqslant 0$ такая,
что $\int\limits_\Omega g(\omega) \, d\Pf = 1$ и $Q(C) = \int\limits_C g(\omega) \, d\Pf$
($g$ --- производная Радона\ч Никодима). $g$ по определению является вариантом УМО.

В случае знакопеременной $\xi$ полагаем $\xi = \xi^+ - \xi^-$ ($\xi^+$ и $\xi^-$
неотрицательны) и берём в качестве варианта УМО
$\Mf(\xi\mid\Cs) = \Mf(\xi^+\mid\Cs) - \Mf(\xi^-\mid\Cs)$.
\end{proof}

\begin{note} Условие $\Mf|\xi| < \infty$ не является необходимым. \end{note}

\begin{stm}
Любые два варианта УМО совпадают почти наверное.
\end{stm}

\begin{proof}
От противного: пусть $g_1$ и $g_2$ --- два варианта УМО $\Mf(\xi \mid \Cs)$.
% FIXME: лучше проводить рассуждение для C_\eps = \{ \omega \mid |g_1 - g_2| >= \eps \}
Пусть также $C = \{ \omega \mid g_1(\omega) \ne g_2(\omega) \}$, $\Pf(C) > 0$.
Имеем, что $C = C_< \cup C_>$, где $C_* = \{ \omega \mid g_1(\omega) * g_2(\omega) \}$,
$* \in \{<,>\}$. Хотя бы одно из множеств $C_<$ и $C_>$ имеет положительную меру
(иначе $\Pf(C) = 0$); без ограничения общности считаем $\Pf(C_<) > 0$.
$g_1$ и $g_2$ измеримы, поэтому $D = C_< \in \Cs$. Тогда по определению варианта УМО
$$
\int\limits_D g_1 \, d\Pf = \int\limits_D \xi \, d\Pf = \int\limits_D g_2 \, d\Pf,
$$
что противоречит тому, что $g_1 < g_2$ всюду на $D$ и $\Pf(D) > 0$.
\end{proof}

Поэтому УМО можно считать однозначно определённым с точностью до множеств $\Pf$-меры нуль.

% FIXME: другой подход к определению УМО
\begin{df}
Пусть $(\Omega, \As, \Pf)$ --- вероятностное пространство, $\Cs$
--- $\sigma$-подалгебра в $\As$, $A \in \As$. Тогда \emph{условная вероятность}
 $A$ относительно $\Cs$ определяется так: $Pf(A \mid \Cs) =
 \Mf(I_A \mid \Cs)$, где $I_A$~--- характеристическая функция $A$.
\end{df}

\begin{df}
Пусть $(\Omega, \As, \Pf)$ --- вероятностное пространство, $\xi$ и
$\eta$~--- случайные величины, $\Cs_\eta$~--- $\sigma$-подалгебра
в $\As$, порождённая $\eta$ (\те прообразы всех борелевских
множеств из $\mathbb{R}$). Тогда \emph{условным матожиданием $\xi$
относительно $\eta$} называется $\Mf (\xi \mid \eta) = \Mf (\xi
\mid \Cs_\eta)$.
\end{df}

\subsubsection{Свойства УМО}

\begin{df} $\xi$ \emph{не зависит} от $\sigma$-алгебры $\Cs$, если для
любого события $C\in\Cs$ случайные величины $\xi$ и $\mathrm{I}_C$
независимы.
\end{df}

Обозначение: $\Cs_0 = \{\Omega, \varnothing\}$ --- тривиальная
$\sigma$-алгебра.

\begin{stm}
\begin{nums}{-2}
\item Если $\Pf(\xi = c) = 1$, то $\Mf(\xi \mid \Cs) = c$ п.н.
\item Линейность: $\Mf(\alpha_1\xi_1 + \alpha_2\xi_2 \mid \Cs) =
\alpha_1 \Mf(\xi_1 \mid \Cs) + \alpha_2 \Mf(\xi_2 \mid \Cs)$ п.н.
\item Если $\xi\leqslant\eta$ п.н., то $\Mf(\xi \mid \Cs) \leqslant \Mf(\eta\mid\Cs)$ п.н.
\item $\bigl| \Mf(\xi\mid\Cs) \bigr| \leqslant \Mf(|\xi| \mid \Cs)$ п.н.
\item Неравенство Йенсена: пусть $g(x)$ --- непрерывная выпуклая вниз функция,
$\Mf\bigl| g(\xi) \bigr| < \infty$. \\Тогда $g(\Mf(\xi\mid\Cs)) \leqslant
\Mf(g(\xi)\mid\Cs)$ п.н. Равенство имеет место тогда и только тогда, когда
$\xi$ $\Cs$-измерима (точнее, совпадает с некоторой $\Cs$-измеримой функцией
почти всюду).
\item $\Mf(\xi \mid \As) = \xi$ п.н.
\item Если $\xi$ не зависит от $\Cs$, то $\Mf(\xi\mid\Cs) = \Mf\xi$ п.н.
\item Пусть $\eta$ является $\Cs$-измеримой и $\Mf |\xi\eta| < \infty$. Тогда
$\Mf(\xi\eta \mid \Cs) = \eta \Mf(\xi \mid \Cs)$.
\item $\Mf(\xi \mid \Cs_0) = \Mf\xi$
\item Если $\Cs_1 \subseteq \Cs_2$, то $\Mf\bigl(\Mf(\xi\mid\Cs_2)\mid\Cs_1%
\bigr) = \Mf(\xi\mid\Cs_1)$. Если $\Cs_1 \supseteq \Cs_2$, то
$\Mf\bigl(\Mf(\xi\mid\Cs_2)\mid\Cs_1\bigr) = \Mf(\xi\mid\Cs_2)$.
\item $\Mf\bigl(\Mf(\xi\mid\Cs)\bigr) = \Mf\xi$
\item Если $\Mf\xi^2 < \infty$, то
$$
\inf\limits_{\text{$g(\omega)$ --- $\Cs$-измер.}}
\Mf(\xi - g(\omega))^2 = \Mf(\xi - \Mf(\xi\mid\Cs))^2,
$$
\те $g_{\min}(\omega) = \Mf(\xi\mid\Cs)$ имеет наименьшее
среднеквадратичное отклонение.
\end{nums}
\end{stm}


\subsection{Теорема Колмогорова\ч Блекуэлла\ч Рао}

$(\Xs, \As, \Ps = \{\Pf_\theta \mid \theta\in\Theta\})$ --- параметрическая модель.
$T(x)$ --- достаточная статистика, если существует вариант регулярной условной
вероятности $P_\theta(A\mid T(\xi)) \, \forall A\in\As$, не зависящий от параметра $\theta$.

\begin{theorem}[Колмогоров\ч Блекуэлл\ч Рао]
Пусть $T(x)$ --- достаточная статистика; $\hattheta(x)$ --- оценка параметра $\theta$ с
$\Mf_\theta \hattheta^2 < \infty$. Тогда оценка $\theta^*(\xi) = \Mf_\theta(\hattheta(\xi)\mid
T(\xi))$ обладает следующими свойствами:
\begin{nums}{-2}
\item $\Mf_\theta\theta^*(\xi) = \Mf_\theta\hattheta(\xi)$,
\item $\Mf_\theta(\theta^*(\xi) - \theta)^2 \leqslant \Mf(\hattheta(\xi) - \theta)^2$.
Равенство имеет место тогда и только тогда, когда $\hattheta$ измерима относительно $\Cs_T$ (является
измеримой функцией достаточной статистики).
\end{nums}
\end{theorem}

\begin{proof}
%$\theta^*$ есть функция от $T$ (т.к. $\theta^* = \Mf(\hattheta \mid T)$
%$\Cs_T$-измерима). Поэтому
В силу определения достаточной статистики $\theta^* = \Mf(\hattheta \mid
T)$ не зависит от $\theta$, поэтому $\theta^*$ --- статистика.
$\theta^*$ $\Cs_T$-измерима (в силу определения УМО), поэтому $\theta^*$
есть функция от $T$.
$\Mf \theta^* = \Mf \bigl(\Mf(\hattheta\mid T)\bigr) = \Mf \hattheta$.
В силу неравенства Йенсена $\Mf\bigl( (\hattheta - \theta)^2 \mid T \bigr)
\geqslant \bigl( \Mf(\hattheta \mid T) - \theta \bigr)^2 =
(\theta^* - \theta)^2$. Возьмём математическое ожидание:
$\Mf (\hattheta - \theta)^2 =
\Mf \bigl( \Mf\bigl( (\hattheta - \theta)^2 \mid T \bigr) \bigr)
\geqslant \Mf (\theta^* - \theta)^2$, причём равенство в
неравенстве Йенсена достигается тогда и только тогда, когда
$\hattheta$ $\Cs_T$-измерима, т.е. есть функция от $T$.
\end{proof}

\subsection{Полная достаточная статистика}

\begin{df}
Достаточная статистика $T(x)$ называется \emph{полной} (для данного параметрического семейства
распределений), если для любой измеримой функции $f(t)$ при любом значении параметра из условия
$\Mf_\theta f(T(\xi)) = 0$ следует, что $f(T(\xi)) = 0$ почти наверное.
\end{df}

Полная достаточная статистика $T$ обладает следующим свойством: если
$$\Mf_\theta f_1(T(\xi)) = \Mf_\theta f_2(T(\xi)),$$
то $f_1(T(\xi)) = f_2(T(\xi))$ почти наверное.
Доказательство очевидно.

\begin{theorem}
Пусть $T(x)$ --- полная достаточная статистика и $f$ и $g$ таковы, что $\Mf_\theta f(T(\xi)) =
g(\theta)$. Тогда $f(T)$ является эффективной оценкой для $g(\theta)$.
\end{theorem}

\begin{proof}
Докажем теорему в частном случае $g(\theta) = \theta$.

Дано, что $\Mf_\theta f(T) = \theta$, т.е. $\hattheta(\xi) = f(T(\xi))$ --- несмещённая оценка
параметра $\theta$. Докажем, что эта оценка имеет наименьшую дисперсию среди всех возможных
несмещённых оценок. От противного. Пусть существует такая оценка $\widetilde\theta$, что
$\Mf \widetilde\theta(\xi) = \theta$ ($\widetilde\theta$ --- несмещённая оценка) и
$\Df_{\theta_0} \widetilde\theta(\xi) < \Df_{\theta_0} \hattheta(\xi)$ хотя бы для одного
$\theta_0 \in \Theta$. Улучшим оценку $\widetilde\theta$ при помощи теоремы
Колмогорова\ч Блекуэлла\ч Рао: $\theta^* = \Mf_\theta(\widetilde\theta \mid T(\xi))$, причём
\begin{nums}{-2}
\item $\theta^* = \theta^*(T(x))$;
\item $\Mf_\theta \theta^* = \theta$;
\item $\Df_\theta \theta^* \leqslant \Df_\theta \widetilde\theta \,\, \forall\theta\in\Theta$.
\end{nums}
Имеем: $\Df_{\theta_0} \theta^* \leqslant \Df_{\theta_0} \widetilde\theta < \Df_{\theta_0}
\hattheta$. Но $\theta^*$ и $\hattheta$ суть функции полной достаточной статистики $T$,
$\Mf_\theta \theta^* = \theta = \Mf_\theta \hattheta$, откуда $\theta^*(\xi) = \hattheta(\xi)$
почти наверное, а поэтому $\Df_{\theta_0} \theta^* = \Df_{\theta_0} \hattheta$. Противоречие.
\end{proof}

\begin{note}
Полная достаточная статистика существует не всегда.
\end{note}


\subsubsection{Замечания о минимальной достаточной статистике}
%FIXME обоснование
\begin{nums}{-2}
\item Любая измеримая взаимно-однозначная функция от достаточной статистики сама является
достаточной статистикой.
\item Если для любой достаточной статистики $T$ существует такая измеримая взаимно-однозначная
функция $\psi$, что $T_{\min}(x) = \psi(T(x))$, то $T_{\min}$ называется \emph{минимальной} достаточной
статистикой. Минимальная достаточная статистика существует всегда (для любой параметрической модели).
Можно проводить редукцию (без потери информации) от выборки к минимальной достаточной статистике.
Дальнейшая редукция невозможна.
\item Как правило, в разложении, которое даёт теорема Неймана\ч Фишера, появляется минимальная
достаточная статистика.
\item Полная достаточная статистика (если она существует) является минимальной.
\end{nums}

%FIXME: задача*

%\subsubsection{Замечания об экспоненциальных семействах распределений}
% FIXME FIXME


\section{Байесовские статистические оценки}
\subsection{Байесовские точечные оценки}
\subsubsection{Функция риска}
Рассмотрим функцию $u$, называемую функцией штрафа (потерь), обладающую следующими
свойствами:
\begin{nums}{-2}
\item $u$ чётна;
\item $u$ возрастает на $(0; +\infty)$;
\item $u(0) = 0$.
\end{nums}
\emph{Функцией риска} оценки $\delta$ параметра $\theta$
называется функция $R(\theta; \delta) = \Mf\bigl(u(\delta - \theta)\bigr)$. В частном
случае $u(x) = x^2$ (обычно рассматриваемом на практике) функция риска
$R(\theta; \delta) = \Mf(\delta - \theta)^2$ называется
\emph{квадратичной функцией риска}. В теории эффективных оценок статистики сравниваются
в смысле равномерной минимизации риска.

\begin{ex}
В схеме Бернулли оцениваем параметр $p$. Эффективная оценка --- частота
$\hat p = \frac 1n (x_1 + \dots + x_n)$. Для неё функция риска (квадратичная)
равна $R_1 = R(p; \hat p) = \frac{p(1-p)}{n}$. Пусть теперь $n=2$ (выборка состоит
из двух элементов $x_1, x_2$). Рассмотрим странную оценку $\hat{\hat p} = \frac 12$
для всех значений $x_i$. Её функция риска есть $R_2 = R(p, \hat{\hat p}) = (\frac 12 -
p)^2$. Если нам откуда-то известно (имеется априорная информация), что $\frac 13 \leqslant
p \leqslant \frac 23$, то странная оценка $\hat{\hat p}$ оказывается лучше.
\end{ex}

\subsubsection{Байесовский подход}
Параметр $\theta$ рассматривается как случайная величина $\btheta$ со
значениями $\theta\in\Theta$ и распределением вероятностей $\Pi$ на измеримом пространстве
$(\Theta, \Bs_\Theta)$. Распределение $\Pi$ называется \emph{априорным распределением} параметра.

Усредним функцию риска $R(\theta;\delta)$ по распределению параметра:
$$
R(\Pi; \delta) = \int\limits_\Theta R(\theta;\delta) \, \Pi(d\theta).
$$
$R(\Pi; \delta)$ называется \emph{априорным риском}. Оценка $\delta^*$, минимизирующая
априорный риск, называется \emph{байесовской оценкой} параметра $\theta$:
$R(\Pi; \delta^*) = \inf\limits_{\delta\in\Delta} R(\Pi; \delta)$ ($\Delta$ ---
множество оценок). Можно схитрить: взять априорное распределение так, чтобы
получить лучшую оценку.

\subsection{Минимаксные оценки}
Пусть теперь нет ни эффективной оценки, ни априорной информации.
$R_M(\delta) = \sup\limits_{\theta\in\Theta} R(\theta; \delta)$ --- наихудшее значение функции
риска.
\begin{df}
$\delta^*$ называется \emph{минимаксной} оценкой $\theta$, если
$R_M(\delta^*) = \inf\limits_{\delta\in\Delta} R_M(\delta)$ (т.е. она
минимизирует максимальное значение функции риска).
\end{df}

\begin{note} Минимаксную оценку можно получить как байесовскую при некотором (,,наименее
благоприятном'') априорном распределении.
\end{note}

\subsection{Теорема о байесовской оценке для квадратичной функции риска}
Рассмотрим функцию $\Pf_\theta(A)$, зависящую от $A\in \As$ и $\theta \in \Theta$.
При каждом фиксированном $A$ $\Pf_\theta(A)$ есть $\Bs_\Theta$-измеримая функция от $\theta$,
а при каждом фиксированном $\theta$ --- задаёт распределение вероятностей на $(\Xs, \As)$.
Рассмотрим на декартовом произведении $\Theta \times \Xs$ $\sigma$-алгебру, порождённую
прямоугольниками $B \times A$, где $B \in \Bs_\Theta$, $A \in \As$. На этих прямоугольниках введём
меру $Q$:
$$
Q(B \times A) = \int\limits_B \Pf_\theta(A) \, \Pi(d\theta).
$$
Более-менее очевидно, что эта мера $\sigma$-аддитивна. % FIXME: более строго
Значит, она продолжается по Лебегу на всю $\sigma$-алгебру, которую мы назовём $\Bs_\Theta \times
\As$. Будем рассматривать квадратичную функцию потерь и предполагать, что $R(\Pi; \delta) =
\Mf_Q(\btheta - \delta(\xi))^2 < \infty$.

\begin{theorem}[о байесовской оценке для квадратичной функции риска]
Пусть априорное распределение $\Pi$ и оценка $\delta(x)$ таковы, что $R(\Pi; \delta) =
\Mf_Q(\btheta - \delta(\xi))^2 < \infty$. Тогда байесовской оценкой параметра $\theta$
является $\delta^*(\xi) = \Mf_Q(\btheta \mid \xi)$ (она определена с точностью до
множеств $Q$-меры нуль).
\end{theorem}

\begin{proof}
\begin{nums}{-2}
\item $\delta^*$ является статистикой, т.к. она зависит от $\btheta$ посредством
распределения, а оно фиксировано.
\item $\Mf_Q\bigl((\btheta - \delta(\xi))^2 \mid \xi\bigr)$, $\Mf_Q(\btheta^2\mid\xi)$ и
$\Mf_Q(\btheta\mid\xi)$ существуют.
\item $\Mf_Q(\btheta - \delta^*(\xi))^2 = \Mf_Q \bigl(\Mf_Q((\btheta - \delta^*(\xi))^2 \mid \theta)
\bigr) \leqslant \Mf_Q(\btheta - \delta^*(\xi))^2$ в силу свойств
УМО ($\delta^* = \Mf_Q(\btheta\mid\xi)$). Равенство~--- при
$\delta^*(\xi) = \Mf_Q(\btheta\mid\xi)$.
\end{nums}
\end{proof}

\begin{imp}[единственность байесовской оценки]
Пусть $\delta_1^*, \delta_2^*$ --- две байесовские оценки (относительно
квадратичной функции потерь). Тогда $Q(\delta_1^* \ne \delta_2^*) = 0$.
\end{imp}

\subsection{Апостериорный риск}
Пусть семейство $\Ps$ абсолютно непрерывно относительно лебеговой меры. Тогда для любого
события $A \in \As$
$$\Pf_\theta(A) = \int\limits_A p_\theta(x) \, dx.$$

\begin{stm}
Пусть $\delta^*$~--- байесовская оценка относительно квадратичной
функции потерь. Тогда $\Pf_\theta$-п.\,н. имеем
$$
\delta^* = \frac{\int\limits_\Theta \theta p_\theta(x) \, \Pi(d\theta)}
{\int\limits_\Theta p_\theta(x) \, \Pi(d\theta)}.
$$
\end{stm}

\begin{proof}
Пусть $A \in \As$ --- произвольное событие. Положим $C = \Theta \times A \in \Bs_\Theta \times \As$.
По определению УМО и теореме о байесовской оценке
$$
\int\limits_C \delta^* \, dQ = \int\limits_C \btheta \, dQ.
$$
Вспоминая определение меры $Q$ и теорему Фубини, получаем:
$$
\int\limits_A \delta^*(x) \left( \int\limits_\Theta p_\theta(x) \, \Pi(d\theta) \right) \, dx =
\int\limits_C \delta^* \, dQ = \int\limits_C \btheta \, dQ =
\int\limits_A \left( \int\limits_\Theta \theta p_\theta(x) \, \Pi(d\theta) \right) \, dx,
$$
откуда в силу произвольности выбора $A$ получаем, что
$$
\delta^*(x) \int\limits_\Theta p_\theta(x) \, \Pi(d\theta) =
\int\limits_\Theta \theta p_\theta(x) \, \Pi(d\theta) \quad \text{$\Pf_\theta$-п.н.}
$$
\end{proof}

Если априорное распределение $\Pi$ имеет плотность $\pi(\theta)$, то
$$
\delta^*(x) = \frac{\int\limits_\Theta \theta p_\theta(x) \pi(\theta) \, d\theta}
{\int\limits_\Theta p_\theta(x) \pi(\theta) \, d\theta} =
\int\limits_\Theta \theta q(\theta \mid x) \, d\theta,
$$
где
$$
q(\theta \mid x) = \frac{p_\theta(x) \pi(\theta)}{\int\limits_\Theta p_\theta(x) \pi(\theta) \, d\theta}
$$
называется \emph{апостериорной плотностью} (плотностью апостериорного распределения параметра).
Это условное распределение $\btheta$ при условии $\xi$. Последняя формула называется
формулой Байеса для плотностей.

\begin{note} Байесовская оценка минимизирует апостериорный риск $\Mf_Q((\btheta - \delta(\xi))^2 \mid
\xi)$ (без доказательства).
\end{note}

\subsection{Связь байесовских оценок с понятием достаточной статистики}
\begin{nums}{-2}
\item Апостериорное распределение параметра относительно любого априорного распределения является
функцией достаточной статистики. Действительно, пусть $T(x)$ --- достаточная статистика. Тогда
по теореме Неймана\ч Пирсона $p_\theta(x) = g(\theta; T(x)) h(x)$. При условии существования плотности $\pi$
априорного распределения для плотности $q(\theta\mid x)$ условного распределения имеет место следующее:
$$
q(\theta \mid x) = \frac{p_\theta \pi(\theta)}{\int\limits_\Theta p_\theta(x) \pi(\theta) \, d\theta} =
\frac{g(\theta; T(x)) \pi(\theta)}{\int\limits_\Theta g(\theta; T(x)) \pi(\theta) \, d\theta} =
f(T(x)),
$$
т.е. плотность апостериорного распределения есть функция достаточной статистики.

\item При некоторых дополнительных условиях на $\Pi$ (например, если $\pi(\theta) \ne 0$ при всех
$\theta \in \Theta$) верно и обратное: если апостериорное распределение
$\btheta$ зависит от $x$ посредством статистики $T(x)$, то $T(x)$ является достаточной статистикой.
Таким образом мы получили байесовский критерий достаточности статистики.
\end{nums}


\subsection{Байесовские интервальные оценки}
Предположим, что распределения $\Pf_\theta$ и $\Pi$ абсолютно непрерывны относительно лебеговой меры,
т.е. существуют плотности $p_\theta(x)$ и $\pi(\theta)$. $q(\theta\mid x)$ --- апостериорная плотность.
Возьмём произвольное $\alpha \in (0,1)$ и произвольным образом разобьём его на две части:
$\alpha = \alpha_1 + \alpha_2$, $\alpha_{1,2} \in (0,1)$. Найдём $\alpha_1$- и $(1-\alpha_2)$-квантили
апостериорного распределения и обозначим их $\utheta = \utheta(\alpha_1, x)$ и $\otheta =
\otheta(\alpha_2, x)$ соответственно:
$$
\int\limits_{-\infty}^{\utheta} q(\theta \mid x) \, d\theta = \alpha_1, \qquad
\int\limits_{-\infty}^{\otheta} q(\theta \mid x) \, d\theta = 1 - \alpha_2.
$$
$$
\Pf(\utheta \leqslant \btheta \leqslant \otheta) =
\int\limits_{\utheta}^{\otheta} q(\theta\mid x) \, d\theta = 1 - \alpha_2 - \alpha_1 = 1 - \alpha,
$$
где вероятность рассматривается в смысле меры $Q$ (и по априорному распределению $\btheta$,
и по распределениям из семейства $\Ps$).
Получили интервал $[\utheta, \otheta]$ для параметра --- это и есть
\emph{байесовская интервальная оценка} (этот интервал определяется по $\alpha$ неоднозначно).
$\alpha$ --- вероятность того, что $\btheta$ не попадёт в интервал (вероятность ошибки при
использовании этого интервала).


% FIXME: remove this!
%\subsection{Условное математическое ожидание}
%I. \emph{УМО случайной величины относительно $\sigma$-подалгебры}
%$(\Omega, \As, \Pf)$ --- вероятностное пространство; $\xi$ --- $\As$-измеримая функция (случайная
%величина); $\Cs\subset\As$ --- $\sigma$-подалгебра в $\As$.

%а) $\xi \geqslant 0$, $C\in\Cs$.
%$Q(C) = \int\limits_C \xi \, d\Pf$ --- мера на $(\Omega, \Cs)$, абсолютно непрерывная
%относительно меры $\Pf$, рассмотренной на $(\Omega, \Cs)$. В силу теоремы Радона\ч Никодима
%существует $\Cs$-измеримая функция $g(\omega) \geqslant 0$ такая, что
%$\int\limits_\Omega g(\omega) \, d\Pf = 1$ и $Q(C) = \int\limits_C g(\omega) \, d\Pf$ для
%всех $C\in\Cs$ ($g$ --- производная Радона\ч Никодима). Случайная величина
%$\tilde\xi(\omega) = g(\omega)$ называется УМО $\xi$ относительно $\Cs$.

%б) $\xi$ знакопеременна. $\xi = \xi^+ - \xi^-$. $\Mf(\xi \mid \Cs) = \Mf(\xi^+ \mid \Cs) -
%\Mf(\xi^- \mid\Cs)$.

%II. \emph{Условная вероятность события относительно $\sigma$-подалгебры} $A\in\As$
%$\Pf(A \mid \Cs) := \Mf(\mathrm{I}_A \mid \Cs)$.

%III. \emph{УМО одной случайной величины относительно другой}
%$\xi$ и $\eta$ --- случ

%\begin{df}
%УМО $\xi$ относительно $\Cs$ будем называть случайную величину $\tilde\xi(\omega) =:
%\Mf(\xi \mid \Cs)$ со следующими свойствами:
%\begin{nums}{-2}
%\item $\tilde\xi$ $\Cs$-измерима;
%\item для всех $C\in\Cs$
%$
%\int\limits_C \xi \,d\Pf = \int\limits_C \tilde\xi \,d\Pf
%$ (т.е. $\Mf \xi \mathrm{I}_C = \Mf \tilde\xi \mathrm{I}_C$).
%\end{nums}
%\end{df}

%\end{document}


%%%%% Про доверительные интервалы
\section{Доверительные интервалы}
$(\Xs, \As, \Ps = \{\Pf_\theta \mid \theta\in\Theta\})$ --- параметрическая
модель, $\theta$ --- скалярный параметр (иначе интервалы заменяются
более сложными областями). $(\xi_1, \dots, \xi_n)$ --- случайная выборка,
$x = (x_1, \dots, x_n)$ --- её реализация. По заданному $\alpha \in (0,1)$ и
по выборке находим такие статистики $\utheta(\alpha; x) <
\otheta(\alpha; x)$, что $\Pf_\theta([\utheta, \otheta] \ni
\theta) \geqslant 1 - \alpha$. В этом случае интервал $[\utheta,
\otheta]$ называется (точным) \emph{доверительным интервалом} для $\theta$
с \emph{доверительной вероятностью} (\emph{доверительным уровнем})
$1 - \alpha$. $\alpha$ --- вероятность ошибки при использовании
данного интервала. $\utheta$ и $\otheta$ называются \emph{доверительными
границами}.

Доверительный интервал --- частный случай интервальной оценки. В
отличие от байесовского подхода здесь $\theta$\т не случайная
величина; зато у Байеса границы постоянные, а у нас\т
случайные (зависят от выборки).

\emph{Сравнение} двух доверительных интервалов производится по
двум характеристикам:
\begin{nums}{-2}
\item по $\alpha$ (или по доверительной вероятности $1 - \alpha$);
\item по средней длине доверительного интервала.
\end{nums}

%\subsection{Примеры точных доверительных интервалов}
\subsection{Доверительные интервалы для параметров нормального распределения}
\subsubsection{Доверительный интервал для среднего при известной дисперсии}
$\xi_1, \dots, \xi_n$ --- повторная выборка,
$\xi \sim \Norm(a, \sigma^2)$, $a = \Mf \xi_i$, $\sigma^2 = \Df \xi_i$;
$\sigma^2$ известно. $\overline{\xi}$ --- лучшая точечная оценка для $a$.


$\overline{\xi} \sim \Norm(a, \dfrac{\sigma^2}{n})$. Отсюда
$
\dfrac{\overline{\xi} - a}{\sqrt{{\sigma^2}/{n}}} \sim
\Norm(0,1)
$ --- распределение не зависит от неизвестного параметра
(заметим, что это не статистика, т.к. она зависит от параметра $a$).
Поэтому
$$
\Pf_a \left( \left| \frac{\overline\xi - a}{\sqrt{{\sigma^2}/n}} \right|
\leqslant u \right) = \Phi(u) - \Phi(-u) = 2\Phi(u) - 1,
\qquad
\text{где }
\Phi(x) = \int\limits_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-{u^2}/2} \, du
\text{ --- ф. р. $\Norm(0,1)$.}
$$

Пусть задано $\alpha \in (0,1)$. По таблицам стандартного нормального
распределения находим $(1 - \frac{\alpha}{2})$-квантиль $\Norm(0,1)$.
Обозначим эту квантиль $u_{1 - \frac{\alpha}{2}}$. Тогда
$\Phi(u_{1-\frac\alpha 2}) = 1 - \frac\alpha 2$, откуда
$2\Phi(u_{1-\frac\alpha 2}) - 1 = 1 - \alpha$. В силу предыдущего
$$
\Pf_a \left( \left| \frac{\overline\xi - a}{\sigma / \sqrt{n}} \right|
\leqslant u_{1 - \frac\alpha 2} \right) = 1 - \alpha,
$$
поэтому $\left[\overline x - u_{1 - \frac\alpha 2} \dfrac{\sigma}{\sqrt n},
\overline x + u_{1 - \frac\alpha 2} \dfrac{\sigma}{\sqrt n}\right]$ ---
доверительный интервал для $a$ с доверительной вероятностью $1-\alpha$.


% FIXME: про распределение Стьюдента (интервал для a при неизв. \sigma^2)

\subsubsection{Доверительный интервал для дисперсии при известном среднем}
$s_0^2 = \dfrac 1n \sum\limits_{k=1}^{n} (x_k - a)^2$ --- несмещённая
($\Mf s_0^2 = \sigma^2$) оценка для $\sigma^2$.
$$
Z_k = \frac{\xi_k - a}{\sigma} \sim \Norm(0,1),
\qquad
\frac{n s_0^2}{\sigma^2} = \sum_{k=1}^{n} \left(
\frac{\xi_k - a}{\sigma} \right)^2
= Z_1^2 + \dots + Z_n^2.
$$
$Z_k$ взаимно независимы и одинаково распределены с $\Norm(0,1)$.
Распределение случайной величины $Z_1^2 + \dots + Z_n^2$ называется
\emph{хи-квадрат} распределением с $n$ степенями свободы и обозначается
$\chi^2_n$. Для квантилей этого распределения составлены таблицы.
Используя эти таблицы, находим квантили $g_1 = g_1(\frac\alpha 2)$ ---
$\frac \alpha 2$-квантиль и $g_2 = g_2(\frac\alpha 2)$ ---
$(1 - \frac\alpha 2)$-квантиль $\chi^2_n$. Получаем, что
$$
\Pf_{\sigma^2} \left( g_1 \leqslant \frac{n s_0^2}{\sigma^2} \leqslant
g_2 \right) = \left( 1 - \frac \alpha 2 \right) - \frac \alpha 2 =
1 - \alpha,
$$
то есть $\left[ \dfrac{n s_0^2}{g_2}, \dfrac{n s_0^2}{g_1} \right]$
является доверительным интервалом для $\sigma^2$ с доверительной
вероятностью $1-\alpha$.

\subsection{Точный доверительный интервал для параметра биномиального
распределения}
Пусть дана случайная выборка $(\xi_1, \dots, \xi_n)$ со значениями
$x_1, \dots, x_n$, $\xi_k$ взаимно независимы и распределены
следующим образом:
$$
\xi_k = \begin{cases}
0 & \text{с вероятностью $1-p$},\\
1 & \text{с вероятностью $p$},
\end{cases}
$$
где $p$ --- неизвестный параметр.
$S_n = \sum\limits_{i=1}{n} \xi_i$, $\Pf(S_n = m) = C^m_n p^m (1-p)^{n-m}$.

Ранее было показано, что если $\eta_1, \dots, \eta_n$ взаимно
независимы и равномерно распределены на $[0,1]$, то
$$\Pf(\xi_{(m)} \leqslant x) = \sum_{k\geqslant m} C_n^k x^k
(1-x)^{n-k} = I_x(m; n-m+1),
$$
где $I_x(a;b)$ --- неполная бета-функция с аргументом $x$
и параметрами $a$ и $b$, которая удовлетворяет соотношению
$I_x(a;b) + I_{1-x}(b;a) = 1$.

Положим
$$f_m(p):= \sum_{k=0}^{m} C_n^k p^k (1-p)^{n-k} = 1 -
\sum_{k=m+1}^{n} C^k_n p^k (1-p)^{n-k} = 1 - I_p(m+1;n-m) =
I_{1-p}(n-m;m+1).
$$
(При фиксированном $m$ это функция от аргумента $p \in [0,1]$; а
$m = S_n$ известно нам из выборки.) $f_m$ --- непрерывная и строго
убывающая функция, $f_m(0) = 1$, $f_m(1) = 0$. Поэтому для каждого
$\beta \in (0,1)$ уравнение $f_m(p) = \beta$ имеет единственный
корень $p = \olp = \olp_m(\beta)$. $\sum\limits_{k=m}^{n} C^k_n
p^k (1-p)^{n-k} = 1 - f_{m-1}(p)$. Для каждого $\beta \in (0,1)$
уравнение $1 - f_{m-1}(p) = \beta$ имеет единственный корень $p =
\ulp = \ulp_{m-1}(\beta)$.

Полагаем $\beta = \alpha/2$ и получаем, что
$$\Pf_p (p > \olp) = \Pf_p(f_m(p) < \beta) = \Pf_p \left(
\sum_{k=S_n}^{n} C_n^k p^k (1-p)^{n-k} < \beta \right) =
\sum_{j \, : \, \sum\limits_{k\leqslant j} \Pf(S_n = k) \leqslant \beta}
\Pf(S_n = j) \leqslant \beta,
$$
то есть $\Pf(p > \olp) \leqslant \beta$. Аналогично
$\Pf(p < \ulp) \leqslant \beta$. Окончательно,
$$
\Pf_p ( \ulp \leqslant p \leqslant \olp ) \geqslant 1 - 2\beta =
1 - \alpha,
$$
то есть интервал $[\ulp,\olp]$ есть доверительный интервал для $p$
с доверительной вероятностью $1 - \alpha$.

\subsection{Общие способы получения доверительных интервалов}
\subsubsection{Метод центральной статистики}
Пусть $\xi_1, \dots, \xi_n$ взаимно независимы и распределены с
абсолютно непрерывным распределением вероятностей $\Pf_\theta$.
Найдём функцию $S(\xi;\theta)$~--- так называемую
\emph{центральную статистику}, хотя на самом деле она статистикой
не является,~--- со следующими свойствами:
\begin{nums}{-1}
\item распределение $S(\xi;\theta)$ не зависит от $\theta$ и
имеет плотность $f_S(y)$,
\item при каждом фиксированном $x\in \Xs$ функция $S(x;\theta)$
(как функция аргумента $\theta$) является непрерывной и строго монотонной.
\end{nums}
Зададим $\alpha\in(0,1)$ и найдём $u_1 < u_2$ так, чтобы
$$
\Pf(u_1 \leqslant S(\xi; \theta) \leqslant u_2) =
\int\limits_{u_1}^{u_2} f_S(y) \, dy = 1-\alpha.
$$
($u_1$ и $u_2$ находятся неоднозначно.) Заметим, что распределение
вероятностей в правой части не зависит от параметра $\theta$.
Обратим неравенство относительно $\theta$ и получим, что
$\Pf(T_1(\xi) \leqslant \theta \leqslant T_2(\xi)) =  1 - \alpha$,
\те $[T_1(x), T_2(x)]$ есть доверительный интервал для $\theta$ с
доверительной вероятностью $1-\alpha$.

\subsubsection{Ещё один метод}
Пусть $T(x)$ --- некоторая статистика, $G_\theta(t) =
\Pf_\theta(T(\xi) \leqslant t)$~--- её функция распределения.
Пусть $G_\theta(t)$ при любом фиксированном $\theta$ непрерывна и
строго монотонна относительно $t$ и при любом фиксированном $t$
является непрерывной строго убывающей функцией от $\theta$. Тогда
$\Pf_\theta(G_\theta(t_1) \leqslant G_\theta(T(\xi)) \leqslant
G_\theta(t_2)) = \Pf_\theta(t_1 \leqslant T(\xi) \leqslant t_2) =
G_\theta(t_2) - G_\theta(t_1)$ при любых $t_1 < t_2$. Если
$G_\theta(t_1) = \alpha_1$, $G_\theta(t_2) = 1 - \alpha_2$,
$\alpha_1 + \alpha_2 = \alpha$, то $\Pf_\theta(\alpha_1 \leqslant
G_\theta(T) \leqslant 1 - \alpha_2) = 1 - \alpha_2 - \alpha_1 = 1
- \alpha$. Положим $\otheta$: $G_\otheta(T(x)) = \alpha_1$;
$\utheta$: $G_\utheta(T(x)) = \alpha_2$. Тогда $\Pf(\utheta
\leqslant \theta \leqslant \otheta) = 1-\alpha$, \те
$[\utheta,\otheta]$~--- доверительный интервал с вероятностью
ошибки $\alpha$.

\subsection{Асимптотические доверительные интервалы}
Как правило, они основаны на асимптотической нормальности некоторых
статистик.

\begin{ex} В схеме Бернулли с параметром $p\in[0,1]$ рассматривается
оценка  % FIXME: macro for \widehat{p}
$\widehat{p}_n = \frac 1n (x_1 + \dots + x_n)$. $\Mf \widehat{p}_n
= p, \Df \widehat{p}_n = \dfrac{p(1-p)}{n}$. В силу ЦПТ
$\widehat{p}_n$ асимптотически нормальна с $\Norm\left(p,
\dfrac{p(1-p)}{n}\right)$, \те
$$
\frac{\widehat{p}_n - p}{\sqrt{\dfrac{p(1-p)}{n}}}
\xrightarrow[\nrai]{\mathrm d} Z \sim \Norm(0,1).
$$

Положим $u = u_{1-\frac\alpha 2}$~--- $(1-\frac\alpha 2)$-квантиль
стандартного нормального распределения. Получаем, что
$$
\Pf \left( \left| \frac{\widehat{p}_n - p}{\sqrt{\frac{p(1-p)}{n}}}
\right| \leqslant u \right) \approx \Phi(u) - \Phi(-u) = 1 - \alpha,
$$
где $\Phi$~--- функция стандартного нормального распределения.
Точность вычисляется из оценок скорости сходимости в ЦПТ. Решая
неравенство под знаком вероятности, получаем интервал для $p$:
$\Pf(\ulp_n \leqslant p \leqslant \olp_n) \approx 1 - \alpha$.

\end{ex}

\begin{lemma} Пусть оценка $\hattheta_n(x)$ параметра $\theta$ асимптотически
нормальна с $\Norm ( \theta; \sigma^2(\theta) / n )$. Пусть
$f(\theta)$ дифференцируема и $f'(\theta) \ne 0$ в рассматриваемой
области. Тогда $f(\hattheta_n)$ асимптотически нормальна с распределением
$$\Norm \left(f(\theta); \dfrac{(f'(\theta))^2 \sigma^2(\theta)}{n}\right).$$
\end{lemma}

\begin{note}
Функцию $f(\theta)$ можно выбрать так, чтобы $f'(\theta) \sigma(\theta) =
\const$, и таким образом избавится от зависимости дисперсии от
параметра.
\end{note}

\begin{proof}
Разложим $f$ по формуле Тейлора: $f(t) = f(\theta) + (t-\theta)
(f'(\theta) + \gamma(t; \theta))$, где $\gamma(t; \theta) \to 0$
при $t \to 0$. Подставим $t = \hattheta_n$:
$$
f(\hattheta_n) - f(\theta) = (\hattheta_n - \theta) f'(\theta)
\left( 1 + \frac{\gamma(\hattheta_n; \theta)}{f'(\theta)} \right).
$$
Положим
$$
Y_n:= \frac{\hattheta_n - \theta}{\sigma(\theta) / {\sqrt n}},
\qquad
Z_n:= \frac{\gamma(\hattheta_n; \theta)}{f'(\theta)},
\qquad
W_n:= \frac{f(\hattheta_n) - f(\theta)}{f'(\theta) \sigma(\theta) /
{\sqrt n}}
$$
и получим, что $W_n = Y_n (1 + Z_n)$. Но из асимптотической
нормальности $\hattheta_n$ $Y_n \xrightarrow{\mathrm d} Y \sim
\Norm(0,1)$, а $\gamma(\hattheta_n; \theta) \xrightarrow{\Pf} 0$
в силу состоятельности, поэтому $W_n \xrightarrow{\mathrm d} Y \sim
\Norm(0,1)$, что и даёт асимптотическую нормальность $f(\hattheta_n)$.
\end{proof}

\begin{ex}
В схеме Бернулли оценка $\widehat{p}_n = \frac 1n (x_1 + \dots + x_n)$
асимптотически нормальна с $\Norm(p; p(1-p)/n)$. Положим
$f(p) = \arcsin \sqrt p$, $p \in (0,1)$. $f'(p) =
\dfrac{1}{2\sqrt{p(1-p)}}$, $f'(p) \sigma(p) =
\dfrac{\sqrt{p(1-p)}}{2\sqrt{p(1-p)}} = \dfrac 12 = \const$.
В силу предыдущей леммы оценка
$\arcsin \sqrt {\widehat{p}_n}$ асимптотически
нормальна с $\Norm(\arcsin \sqrt p, \frac 1 {4n})$. Пользуясь
монотонностью, получаем асимптотический доверительный
интервал для $p$.
\end{ex}

\section{Отступление про некоторые распределения вероятностей}
\subsection{Хи-квадрат распределение}
Пусть случайные величины $Z_1, \dots, Z_n$ взаимно независимы и
имеют стандартное нормальное распределение. Тогда распределение
случайной величины $Z_1^2 + \dots + Z_n^2$ называется \emph{хи-квадрат}
распределением с $n$ степенями свободы и обозначается $\chi^2_n$.
При $\chi^2_2$ имеет показательное распределение с параметром
$\lambda = \frac 12$. Плотность хи-квадрат распределения даётся
формулой:
$$
p_{\chi^2_n} = \frac{1}{2^{\frac n2} \Gamma(\frac n2)} x^{\frac n2 - 1}
e^{- \frac x2}, \qquad
\text{где } \Gamma(\lambda) = \int\limits_0^\infty x^{\lambda - 1}
e^{-x} \, dx \text{ --- гамма-функция Эйлера.}
$$
$\Mf \chi^2_n = n$, $\Df \chi^2_n = 2n$. Характеристическая
функция: $\ph_{\chi^2_n} = (1-2it)^{- \frac n2}$.

{\small {\bf Мелким шрифтом про вычисление плотности $\chi^2$.}
$$
\Pf ( Z_1^2 + \dots + Z_n^2 \leqslant y ) =
\idotsint\limits_{\sum\limits_k x_k^2 \leqslant y}
\left( \frac 1 {\sqrt{2\pi}} \right)^n \exp \left(
- \frac 12 \sum\limits_k x_k^2 \right) \, dx_1 \dots dx_n.
$$
Применяем обобщённую сферическую замену координат:
$
x_1, \dots, x_n \mapsto \rho, \ph_1, \dots, \ph_{n-1}
$, $\rho \in [0, \sqrt y]$, $\ph_i \in [0, \pi]$ при $i = 1, \dots, n-2$,
$\ph_{n-1} \in [0, 2\pi]$.
$$
\begin{cases}
x_1 = \rho \cos \ph_1\\
x_2 = \rho \sin \ph_1 \cos \ph_2 \\
\dots \\
x_{n-1} = \rho \sin \ph_1 \dots \sin\ph_{n-2} \cos\ph_{n-1}\\
x_n = \rho \sin \ph_1 \dots \sin \ph_{n-1}
\end{cases}
$$
Якобиан замены равен $J = \rho^{n-1} \sin^{n-2} \ph_1 \sin^{n-3} \ph_2
\dots \sin\ph_{n-2}$.
Исходный интеграл распадается в произведение одномерных интегралов
и записывается в виде:
$$
C_n \int\limits_0^{\sqrt y} e^{-\frac {\rho^2}{2}} \rho^{n-1} \, d\rho,
$$
где $C_n$ есть произведение интегралов по $\ph_k$, не зависит от $y$
и вычисляется при $y \to \infty$:
$$
C_n = \frac{1}{\Gamma(\frac n 2){2^{\frac n 2 + 1}}}.
$$
Собирая всё вместе и заменяя $\rho^2 = x$, получаем требуемое.
}

\subsection{Распределение Стьюдента}
Если $Z_0, Z_1, \dots, Z_n$ взаимно независимы и имеют стандартное нормальное
распределение, то распределение случайной величины
$$
\frac{Z_0}{\sqrt{\dfrac{Z_1^2 + \dots + Z_n^2}{n}}} =
\frac{Z_0}{\sqrt{\chi^2_n / n}}
$$
называется \emph{распределением Стьюдента} ($t$-распределением) и
обозначается $t_n$. Плотность распределения Стьюдента
даётся формулой
$$
p_{t_n}(x) = \frac{C_n}{\left( 1 + \frac{x^2}{n} \right)^{\frac{n+1}{2}}},
\qquad \text{где } C_n = \frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac n2)
\sqrt{\pi n}}.
$$
При $n=1$ распределение Стьюдента совпадает с распределением Коши,
имеющим плотность $\dfrac{1}{\pi (1+x^2)}$.

\subsection{Многомерное нормальное распределение}
\subsubsection{Четыре эквивалентных определения}
Пусть $\xi = (\xi_1, \dots, \xi_n)^T$~--- случайный
вектор\footnote{Здесь и далее буква $T$ обозначает
транспонирование. Векторы-столбцы записаны как транспонированные
строки для экономии места. --- \emph{примеч. С. К.}}, $a = (a_1,
\dots, a_n)^T$ --- некоторый постоянный вектор, $x = (x_1, \dots,
x_n)^T \in {\mathbb R}^n$, $A$~--- положительно определённая
матрица $n \times n$.

{\bf Первое определение.} Если плотность (совместного) распределения
вектора $\xi$ имеет вид
$$
p_\xi(x, a, A) = \frac{1}{\left(\sqrt{2\pi}\right)^n} (\det A)^{\frac 12}
\exp \left( - \frac 12 (x-a)^T A (x-a) \right),
$$
то говорят, что $\xi$ имеет \emph{многомерное} ($n$-мерное)
\emph{нормальное распределение}. Вектор $a = \Mf \xi$ ($a_i = \Mf
\xi_i$) называется средним значением, а матрица $\Sigma =
\Sigma_\xi = A^{-1} = \Mf(\xi - a)(\xi - a)^T$~---
\emph{ковариационной матрицей}. $\Sigma = (\sigma_{ij})$,
$\sigma_{ij} = \Mf(\xi_i - a_i)(\xi_j - a_j)$. Ковариационная
матрица является квадратной, симметричной и положительно
определённой. Распределение, заданной такой плотностью,
обозначается $\Norm(a, \Sigma)$.

{\bf Второе определение.} Определим многомерное нормальное
распределение через характеристические функции. $t = (t_1, \dots,
t_n)^T \in {\mathbb R}^n$. $\ph_\xi(t) = \Mf e^{it^T\xi}$~---
\emph{характеристическая функция} случайного вектора. Будем
считать, что $\xi \sim \Norm(a, \Sigma)$, если
$$
\ph_\xi(t) = \exp \left( it^Ta - \frac 12 t^T\Sigma t \right).
$$
В отличие от предыдущего определения, сюда включается случай вырожденной
ковариационной матрицы.

\begin{note} Характеристическая функция вычисляется по плотности, а по
многомерной формуле обращения плотность однозначно восстанавливается по
характеристической функции. Поэтому первое и второе определения равносильны.
\end{note}

{\bf Третье определение.} Если любая линейная комбинация компонент вектора
$\xi$ имеет (одномерное) нормальное распределение, то $\xi$ имеет многомерное
нормальное распределение.

{\bf Четвёртое определение.} $\xi$ имеет $\Norm(a; \Sigma)$, если для
некоторого $k$ существует набор независимых и имеющих стандартное нормальное
распределение случайных величин $\eta_1, \dots, \eta_k$ и существует
$n\times k$-матрица $A$ такие, что $\xi = A\eta$, $\eta = (\eta_1, \dots,
\eta_k)^T$. На самом деле $k$ есть ранг матрицы $\Sigma$.
На практике обычно считается, что $k=n$, а $a$ --- нулевой вектор.

Третье и четвёртое определения будут использоваться в теории
случайных процессов.

\subsubsection{Свойства многомерного нормального распределения}
% FIXME FIXME FIXME


\subsubsection{Лемма Фишера}
\begin{lemma}[Фишер]
Пусть $\eta_1, \dots, \eta_n$ --- независимые (одномерные) случайные
величины с $\Norm(a, \sigma^2)$. Тогда:
\begin{nums}{-2}
\item существуют ортогональная матрица $C$ и случайный вектор $\xi$
такие, что $\xi = C\eta$, $\Mf \xi_1 = \sqrt n a$, $\Mf \xi_i = 0$
при $i>1$ и $\Df \xi_i = \sigma^2$ при всех $i$;
\item $\overline \eta = \dfrac{\eta_1 + \dots + \eta_n}{n}$ и
$\sum\limits_{i=1}^{n} (\eta_i - \overline\eta)^2$ независимы;
\item
$$
\frac{1}{\sigma^2} \sum (\eta_i - \overline\eta)^2 \sim \chi^2_{n-1},
\qquad
\frac{\sqrt n (\overline\eta - a)}{s} \sim t_{n-1},
\quad
\text{где } s^2 = \frac{1}{n-1} \sum_{i=1}^n (\eta_i - \overline\eta)^2.
$$
\end{nums}
\end{lemma}

\begin{proof}
\begin{nums}{-2}
\item Положим $c_{11} = c_{12} = \dots = c_{1n} = c_1 = \dfrac{1}{\sqrt n}$.
$\sum\limits_{j=1}^n c_{1j}^2 = 1$. $\xi_1 = \sum\limits_{j=1}^n c_{1j}
\eta_j = \sqrt n \overline\eta$, $\Mf \xi_1 = \sqrt n a$, $\Df \xi_1 =
n \frac{\sigma^2}{n} = \sigma^2$. Остальные $c_{ij}$ ($2 \leqslant i
\leqslant n$, $1 \leqslant j \leqslant n$) подбираются из условий:
$\sum\limits_{j=1}^n c_{ij}^2 = 1$ (при всех $i > 1$) и
$\frac{1}{\sqrt n} \sum\limits_{j=1}^{n} c_{ij} = 0$. Из этих условий
получаем, что матрица $C = (c_{ij})$ ортогональна, и при $i>1$
$\Mf \xi_i = 0$ и $\Df \xi_i = \sigma^2$.

\item В силу ортогональности $C$ $\sum\limits_{i=1}^n \xi_i^2 =
\sum\limits_{i=1}^n \eta_i^2$. Отсюда $\sum\limits_{i=2}^n \xi_i^2 =
\sum\limits_{i=1}^n \eta_i^2 - \xi_1^2 = \sum\limits_{i=1}^n \eta_i^2 -
n {\overline\eta}^2$, то есть $\sum\limits_{i=1}^n
(\eta_i - \overline\eta)^2 = \sum\limits_{i=2}^n \xi_i^2$ --- не зависит
от $\overline\eta = \xi_1 / \sqrt n$.

\begin{note} Если $\eta_1, \dots, \eta_n$ независимы и имеют одинаковое
нормальное распределение, то $\overline\eta$ и $\frac 1n \sum (\eta_i -
\overline\eta)^2$ независимы. На самом деле это характеристическое
свойство нормальных распределений.
\end{note}

\item
$Z_i = \dfrac{\xi_i}{\sigma} \sim \Norm(0,1)$ при $i>1$, откуда
$$
\frac 1 {\sigma^2} \sum_{i=1}^n (\eta_i - \overline\eta)^2 =
\sum_{i=2}^n \frac{\xi_i^2}{\sigma^2} = Z_2^2 + \dots + Z_n^2
\sim \chi^2_{n-1},
\quad
s^2 = \frac{1}{n-1} \sum_{i=1}^n (\eta_i - \overline\eta)^2,
$$
откуда $\dfrac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}$.

$\dfrac{\overline\eta - a}{\sigma/{\sqrt n}} \sim \Norm(0,1)$,
$\dfrac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}$ и эти случайные
величины независимы по предыдущему пункту, поэтому
$$
\frac{\sqrt n (\overline\eta - a)}{s} =
\frac{\frac{\overline\eta - a}{\sqrt{\sigma^2/n}}}{\sqrt{s^2/\sigma^2}}
\sim t_{n-1}.
$$
\end{nums}
\end{proof}

\subsubsection{Доверительные интервалы для параметров нормального
распределения II}

$x_1, \dots, x_n$ --- повторная выборка из $\Norm(a, \sigma^2)$.

{\bf I. Доверительный интервал для $a$ при неизвестном $\sigma^2$.}
$\Mf \overline\xi = a$, $\Df \overline\xi = \sigma^2/n$. По лемме
Фишера $\dfrac{\overline x - a}{\sqrt{s^2/n}}$ имеет распределение
Стьюдента с $n-1$ степенью свободы. Зафиксируем $\alpha \in (0,1)$
$t = t_{1-\frac\alpha 2}(n-1)$ --- $(1-\frac\alpha 2)$-квантиль $t_{n-1}$.
$$
\Pf_{a,\sigma^2} \left( \left| \frac{\overline\xi - a}{s} \sqrt n \right|
\leqslant t \right) = 1 - \alpha
\qquad
\text{(на самом деле распределение вероятностей не зависит от параметров),}
$$
поэтому $\left[ \overline x - t \dfrac s {\sqrt n}, \overline x +
t \dfrac s {\sqrt n} \right]$~--- доверительный интервал для $a$ с
доверительной вероятностью $\alpha$.

{\bf II. Доверительный интервал для $\sigma^2$ при неизвестном $a$.}
По лемме Фишера $\dfrac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}$.
Пусть задано $\alpha$. $g_{\frac\alpha 2} (n-1)$ есть $\frac\alpha 2$-квантиль,
а $g_{1 - \frac\alpha 2} (n-1)$ --- $1 - \frac\alpha 2$-квантиль
распределения хи-квадрат с $n-1$ степенью свободы. С помощью этих
квантилей строится доверительный интервал для $\sigma^2$.

\section{Проверка статистических гипотез}
\subsection{Проверка гипотез о параметрах нормального распределения}
Пусть дана выборка из $\Norm(a, \sigma^2)$. $H_0$: $a = a_0$
($a_0$ фиксировано); $H_1$: $a \ne a_0$. Пусть $\sigma^2$ известно и
равно $\sigma^2_0$.
Тогда (см. выше) $\left[ \overline x - u_{1-\frac\alpha 2}
\dfrac{\sigma^2_0}{\sqrt n}, \overline x + u_{1-\frac\alpha 2}
\dfrac{\sigma^2_0}{\sqrt n} \right]$ --- доверительный интервал для
$a$ с доверительной вероятностью $1 - \alpha$. Критерий:
если $a_0$ принадлежит нашему доверительному интервалу, то $H_0$
принимается, иначе отклоняется. Вероятность ошибки первого рода
(отклонить $H_0$, когда она верна) равна $\alpha$. Вероятность
ошибки второго рода явно не вычисляется (гипотеза $H_1$ --- не конкретная).

Если альтернативная гипотеза имеет вид $H_1$: $a > a_0$, то вместо
рассмотренного ранее двустороннего используем \emph{односторонний критерий}:
оставляем только верхнюю критическую границу.

Аналогично при помощи доверительных интервалов строятся критерии для
$a$ при неизвестном $\sigma^2$ и для $\sigma^2$ (при известном или
неизвестном $a$).

Критерии, в которых $H_0$ задаёт конкретное распределение вероятностей,
а $H_1 = \lnot H_0$, называются \emph{критериями согласия}.

%%%%%%%
%%%%%%%
%% Lect. 13 by Alexander V. Kharitonov
%%%%%%%
%%%%%%%

% FIXME: remove these comments

%\pagebreak
%{\Huge Лекция \textnumero~13}

% 25-Nov-2006
%\documentclass{article}
%\usepackage{amssymb,amsthm,amsmath}
%\usepackage[T2A]{fontenc}
%\usepackage[utf-8]{inputenc}
%\usepackage[russian]{babel}
%\newcommand{\eps}{\varepsilon}

%\newtheorem{theorem}{Теорема}
%\newtheorem{lemma}{Лемма}

% FIXME: переместить в преамбулу
\let\le=\leqslant
\let\leq=\leqslant
\let\ge=\geqslant
\let\geq=\geqslant
%\let\preceq=\preccurlyeq
%\let\succeq=\succcurlyeq

\newcommand{\perems}[2]{#1_1,\dots,#1_{#2}}
\newcommand{\perem}[3]{#1_{#2},\dots,#1_{#3}}

%\begin{document}
%%В случае одностороннего критерия можно использовать не доверительный интервал, а доверительные границы.

\subsection{Проверка гипотезы однородности нормальных выборок}
Пусть имеется две выборки $\perems xm; \perems yn$, порожденные соответственно независимыми в совокупности случайными величинами
$\perems \xi m, \perems \eta n$, причём $\perems \xi m \thicksim \Norm(a_1,\sigma_1^2), \perems \eta n \thicksim \Norm(a_2,\sigma_2^2)$ и все параметры $a_1, \sigma_1^2, a_2, \sigma_2^2$ неизвестны.

$H_0: a_1 = a_2, \sigma_1 = \sigma_2$ --- \emph{гипотеза однородности}.
$H_1 = \lnot H_0$. Разобьём задачу проверки гипотезы однородности на две задачи: \par\noindent
I. $H'_0: \sigma_1^2 = \sigma_2^2$ (при любых $a$), $H'_1: \sigma_1^2 \neq \sigma_2^2$ (если $H'_0$
отклоняется, то отклоняем $H_0$; в противном случае движемся дальше).
 \par\noindent
II. Если $H'_0$ принимается, то $H''_0: a_1 = a_2, H''_1: a_1 \neq a_2$. Теперь $H_0$ принимается
тогда и только тогда, когда принимается $H''_0$.

\subsubsection{О распределении Фишера\ч Снедекора ($F$-распределении)}
Случайная величина $F_{m,n} = \dfrac{\chi^2_{m}/m}{\chi^2_{n}/n},$
где $\chi^2_m$ и $\chi^2_n$ независимы, имеет \emph{$F$-распределение} с $m$ и $n$ степенями свободы.
Плотность:

$$
p_{m,n}(x) = \begin{cases}
\frac{1}{B(m/2,n/2)}(\frac{m}{n})^{m/2}x^{n/2-1}(1 + \frac{m}{n}x)^{-\frac{m+n}{2}},& x \ge 0\\
0,& x \leq 0
\end{cases}
$$

Функция распределения:
$$
P(F_{m,n} \leq x) = \frac{I_{\frac{m}{n}x}(m/2,n/2)}{1 + \frac{m}{n}x}, \quad
\text{где $I_x(a,b)$ - неполная $\beta$-функция с параметрами $a$ и $b$.}
$$

\subsubsection{Критерий Фишера равенства дисперсий}

Рассмотрим две статистики:
$$
s_1^2 = \frac{1}{m-1}\sum_{k=1}^m (x_k - \bar x)^2, \qquad
s_2^2 = \frac{1}{n-1}\sum_{j=1}^n (y_j - \bar y)^2.$$

Отношение $\hat F_{m,n} = \dfrac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2}$ при условии $H'_0$
($\sigma_1^2 = \sigma_2^2$) равно $s_1^2 / s_2^2$ и
%
%отношение при условии $H'_0$($\sigma_1^2 = \sigma_2^2 = \sigma^2$)
%$$
%\frac{\frac{s_1^2}{\sigma_1^2}}{\frac{s_2^2}{\sigma_2^2}} = \frac{s_1^2}{s_2^2}
%$$
%
%$\hat F_{m,n} = s_1^2/s_2^2$ при $H'_0$
имеет $F$-распределение с $(m-1)$ и $(n-1)$ степенями свободы:

$$
\frac{s_1^2}{s_2^2} = \frac{\frac{(m-1)s_1^2}{\sigma^2}\cdot\frac{1}{m-1}}{\frac{(n-1)s_2^2}{\sigma^2}\cdot\frac{1}{n-1}} = \frac{\chi^2_{m-1} \cdot \frac{1}{m-1}}{\chi^2_{n-1} \cdot \frac{1}{n-1}} = F_{m-1,n-1},
$$
где предпоследнее равенство справедливо в силу леммы Фишера.

Для заданного $\alpha$ находим квантили $f_{\frac{\alpha}{2}}(m-1,n-1)$ и $f_{1-\frac{\alpha}{2}}(m-1,n-1)$ $F$-распределения с $(m-1)$ и $(n-1)$ степенями свободы и берем их в качестве критических значений.

{\bf Критерий Фишера.} Если $f_{\frac{\alpha}{2}} \leq \hat
F_{m,n} \leq f_{1-\frac{\alpha}{2}}$, то $H'_0$ принимается, иначе
отвергается. Вероятность ошибки первого рода в точности равна
$\alpha$. (Можно использовать и односторонний критерий)


\subsubsection{Критерий Стьюдента равенства средних значений при условии равенства дисперсий}

%\subsection*{II.(о равенстве средних значений при условии равенства дисперсий)}

Если $H'_0$ отклоняется, то отклоняется и гипотеза однородности $H_0$. Пусть теперь $H'_0$
принята (т.е. $\sigma_1^2 = \sigma_2^2 = \sigma^2$). Будем проверять гипотезу
$H''_0: a_1 = a_2$ ($H''_1 = \lnot H''_0$).

%\subsubsection*{Критерий Стьюдента}

Мы знаем, что $\Mf (\bar x - \bar y) = a_1 - a_2$ и $\Df (\bar x - \bar y) = \sigma^2(1/m + 1/n)$
($\bar x$ и $\bar y$ независимы и $\bar x - \bar y \sim \Norm(a_1 - a_2, \sigma^2(1/m + 1/n))$.)

$$
\frac{(\bar x - \bar y) - (a_1 - a_2)}{\sqrt{\sigma^2(1/m + 1/n)}} \sim \Norm(0,1),
$$
но $\sigma^2$ неизвестно. Подставим для неё несмещённую оценку
$$
\hat \sigma^2 = \frac{(m-1)s_1^2 + (n-1)s_2^2}{m+n-2}
$$

При условии $H''_0$: $a_1 - a_2 = 0$ статистика $\hat t_{m+n-2} = \frac{\bar x - \bar y}{\sqrt{\sigma^2(1/m+1/n)}}$ имеет распределение Стьюдента
с $m+n-2$ степенями свободы (следует из леммы Фишера).

Зададим $\alpha \in (0,1)$. $t_{\alpha/2} = - t_{1 - \alpha/2}$ --- соответствующие квантили
распределения Стьюдента (равенство имеет место в силу симметричности распределения).

{\bf Критерий Стьюдента.} Если $|\hat t_{m+n-2}| > t_{1-\alpha/2}$, то $H''_0$ отклоняется,
иначе~--- принимается. Вероятность ошибки первого рода равна $\alpha$.
Если при этом $H''_0$ принимается, то принимается и исходная гипотеза однородности $H_0$.

\subsection{Дисперсионный анализ однофакторной модели}

Пусть $\perems xn$~--- выборка, где все элементы получены независимо. %Выделяем $k, k \geq 3$ подвыборок (a priori).
A priori разобьём её на $k, k \geq 3$ подвыборок: $x_{11}, \dots, x_{n_11}; \dots; x_{1k}, \dots,
x_{n_kk}$, $n = n_1 + \dots + n_k$. $\bar x_i = (x_{1i} + \dots + \bar{n_ii}) / {n_i}$ --- средние
значения по подвыборкам,
$\bar{\bar x} = (x_{11} + \dots + x_{n_kk}) / n$ --- среднее значение.

Эти данные удобно представлять в виде \emph{таблицы дисперсионного анализа}:

\begin{tabular}{|c|c|c|c|c|}
\cline{1-4}
1 & 2 & $\ldots$ & $k$ \\\cline{1-4}
$x_{11}$ & $x_{12}$ & $\ldots$ & $x_{1k}$  \\
$x_{21}$ & $x_{22}$ & $\ldots$ & $x_{2k}$  \\
$\vdots$ & $\vdots$ &          & $\vdots$ \\
\hline
$n_1$    & $n_2$    & $\ldots$ & $n_k$ & $n$\\\hline
$\bar x_1$ & $\bar x_2$ & $\ldots$ & $\bar x_k$ & $\bar{\bar x}$\\\hline
\end{tabular}

Предполагается, что существуют постоянные (но неизвестные нам)
$\perems ak$ такие, что $x_{ij} = a_j + e_{ij}$ при всех $i$ и
$j$, где $e_{ij}$~--- случайные ошибки, взаимно независимые при
разных $i$ и $j$, $\Mf e_{ij} = 0$, $\Df e_{i,j} = \sigma^2$
(считаем измерения равноточными), $\sigma^2$ неизвестно.

Эта модель называется \emph{однофакторной}. Считается, что на эксперименты, порождающие
данные в столбцах, отличаются влиянием некоторого фактора;
$1,2,\ldots,k$~--- номера уровней фактора, $a_j$~--- характеристики уровня фактора.
Различия между элементами одних столбцов ,,чисто случайны''.
Существуют и многофакторные модели.

$H_0$: гипотеза об отсутствии фактора ($a_1 = \ldots = a_k$) (гипотеза однородности
подвыборок), $H_1 = \lnot H_0$.
Если $e_{ij} \sim \Norm(0,\sigma^2) \Leftrightarrow x_{ij} \sim \Norm(a_j, \sigma^2),$
то $H_0$ есть гипотеза о равенстве средних значений $k$ нормальных выборок.
[Случай, когда $x_{ij}$ имеют непрерывные распределения, существенно отличающиеся от нормального,
относится к непараметрическому анализу (ранговые критерии, etc). Этим мы заниматься не будем.]

\emph{Изменчивость данных}~--- отклонение от среднего (измеряется как выборочная дисперсия)
$$
\sum_{i,j} (x_{ij} - \Bar{\Bar x})^2 = \sum_{i,j} (x_{ij} - \bar
x_j)^2 + \sum_{i,j}(\bar x_j - \Bar{\Bar x})^2$$
%где $\Bar{\Bar x} = 1/n \sum_{i,j} x_{ij}$~--- полное среднее, $\bar x_j = 1/n_j \sum_{i=1}^{n_j} x_{ij}$ - среднее по $j$-му столбцу.

$$
\sum_{i,j} x_{ij} = n \Bar{\Bar x} = \sum_{j=1}^k n_j \bar x_j
$$

Найдём распределение первой суммы. По лемме Фишера
$$
\frac{\sum\limits_{i=1}^{n_j} (x_{ij} - \bar x_j)^2}{\sigma^2} = \chi^2_{n_j - 1},%\quad \text{(по лемме Фишера)}
$$
откуда
$$
\frac{1}{\sigma^2}\sum_{i,j} (x_{ij} - \bar x_j)^2 = \sum_{j=1}^k \chi^2_{n_j - 1} = \chi^2_{\sum_j (n_j - 1)} = \chi^2_{n-k},
$$
т.к. все $\chi^2_{n_j - 1}$ у нас независимы.

$\bar x_j$ не зависит от $\sum(x_{ij} - \bar x_j)^2$ по лемме Фишера, следовательно, $\sum_j(x_{ij} - \bar x_j)^2$ и
$\sum_j (\bar x_j - \Bar{\Bar x})^2$ независимы.

Если $H_0$ верна, то $\frac 1{\sigma^2} \sum (x_{ij} - \Bar{\Bar x})^2 = \chi^2_{n-1}$:
$$
\frac 1{\sigma^2} \sum_j n_j(\bar x_j - \Bar{\Bar x})^2 = \chi^2_{k-1} \text{(по лемме Фишера)}
$$
$$\chi^2_{n-1} = \chi^2_{n-k} + \chi^2_{k-1} \quad \text{(эти величины независимы)}
$$
$\hat \sigma_1^2 = \frac{1}{n-k} \sum_{i,j}(x_{ij} - \bar x_j)^2$~--- оценка, не зависящая от гипотезы $H_0$,
$\hat \sigma_2^2 = \frac{1}{k-1} \sum_{j} n_j (\bar x_j - \Bar\Bar x)^2$~--- реагирует на $H_0$\\
%$$\frac{\hat \sigma_2^2}{\hat \sigma_1^2} = \hat F_{k-1,n-k}\quad\text{--- дисперсионное отношение}$$
Дисперсионное отношение $\dfrac{\hat \sigma_2^2}{\hat \sigma_1^2}$
имеет распределение Фишера\ч Снедекора с $k-1,\,n-k$ степенями
свободы.

{\bf Правосторонний критерий Фишера.} Пусть $\alpha:\quad
f_{1-\alpha}(k-1,n-k)$~--- $(1-\alpha)$-квантиль $F$-распределения
с $k-1,n-k$ степенями свободы. Если $\hat F_{k-1,n-k} >
f_{1-\alpha}$, то $H_0$ отклоняется (фактор существует); иначе~---
принимается.

\subsection{Множественные сравнения}
Рассмотрим случай, когда в дисперсионном анализе однофакторной модели гипотеза отсутствия фактора
($H_0$) отвергнута. Проведём более тонкое исследование.
\subsubsection{Парное сравнение}
Проверяем гипотезу $H_{0(j,l)}$: $a_j = a_l \Leftrightarrow a_j - a_l = 0$;  $H_1 = \lnot H_0$.
(однородности двух подвыборок).

Строим доверительный интервал для разности $a_j - a_l$. $\bar x_j, \bar x_l$~--- оценки для $a_j, a_l$.

$$
\frac{(\bar x_j - \bar x_l) - (a_j - a_l)}{\sqrt{\hat \sigma_1^2 \left( \frac{1}{n_j} + \frac{1}{n_l} \right)}} = \hat t_{n-k}
$$
$\hat t_{n-k}$ имеет распределение Стьюдента с $(n-k)$ степенями свободы, фиксируем $\alpha$, строим доверительный интервал для $a_j - a_l$
стандартным способом: $$\left[ (\bar x_j - \bar x_l) - t_{1-\frac\alpha 2}(n-k)
\sqrt{\hat\sigma_1^2 \left(\frac 1 {n_j}  + \frac 1 {n_l}\right)},\:
(\bar x_j - \bar x_l) + t_{1-\frac\alpha 2}(n-k)
\sqrt{\hat\sigma_1^2 \left(\frac 1 {n_j}  + \frac 1 {n_l}\right)} \, \right].$$


{\bf Критерий:} если $0$ лежит внутри этого интервала, то
принимаем $H_{0(j,l)}$, иначе~---  отклоняем. Таким образом можем
выявить излишние уровни факторов.

\subsubsection{Собственно множественные сравнения}
$$
\Psi = \sum_{j=1}^k c_j a_j,\quad \sum_{j-1}^k c_j = 0
$$

$\Psi$~--- ,,\emph{сравнение}'' $\perems ak$ (,,контраст''),
парное сравнение~--- частный случай такого. Оценим: $\hat \Psi =
\sum_{j=1}^k c_j$
$$
\Mf \hat\Psi = \Psi, \qquad \Df \hat\Psi = \sigma^2 \sum_{j=1}^k c^2_j/n_j
$$
$$
\frac{\hat \Psi - \Psi}{\sqrt{\hat \sigma^2_1 \sum_{j}
\frac{c^2_j}{n_j}}} = \hat t_{n-k}
$$

Далее стандартным образом строим доверительный интервал для $\Psi$ ($\hat t_{n-k}$ имеет распределение Стьюдента).

\subsection{Критерий Пирсона ($\chi^2$)}
%\subsubsection{Схема Бернулли с параметром $p \in (0,1)$}
\subsubsection{Биномиальный критерий}
Рассмотрим схему Бернулли с параметром $p\in(0,1)$.
Проверяем гипотезу $H_0: p = p_0,\quad H_1: p \neq p_0$.
$T(x) = x_1 + \ldots + x_n$~--- достаточная статистика, $\alpha$~--- уровень значимости (вероятность
ошибки первого рода), $m^*_\alpha$~---
 некоторая критическая граница.\par\noindent
{\bf Критерий:} если $T(x) > m^*_\alpha$, то $H_0$ отклоняем, иначе принимаем.
Запишем вероятность ошибки первого рода:
$
\sum\limits_{m > m^*_\alpha} \Cb_n^m p_0^m (1-p_0)^{n-m} \leq \alpha
$

Заменим точный критерий на приближенный с помощью ЦПТ.
$$
\frac{T(\xi) - np}{\sqrt{np(1-p)}} \radnrai \Norm(0,1).
$$
Вероятность ошибки первого рода:
$$
\Pf_{p_0}\left( \frac{T(\xi) - np_0}{\sqrt{np_0(1-p_0)}} > \frac{m^*_\alpha - np_0}{\sqrt{np_0(1-p_0)}} \right) = \alpha
$$
$$
1 - \Phi\left(\frac{m^*_\alpha - np_0}{\sqrt{np_0(1-p_0)}}\right) = \alpha
$$
$$
u_{1-\alpha} = \frac{m^*_\alpha - np_0}{\sqrt{np_0(1-p_0)}} \text{~--- $(1-\alpha)$-квантиль $\Norm(0,1)$}
$$
Из последнего условия находится граница $m^*_\alpha = np_0 + u_{1-\alpha} \sqrt{np_0(1-p_0)}$.

%%%
%%% Lect. 14 by A.V.Kharitonov
%%%

%\subsection*{О проверке гипотез о параметре биномиального распределения}

%Рассмотрим схему Бернулли и статистические гипотезы о значении вероятности успеха в ней. $H_0: p = p_0$~--- простая гипотеза (задаёт конкретное распределение вероятностей), $H_1 = \lnot H_0$~--- сложная гипотеза.
%
%\subsubsection*{Биномиальный критерий}
% Если $\sum_{i=1}^n x_i > m^{*}_\alpha$, то $H_0$ отклоняется. При этом $m^*_\alpha$ вычисляется исходя из заданного уровня
%значимости $\alpha$.
%С помощью ЦПТ можно получить приближённое значение $m^*_\alpha = n p_0 + u_{1-\alpha} \sqrt{np_0(1-p_0)}$, где $u_{1-\alpha}$~--- $(1-\alpha)$-квантиль
%$\Norm(0,1)$. С таким значением $m^*_\alpha$ вероятность ошибки первого рода $\to \alpha (n \to \infty)$.

\subsubsection{Критерий $\chi^2$ для схемы Бернулли (предисловие к критерию Пирсона)}
Есть и другой критерий. Пусть $m = x_1 + \ldots + x_n$. $\Pf(S_n = m) = \Cb^m_n p^m (1-p)^{n-m}$.
Рассмотрим статистику
$$
\widehat X^2_1 = \frac{(m-np_0)^2}{np_0} + \frac{(n-m-n(1-p_0))^2}{n(1-p_0)}
$$

$\widehat X^2_1$ есть сумма относительных квадратичных отклонений эмпирических результатов от ожидаемых. В условиях $H_0$ $\Mf S_n = np_0$.
$$
\widehat X^2_1 = \frac{(m-np_0)^2}{np_0} + \frac{(m-np_0)^2}{n(1-p_0)} = \frac{(m-np_0)^2}{np_0(1-p_0)} = \left( \frac{m-np_0}{\sqrt{np_0(1-p_0)}} \right)^2
$$

Следовательно, так как в силу ЦПТ $\frac{S_n - np_0}{\sqrt{np_0(1-p_0)}} \rad \Norm(0,1)$,
то $\widehat X^2_1$, как квадрат этой величины,
сходится по распределению к $\chi^2_1$ при условии $H_0$.
$\widehat X^2_1$ называется \emph{статистикой Пирсона (хи-квадрат)}.
Критерий строится стандартным образом при помощи квантилей хи-квадрат распределения.

\subsubsection{Полиномиальный критерий}
Обобщим наш критерий. Будем проверять гипотезу о том, что данная
выборка $x_1,\ldots,x_n$ имеет полиномиальное распределение с $s$
исходами и заданными вероятностями: $H_0: p_1 = p_1^0, \ldots, p_s
= p_s^0$, $H_1 = \lnot H_0$. Обозначим за $m_k$ количество исходов
типа $k$ и составим статистику a l\`{a} $\widehat X^2_1$.

{\bf Статистика Пирсона:}
$$\widehat X^2_{s-1} = \sum_{k=1}^s \frac{(m_k - np_k^0)^2}{np_k^0}$$

Далее мы покажем, что в условии $H_0$ и $n \to \infty$ $\widehat X^2_{s-1}$ имеет
асимптотическое распределение $\chi^2_{s-1}$ (теорема Пирсона).

{\bf Критерий Пирсона} ($\chi^2$) о данном распределении в полиномиальной модели.
Пусть $g_{1-\alpha}$~--- $(1-\alpha)$-квантиль $\chi^2_{s-1}$. Если $\widehat X^2_{s-1} >
g_{1-\alpha}(s-1)$, то $H_0$ отклоняется, иначе принимается.
Асимптотическая ошибка первого рода, как будет следовать из теоремы, которую мы сейчас докажем,
будет равна $\alpha$.

\subsubsection{Теорема Пирсона}

Пусть $\xi_1,\ldots,\xi_n$ взаимно независимы и одинаково распределены: $\xi_i = (\xi_{i_1},\ldots,\xi_{i_s})^T$~--- случайные векторы, причем
$
\Pf(\xi_{i_1} = a_1, \ldots, \xi_{i_s} = a_s) = p_1^{a_1} \cdot \ldots \cdot p_s^{a_s}$ для всех $i$,
$p_i > 0$, $\sum_i p_i = 1$; $a_i \in \{ 0,1 \}$, $\sum_i a_i = 1$ ($\xi_i$~--- вектор из 0 и 1 с ровно одной единицей). Очевидно, что $\Mf \xi_{i_k} = p_k$, $\Df \xi_{i_k} = p_k(1-p_k)$,
$\Mf \xi_i = (p_1,\ldots,p_s)^T = \Vec{p}$. Элементы ковариационной матрицы
$\Sigma = \Sigma_{\xi} =  \Mf(\xi - \Mf\xi)(\xi - \Mf\xi)^T = ( \sigma_{jl} )$ имеют вид
$\sigma_{jl} = \Mf (\xi_{i_j} - p_j)(\xi_{i_l} - p_l) = - p_j p_l$ при $j \ne l$ (в общем
случае $\sigma_{jl} = p_j \delta_{jl} - p_j p_l$).
%$$\Sigma_xi = \Mf (\xi - \Mf \xi)(\xi - \Mf \xi)^t$$
Рассмотрим случайный вектор
$S_n = \xi_1 + \ldots + \xi_n = (\mu_1,\ldots,\mu_s)^T$, где $\mu_k = \sum_{i-1}^n \xi_{i_k}$~-- сумма независимых случайных величин. $\Mf \mu_k = np_k$, $\Df \mu_k = np_k(1-p_k)$,
$\Mf S_n = (np_1,\ldots,np_s)^t = n\Vec{p}$; $\Sigma_{S_n} = n \Sigma_\xi$ (докажите это!).

Распределение $S_n$ называется \emph{полиномиальным} распределением. Найдём распределение статистики

$$\widehat X^2_{s-1} = \sum_{k=1}^s \frac{(\mu_k - np_k)^2}{np_k},\quad \Mf \widehat X^2_{s-1} = s-1%, \quad \Df \widehat X^2_{s-1} = ...$$
$$

\begin{theorem}[Пирсон]
$\widehat X^2_{s-1} \rad \chi^2_{s-1}$
\end{theorem}

\begin{proof}
Перейдём к $S_n^* = \frac{S_n - n \Vec{p}}{\sqrt n}, \Mf S_n^* = 0, \Sigma_{S_n^*} = \Sigma_\xi = \Sigma$.
Доказательство будет основано на одном из вариантов многомерной ЦПТ, а именно
\fbox{ \strut $S_n^* \rad Z \sim \Norm(0,\Sigma)$ }
[Севастьянов, гл.~11, \S~46, теорема 7]

%\begin{proof}
Положим $\Delta_k = \dfrac{\mu_k - np_k}{\sqrt{np_k}}$, $k = 1,
\ldots, s$; $\Delta = (\Delta_1,\ldots,\Delta_s)^T$.
$\sum\limits_{k=1}^s \sqrt{p_k} \Delta_k = \dfrac{(n - n)}
{\sqrt{n}} = 0$, поэтому $s$-мерное распределение $\Delta$~---
вырожденное.

$S_n^*$ = $B\Delta$, $B = \diag (\sqrt{p_1},\ldots,\sqrt{p_s})$.
$\Delta = B^{-1} S_n^*$, поэтому $\Mf \Delta = 0$. Ковариационная матрица
$\Sigma_{\Delta} = \Mf (\Delta\Delta^T) = B^{-1} \Mf(S_n^* S_n^{*T}) B^{-1} = B^{-1} \Sigma B^{-1}$.
$(\Sigma_{\Delta})_{ij} = \delta_i^j - \sqrt{p_i p_j}$ (на главной диагонали $1 - p_1, \dots,
1 - p_k$, вне неё: $- \sqrt{p_k p_j}$).

Положим $\eta = C \Delta$, где $C$~--- ортогональная матрица с заданной первой строкой: $c_{1k} = \sqrt{p_k}$.
$\eta = (\eta_1,\ldots,\eta_s)^T$, $\Mf \eta = 0$; $\eta_1 = \sum_{k=1}^s c_{1k} \Delta_k = 0$.
Докажем, что $(\Sigma_\eta)_{ij} = \delta_i^j$, если $i,j \geqslant 2$, и $0$ иначе. При $j, l \geqslant 2$
имеем:

\begin{multline*}
\Mf (\eta_j \eta_l) =
\Mf \left(\sum_k c_{jk} \Delta_k \cdot \sum_k c_{lk} \Delta_k \right) =
\Mf \left(\sum_{s,t} c_{js} c_{lt} \Delta_s \Delta_t \right) =
\sum_{s = 1}^m c_{js} c_{ls} \Mf \Delta_s^2 + \sum_{s \neq t} c_{js} c_{lt}
\Mf (\Delta_s \Delta_t) =\\=
 \sum_{s=1}^m c_{js} c_{ls} (1 - p_s) -
 \sum_{s \neq t} c_{js} c_{lt} \sqrt{p_s p_t}  = \sum_s c_{js} c_{ls}
 - \left( \sum_s c_{js} \sqrt{p_s} \right) \left( \sum_t c_{lt} \sqrt{p_t} \right) =
  \sum_s c_{js} c_{ls} = \delta_{jl}
\end{multline*}

Последнее равенство верно в силу ортогональности строк матрицы $C$.
$$\widehat X^2_{s-1} = \sum_{k=1}^s \Delta_k^2 = \sum_{k=1}^s \eta_k^2 = \eta_2^2 + \ldots + \eta_s^2\quad(\eta_1 = 0)$$
Мы доказали, что при $i > 2$ $\eta_i$ не коррелируют. Если докажем, что $\eta_i \rad \Norm(0,1)$ (и к
тому же они независимы, а не просто не коррелируют --- тут-то нам и потребуется
многомерная ЦПТ!), то получим $\chi^2_{s-1}$. При $j \geqslant 2$
$$\eta_j = \sum_{k=1}^s c_{jk} \Delta_k = \sum_{i=1}^n \sum_{k=1}^s \frac{c_{jk}}{\sqrt{np_k}}(\xi_{i_k} - p_k) = \sum_{i=1}^n \eta_{ij}.$$
При фиксированном $j$ и разных $i$ $\eta_{ij}$ независимы и одинаково распределены, $\Mf \eta_{ij} = 0,\quad \Df \eta_j = 1$.
$$\eta_j = \sum_i \eta_{ij}, \quad \Mf \eta_{ij} = 0, \quad \Df \eta_{ij} = \frac1n\quad \text{докажите это!}$$
По одномерной ЦПТ  $\eta_j \rad \Norm(0,1)$ для всех $j \geqslant 2$.

Памятуя, что $\eta = (CB^{-1})S_n^*$, а $S_n^*$ сходится по распределению к $\Norm(0, \Sigma)$ (по формуле
в рамочке), получаем, что $\eta \rad \Norm(0, E')$, где $E'$ --- единичная матрица без верхней
левой единицы ($\eta_1 = 0$), причём компоненты его некоррелированы, а потому независимы (это
свойство многомерного нормального распределения). Поэтому
${\widehat X}^2_{s-1} = \eta_2^2 + \ldots + \eta_s^2 \rad \chi^2_{s-1}$, что и требовалось.
\end{proof}

\subsubsection{Критерий $\chi^2$}
Пусть $x_1,\ldots,x_n$~--- повторная выборка, $\xi_i$ одинаково распределены с ф.р. $\mathcal{F}(x)$.\\
$H_0: \mathcal{F}(x) = F_0(x), \quad H_1 = \lnot H_0.$

Разобьем $\mathbb{R}$ на $s$ интервалов $\Delta_k = (t_{k-1},t_k]$ произвольным образом. Тогда наша задача сведется к проверке гипотезы о данном
распределении вероятностей успеха в полиномиальной модели, где $m_k$~--- число $x_i$, попавших в $\Delta_k$, $p_k^0$~--- вероятностная мера $\Delta_k$ (при условии $H_0$), и $\widehat X^2_{s-1}$, определенное ранее, будет иметь асимптотическое распределение $\chi^2_{s-1}$

{\bf Замечания}. Задачи, решаемые с помощью $\chi^2$:
\begin{itemize}
\item $F_0(x)$ может быть задана с точностью до $r$ параметров; тогда статистика Пирсона, в которой $p_k^0$ вычислены с подстановкой вместо неизвестных параметров статистик для них, будет иметь асимптотическое распределение $\chi^2_{s-1-r}$.
\item Критерий $\chi^2$ можно использовать для проверки гипотезы однородности. Пусть есть две полиномиальные схемы с $n$ и $l$ испытаниями, с результатами испытаний
$(m_1,\ldots,m_s)$, вероятностями $(p_1,\ldots,p_s)$ и $(m'_1,\ldots,m'_s)$ и $(p'_1,\ldots,p'_s)$ соответственно.

$H_0: p'_i = p_i, \quad H_1 = \lnot H_0$.
При условии $H_0$ выборки объединяются и оцениваются общие значения $p_i = p'_i$: $\hat p_i = \frac{m_i + m'_i}{n+l}$ (по методу МП). Тогда
$$
\widehat X^2_{s-1} = \sum_{k=1}^s \frac{(m_k - n \hat p_k)^2}{n \hat p_k} + \sum_{k=1}^s \frac{(m'_k - l \hat p_k)^2}{l \hat p_k} \Rightarrow \chi^2_{s-1}
$$
(в общем случае, имея r полиномиальных схем, получим $\chi^2_{(s-1)(r-1)}$).

\end{itemize}


%%%
%%% Lecture 15 by SK
%%%

\subsection{Критерий знаков}
$(x_1, y_1), \dots, (x_n, y_n)$ --- набор парных наблюдений, порождённый
парой многомерных случайных величин $(\xi, \xi')$ ($\xi$ и $\xi'$ могут
быть зависимыми, а вот $\xi_1, \dots, \xi_n$ независимы в совокупности
и одинаково распределены, равно как и $\xi_1', \dots, \xi_n'$). Пусть
$\xi_1, \dots, \xi_n$ распределены с функцией распределения $F(x)$, а
$\xi_1', \dots, \xi_n'$ --- с функцией распределения $G(x) =
F(x - \theta)$, $\theta\in\mathbb{R}$ --- параметр.
Гипотеза $H_0$ состоит в однородности выборок (т.е. $\theta = 0$);
$H_1 = \lnot H_0$.

Перейдём к разностям $z_i = x_i - y_i$ (это значения случайных величин
$\eta_i = \xi_i - \xi_i'$; $\eta_1, \dots, \eta_n$ независимы в
совокупности). Рассмотрим случай, когда $\forall i \, z_i \ne 0$.
Тогда гипотеза $H_0$ равносильна гипотезе $H_0'$:
$\forall i \, \Pf(\eta_i > 0) = \Pf(\eta_i < 0) = \frac 12$.
Судим о справедливости гипотезы $H_0$ по соотношению знаков
,,$+$'' и ,,$-$'' среди $z_i$. По сути имеется $n$ испытаний
Бернулли, где элементарными событиями являются
$A_i = \{ \eta_i > 0 \}$, $\overline{A_i} = \{ \eta_i < 0 \}$.
Гипотеза $H_0'$ равносильна гипотезе $H_0''$: ,,в этой схеме
Бернулли $p = \frac 12$'' ($p = \Pf(A_i)$ --- параметр схемы
Бернулли). Строим биномиальный критерий для случайной величины
$$
\mu_n^+ = \sum_{i=1}^{n} \ph_i,
\qquad
\ph_i = \begin{cases}
1, & \eta_i > 0 \\
0, & \eta_i < 0
\end{cases}
$$
с основной гипотезой $H_0'''$: $p = \frac 12$.

\emph{Практический совет} на случай, когда среди $z_i$ встречаются
нули: если нулей много, то критерий неприменим (потому что в
этом критерии считается, что $\Pf(\eta_i = 0) = 0$). Если же
нулей мало, то просто выкидываем те испытания, в которых
$z_i = 0$.


\subsection{Задача различения двух статистических гипотез}
$(\Xs, \As, \Ps)$ --- статистическая модель, $x = (x_1, \dots, x_n)
\in \Xs$ --- выборка. Пусть семейство распределений разбито на
два подсемейства: $\Ps = \Ps_1 \sqcup \Ps_2$. Хотим построить
критерий для выбора одной из двух гипотез: $H_0$: $\Pf \in \Ps_0$,
$H_1$: $\Pf \in \Ps_1$. Рассмотрим произвольное множество
$S \in \As$ и назовём его \emph{критическим множеством}
(\emph{критической областью}) критерия.
Сам критерий в терминах
$S$ формулируется так: если $x \in S$, то $H_0$ \emph{отклоняется}
($H_1$ принимается); если $x \notin S$, то $H_0$ принимается
($H_1$ отклоняется). Критерий с критическим множеством $S$
называется $S$-критерием.

\emph{Ошибка первого рода} --- отклонить
$H_0$, когда она верна. \emph{Ошибка второго рода} --- принять
$H_0$, когда верна $H_1$. Нужно принять решение так, чтобы
вероятности ошибок были минимальными. (Пока это не вполне
понятно: как считать вероятность ошибки, если в $\Ps_0$ и
$\Ps_1$ много разных распределений?)

\begin{ex}
$\Ps = \{ \Pf_0, \Pf_1 \}$ --- два разных распределения,
$\Ps_i = \{ \Pf_i \}, i \in \{ 0, 1 \}$. Пусть заданы
малые числа $\alpha, \beta \in (0, 1)$. Потребуем
$\Pf_0(S) \leqslant \alpha$ и $\Pf_1(\bar S) \leqslant \beta$
(или, что то же самое, $\Pf_0(\bar S) \geqslant 1 - \alpha$ и
$\Pf_1(S) \geqslant 1 - \beta$), то есть числа $\alpha$
и $\beta$ ограничивают сверху ошибки первого и второго рода
соответственно. Нужно найти такое множество $S$, чтобы различить
распределения $\Pf_0$ и $\Pf_1$.
\end{ex}

Перейдём к параметрической модели: $\Ps = \{ \Pf_\theta \mid
\theta\in\Theta \}$, причём распределения попарно различны
($\theta' \ne \theta \, \Rightarrow \, \Pf_{\theta'} \ne \Pf_\theta$).
$\Theta = \Theta_0 \sqcup \Theta_1$; $H_0$: $\theta\in\Theta_0$,
$H_1$: $\theta\in\Theta_1$.

Пусть дан критерий, заданный критическим множеством $S$ ($S$-критерий).
Тогда функция $g_S(\theta) := \Pf_\theta(S)$ от переменной $\theta$ называется
\emph{функцией мощности} $S$-критерия. При $\theta\in\Theta_0$
$g_S(\theta) = \Pf_\theta(S)$ есть вероятность ошибки первого рода,
а для $\theta\in\Theta_1$ вероятность ошибки второго рода выражается
так: $\Pf_\theta(\bar S) = 1 - \Pf_\theta(S) = 1 - g_S(\theta)$.
$\sup\limits_{\theta\in\Theta_0} g_S(\theta)$ --- \emph{размер критерия}.
Мы требуем, чтобы $\sup\limits_{\theta\in\Theta_0} g_S(\theta) \leqslant
\alpha$; в этом случае $\alpha$ есть \emph{уровень значимости} критерия.
$\inf\limits_{\theta\in\Theta_1} g_S(\theta)$ --- \emph{мощность
критерия} (мощность есть наименьшая вероятность принять альтернативную
гипотезу, когда она верна). Идеальной (недостижимой) функцией мощности
является индикатор множества $\Theta_1$.

Критерий можно определить также при помощи \emph{критической функции}
$\ph_S$ --- индикатора множества $S$. Тогда
$g_S(\theta) = \Pf_\theta(S) = \Mf_\theta \ph_S(\xi)$.

Статистическая гипотеза называется \emph{простой}, если она имеет
вид $\Pf = \Pf_0$, где $\Pf_0$ --- заданное распределение (или, что
то же самое, $F(x) = F_0(x)$ или $\theta = \theta_0$; $F_0$ и
$\theta_0$ известны), т.е. речь идёт о конкретном распределении
вероятностей. Остальные гипотезы --- сложные.

\subsubsection{Сравнение двух простых гипотез. Теорема Неймана\ч
Пирсона}
Пусть $\Theta = \{ \theta_0, \theta_1 \} \subset \mathbb{R}$, $\theta_0
< \theta_1$. $H_0$: $\theta = \theta_0$, $H_1$: $\theta = \theta_1$.
$\Pf_{\theta_0} (S)$ --- вероятность ошибки первого рода (размер критерия).
$\Pf_{\theta_1} (\bar S)$ --- вероятность ошибки второго рода;
$1 - \Pf_{\theta_1} (\bar S) = \Pf_{\theta_1} (S)$ --- мощность критерия.

{\bf Сравнение критериев.} Зададим $\alpha \in (0,1)$ --- уровень
значимости (ограничитель размера критерия). Рассмотрим все критерии
(критические функции) $\ph_S(x)$ с $\Mf_{\theta_0} \ph_S(\xi) = \alpha$
(если $\Pf_{\theta_0}$ --- дискретное распределение, то такого $S$
может и не найтись --- тогда требуем $\leqslant \alpha$ или
применяем рандомизацию --- см. ниже).
Среди этих критериев выделяем $\ph_{S^*} = \ph_S^*$ --- такой, что
$\Mf_{\theta_1} \ph_S^*(\xi) = \sup\limits_{\ph_S\, :\, \alpha = \Mf_{\theta_0}
\ph_S(\xi)} \Mf_{\theta_1} \ph_S(\xi)$ (в общем случае берём $\sup$
по $\theta\in\Theta_1$) --- если такой критерий существует, то
это \emph{наиболее мощный критерий} уровня значимости $\alpha$.

{\bf Рандомизированные критерии.} Расширим множество рассматриваемых
критериев. Пусть $\ph(x)$ --- произвольная функция, принимающая значения
из $[0,1]$ (не обязательно индикатор). Если $\ph(x) = 1$, то $H_0$
отклоняется, если $\ph(x) = 0$, то $H_0$ принимается, а вот если
$0 < \ph(x) < 1$, то бросается жульническая монетка и $H_0$ отклоняется
в вероятностью $\ph(x)$. Такая процедура называется \emph{рандомизацией},
а сам критерий --- \emph{рандомизированным}. (Бросание монетки ---
вспомогательный эксперимент.) В этом случае функция мощности критерия
задаётся формулой: $g(\theta) = \Mf_\theta \ph(\xi)$.

\begin{theorem}[теорема Неймана\ч Пирсона; фундаментальная лемма]
Пусть $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$,
$\Theta = \{ \theta_0, \theta_1 \} \subset \mathbb{R}$,
$\theta_0 < \theta_1$; семейство $\Ps$ абсолютно непрерывно
относительно некоторой меры (например, меры Лебега в $\mathbb{R}^n$),
т.е. существует плотность $p_\theta(x) = L(\theta; x)$ --- функция
правдоподобия; $L_i(x) = L(\theta_i; x), i \in \{ 0, 1 \}$. Пусть
$p_\theta(x) > 0$ (для всех $x \in \mathbb{R}, \theta\in\Theta$).
$H_0$: $\theta = \theta_0$, $H_1$: $\theta = \theta_1$. Задано
$\alpha \in (0,1)$ --- уровень значимости (вероятность ошибки
первого рода).\par Тогда критерий с критической функцией
$$
\ph^*(x) = \begin{cases}
1,          & L_1(x) > c_\alpha L_2(x), \\
\ep_\alpha, & L_1(x) = c_\alpha L_2(x), \\
0,          & L_1(x) < c_\alpha L_2(x),
\end{cases}
$$
где $c_\alpha$ и $\ep_\alpha$ определяются из условия
$\Mf_{\theta_0} \ph^*(\xi) = \alpha$, таков, что для любой
критической функции $\ph$ с $\Mf_{\theta_0} \ph(\xi) = \alpha$ имеет
место неравенство $\Mf_{\theta_1} \ph^*(\xi) \geqslant
\Mf_{\theta_1} \ph(\xi)$ (т.е. этот критерий обладает максимальной
мощностью среди критериев уровня значимости $\alpha$).
\end{theorem}

\begin{proof}
Сначала найдём $c_\alpha$ и $\ep_\alpha$.
Рассмотрим функцию $f(c) = \Pf_{\theta_0} \{ L_1(\xi) > c L_0(\xi) \}$,
$c \in [0, +\infty)$. $f$ --- невозрастающая функция $f(0) = 1$,
$f(+\infty) = 0$ (последняя запись понимается как предел).
$1 - f(c) = \Pf_{\theta_0} \left\{
\dfrac {L_1(\xi)}{L_2(\xi)} \leqslant c \right\}$ ---
функция распределения отношения значений функций правдоподобия
(\emph{отношения правдоподобия}) --- неубывающая непрерывная слева
функция.

Положим $c_\alpha = \min \{ c \mid f(c) \leqslant \alpha < f(c - 0) \}$
(если $c_\alpha$ --- точка непрерывности $f$, то $f(c_\alpha) = \alpha$).
Определим $\ep_\alpha$:
$$
\ep_\alpha = \begin{cases}
0, & \text{если $c_\alpha$ --- точка непрерывности $f$ ($f(c_\alpha) =
f(c_\alpha - 0)$)}, \\
\dfrac{\alpha - f(c_\alpha)}{f(c_\alpha - 0) - f(c_\alpha)}
& \text{в ином случае.}
\end{cases}
$$

Критерий определён. Будем доказывать его свойства.

\begin{multline*}
\Mf_{\theta_0} \ph^*(\xi) = \int\limits_{\Xs} \ph^*(x) p_{\theta_0}(x) \, dx =
\int\limits_{\{x \mid L_1(x) > c_\alpha L_0(x)\}} p_{\theta_0}(x) \, dx +
\ep_\alpha
\int\limits_{\{x \mid L_1(x) = c_\alpha L_0(x) \}} p_{\theta_0}(x) \, dx =\\=
\begin{cases}
f(c_\alpha) = \alpha, & \text{если $c_\alpha$ --- точка непрерывности $f$}, \\
f(c_\alpha) +
\dfrac{\alpha - f(c_\alpha)}{f(c_\alpha - 0) - f(c_\alpha)}
(f(c_\alpha - 0) - f(c_\alpha)) = \alpha & \text{в ином случае.}
\end{cases}
\end{multline*}
Итак, ошибка первого рода равна $\alpha$.

Теперь возьмём любой другой критерий с критической функцией $\ph$
с условием $\Mf_{\theta_0} \ph(\xi) = \alpha$. Покажем, что
$\Mf_{\theta_1} \ph^*(\xi) \geqslant \Mf_{\theta_1} \ph(\xi)$.
$\Xs^+:= \{ x\in\Xs \mid \ph^*(x) - \ph(x) \geqslant 0 \}$,
$\Xs^-:= \{ x\in\Xs \mid \ph^*(x) - \ph(x) < 0 \}$. $\Xs =
\Xs^+ \sqcup \Xs^-$. Для всех $x\in\Xs^+$ имеем, что
$\ph^*(x) - \ph(x) \geqslant 0$ $\Rightarrow$ $\ph^*(x) \geqslant
\ph(x) \geqslant 0$ $\Rightarrow$ $\ph^*(x) \geqslant 0$
$\Rightarrow$ $L_1(x) \geqslant c_\alpha L_0(x)$, откуда
$(\ph^*(x) - \ph(x))(p_{\theta_1}(x) - c_\alpha p_{\theta_0}(x))
\geqslant 0$. Если же $x \in \Xs^-$, то
$\ph^*(x) - \ph(x) < 0$, откуда $\ph^*(x) < 1$, то есть
$L_1(x) \leqslant c_\alpha L_0(x)$, и опять
$(\ph^*(x) - \ph(x))(p_{\theta_1}(x) - c_\alpha p_{\theta_0}(x))
\geqslant 0$. Поэтому
$$
\Mf_{\theta_1} (\ph^*(\xi) - \ph(\xi)) =
\int\limits_\Xs (\ph^*(x) - \ph(x)) p_{\theta_1}(x) \, dx =
\int\limits_\Xs (\ph^* - \ph)(p_{\theta_1}(x) -
\underbrace{c_\alpha p_{\theta_0}(x)}_{\text{\hspace*{-25pt}вычли нулевой интеграл\hspace*{-25pt}}}
) \, dx \geqslant 0.
$$
\end{proof}

Если $\Pf_{\theta_0}(S) \leqslant \Pf_{\theta_1}(S)$, то $S$-критерий
называется \emph{несмещённым}. Теорема Неймана\ч Пирсона даёт наиболее
мощный критерий, который к тому же является несмещённым.

\end{document}

